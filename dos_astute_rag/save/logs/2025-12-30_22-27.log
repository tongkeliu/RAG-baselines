2025-12-30 22:27:04,313 - INFO - Starting precreate_nodes process...
2025-12-30 22:27:04,314 - INFO - Initializing models...
2025-12-30 22:27:16,774 - INFO - Use pytorch device_name: cuda:0
2025-12-30 22:27:16,775 - INFO - Load pretrained SentenceTransformer: Snowflake/snowflake-arctic-embed-m-v1.5
2025-12-30 22:27:26,632 - INFO - 1 prompt is loaded, with the key: query
2025-12-30 22:27:26,851 - INFO - Processing document AHWH4MZ2KD5RASDETTSVDK3CYTGA...
2025-12-30 22:27:26,860 - ERROR - Error processing document_id AHWH4MZ2KD5RASDETTSVDK3CYTGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,864 - INFO - Processing document AEX5SG4YNMTJXAAONXSG3Z4LYQ5A...
2025-12-30 22:27:26,870 - ERROR - Error processing document_id AEX5SG4YNMTJXAAONXSG3Z4LYQ5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,872 - INFO - Processing document AECTRGMRKOGAYIV3YXX73CQEQCSQ...
2025-12-30 22:27:26,877 - ERROR - Error processing document_id AECTRGMRKOGAYIV3YXX73CQEQCSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,880 - INFO - Processing document AHIN57TM7CPPE3RRGKODLYVUNDYA...
2025-12-30 22:27:26,884 - ERROR - Error processing document_id AHIN57TM7CPPE3RRGKODLYVUNDYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,886 - INFO - Processing document AH5QXAX6DF5RDZVO77LNZTC5YYTQ...
2025-12-30 22:27:26,890 - ERROR - Error processing document_id AH5QXAX6DF5RDZVO77LNZTC5YYTQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,893 - INFO - Processing document AF4PX5K4TGJ35YOESS7VMUT25RBA...
2025-12-30 22:27:26,896 - ERROR - Error processing document_id AF4PX5K4TGJ35YOESS7VMUT25RBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,898 - INFO - Processing document AFFE5A2BGXJQTQMCYHFC4YHV6KFQ...
2025-12-30 22:27:26,907 - ERROR - Error processing document_id AFFE5A2BGXJQTQMCYHFC4YHV6KFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,909 - INFO - Processing document AFNYGE44MDUCNFBC2KMOHEM3EVIA...
2025-12-30 22:27:26,912 - ERROR - Error processing document_id AFNYGE44MDUCNFBC2KMOHEM3EVIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,914 - INFO - Processing document AGCJDIFFVTYXRE32FSBTYU447K2A...
2025-12-30 22:27:26,917 - ERROR - Error processing document_id AGCJDIFFVTYXRE32FSBTYU447K2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,919 - INFO - Processing document AHLFFZMUXFR4XOG347MLKA5JOKWQ...
2025-12-30 22:27:26,922 - ERROR - Error processing document_id AHLFFZMUXFR4XOG347MLKA5JOKWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,924 - INFO - Processing document AEHUXBCXJCRXIFD72W3JBFMNNPYQ...
2025-12-30 22:27:26,926 - ERROR - Error processing document_id AEHUXBCXJCRXIFD72W3JBFMNNPYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,927 - INFO - Processing document AHXX4YOBOOTFSGD7YBWJ7JLOQ32Q...
2025-12-30 22:27:26,929 - ERROR - Error processing document_id AHXX4YOBOOTFSGD7YBWJ7JLOQ32Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,931 - INFO - Processing document AG5SKMFDTH62RTVD2Y4SNKRHBE7Q...
2025-12-30 22:27:26,933 - ERROR - Error processing document_id AG5SKMFDTH62RTVD2Y4SNKRHBE7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,934 - INFO - Processing document AGKJLXHEGQCMYOBJQR3SRS5WZKJA...
2025-12-30 22:27:26,938 - ERROR - Error processing document_id AGKJLXHEGQCMYOBJQR3SRS5WZKJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,940 - INFO - Processing document AG2O5XWVSLNQL2WFXOAMUURRLDFA...
2025-12-30 22:27:26,941 - ERROR - Error processing document_id AG2O5XWVSLNQL2WFXOAMUURRLDFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,943 - INFO - Processing document AGXUCZJ54ZTURKHDJINFKFKLNLFA...
2025-12-30 22:27:26,948 - ERROR - Error processing document_id AGXUCZJ54ZTURKHDJINFKFKLNLFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,950 - INFO - Processing document AGAM7XSVSBJ4IG5TMY5SBDAOEZBA...
2025-12-30 22:27:26,952 - ERROR - Error processing document_id AGAM7XSVSBJ4IG5TMY5SBDAOEZBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,954 - INFO - Processing document AGYGIKY6HP4Z2WXRCSS3WONMDEFQ...
2025-12-30 22:27:26,956 - ERROR - Error processing document_id AGYGIKY6HP4Z2WXRCSS3WONMDEFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,958 - INFO - Processing document AECLRKKLCNSP3KT4RJR44K4WTJOQ...
2025-12-30 22:27:26,960 - ERROR - Error processing document_id AECLRKKLCNSP3KT4RJR44K4WTJOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,961 - INFO - Processing document AH5M56UWJNMMI3EBKUQPALNQG4ZQ...
2025-12-30 22:27:26,963 - ERROR - Error processing document_id AH5M56UWJNMMI3EBKUQPALNQG4ZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,965 - INFO - Processing document AFE2C6ASNFA6HVK6BRLJTTSUKS4A...
2025-12-30 22:27:26,967 - ERROR - Error processing document_id AFE2C6ASNFA6HVK6BRLJTTSUKS4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,969 - INFO - Processing document AGZKZWIJEF7TMMDKJY25FLRQ6B3A...
2025-12-30 22:27:26,971 - ERROR - Error processing document_id AGZKZWIJEF7TMMDKJY25FLRQ6B3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,973 - INFO - Processing document AFYKVHKB7XHW4LWQ6ILAST72NHLA...
2025-12-30 22:27:26,977 - ERROR - Error processing document_id AFYKVHKB7XHW4LWQ6ILAST72NHLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,978 - INFO - Processing document AHCBLEYD2E3XUXQEE3EOJ5SLCESQ...
2025-12-30 22:27:26,981 - ERROR - Error processing document_id AHCBLEYD2E3XUXQEE3EOJ5SLCESQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,983 - INFO - Processing document AHFAVTVOL2KUU4PRARH7ZJQOR75Q...
2025-12-30 22:27:26,985 - ERROR - Error processing document_id AHFAVTVOL2KUU4PRARH7ZJQOR75Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,987 - INFO - Processing document AFMC775V56QYCNU3ZWLDQOGCNAJQ...
2025-12-30 22:27:26,990 - ERROR - Error processing document_id AFMC775V56QYCNU3ZWLDQOGCNAJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,991 - INFO - Processing document AHMWXJVZURA6LKFDAOAJ26CXFZ6A...
2025-12-30 22:27:26,994 - ERROR - Error processing document_id AHMWXJVZURA6LKFDAOAJ26CXFZ6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,995 - INFO - Processing document AFH7NBWFUDTQMK7IW35A2B2ZH4VQ...
2025-12-30 22:27:26,998 - ERROR - Error processing document_id AFH7NBWFUDTQMK7IW35A2B2ZH4VQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:26,999 - INFO - Processing document AG2WJY7J54POLCXLNX7KWXJD5EHQ...
2025-12-30 22:27:27,002 - ERROR - Error processing document_id AG2WJY7J54POLCXLNX7KWXJD5EHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,004 - INFO - Processing document AEFGX4JZYT6HHSUK7KHCBPNYAGPA...
2025-12-30 22:27:27,006 - ERROR - Error processing document_id AEFGX4JZYT6HHSUK7KHCBPNYAGPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,007 - INFO - Processing document AEAWWN5FWR7PIAXCN5W76DZ6E2LQ...
2025-12-30 22:27:27,010 - ERROR - Error processing document_id AEAWWN5FWR7PIAXCN5W76DZ6E2LQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,011 - INFO - Processing document AELHNTAU54MHMZWHKPD3DNMJJRRA...
2025-12-30 22:27:27,014 - ERROR - Error processing document_id AELHNTAU54MHMZWHKPD3DNMJJRRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,015 - INFO - Processing document AEBS5ZGPCJE2ZT6LU3WEP4XMX37Q...
2025-12-30 22:27:27,018 - ERROR - Error processing document_id AEBS5ZGPCJE2ZT6LU3WEP4XMX37Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,020 - INFO - Processing document AGC3TAYAVB34XMXA5H5QYDJWALLQ...
2025-12-30 22:27:27,023 - ERROR - Error processing document_id AGC3TAYAVB34XMXA5H5QYDJWALLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,024 - INFO - Processing document AEB3334PRZKEQMRTCM7NJUDEV6MQ...
2025-12-30 22:27:27,028 - ERROR - Error processing document_id AEB3334PRZKEQMRTCM7NJUDEV6MQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,029 - INFO - Processing document AEIGTLL5GIAAWSJEYMQZ7I66U2HQ...
2025-12-30 22:27:27,035 - ERROR - Error processing document_id AEIGTLL5GIAAWSJEYMQZ7I66U2HQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,037 - INFO - Processing document AFQDBLNM2JWF2OYKHDOHPKGASXPA...
2025-12-30 22:27:27,039 - ERROR - Error processing document_id AFQDBLNM2JWF2OYKHDOHPKGASXPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,041 - INFO - Processing document AFH6LQK5L2LH2ZPBFWKAMKOVY7AQ...
2025-12-30 22:27:27,044 - ERROR - Error processing document_id AFH6LQK5L2LH2ZPBFWKAMKOVY7AQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,046 - INFO - Processing document AHTLG4HMRAKA2PV64QS2LL2RSOHA...
2025-12-30 22:27:27,048 - ERROR - Error processing document_id AHTLG4HMRAKA2PV64QS2LL2RSOHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,050 - INFO - Processing document AHP5W43QC72KSDMLNBR2XLX6UKXQ...
2025-12-30 22:27:27,053 - ERROR - Error processing document_id AHP5W43QC72KSDMLNBR2XLX6UKXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,054 - INFO - Processing document AFPMVI6ZRR7KS7AWRIBCKILWDVIA...
2025-12-30 22:27:27,058 - ERROR - Error processing document_id AFPMVI6ZRR7KS7AWRIBCKILWDVIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,060 - INFO - Processing document AHSLQ43ODLVUIP5M22XFLIXSISGA...
2025-12-30 22:27:27,062 - ERROR - Error processing document_id AHSLQ43ODLVUIP5M22XFLIXSISGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,064 - INFO - Processing document AGWR3PXKUQ7AYUKSPOAF3ZNCBN5A...
2025-12-30 22:27:27,066 - ERROR - Error processing document_id AGWR3PXKUQ7AYUKSPOAF3ZNCBN5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,068 - INFO - Processing document AFWQEWNTM3LFG3QGHOWKRI2RI6DQ...
2025-12-30 22:27:27,071 - ERROR - Error processing document_id AFWQEWNTM3LFG3QGHOWKRI2RI6DQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,072 - INFO - Processing document AGSUM25V66G6ZB3NZSYSNGYGQBBA...
2025-12-30 22:27:27,074 - ERROR - Error processing document_id AGSUM25V66G6ZB3NZSYSNGYGQBBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,076 - INFO - Processing document AGV56EDQPBW4UAEM32KE6VMIIAIQ...
2025-12-30 22:27:27,078 - ERROR - Error processing document_id AGV56EDQPBW4UAEM32KE6VMIIAIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,079 - INFO - Processing document AHZZL323ORE6AHMIB2OCLYNCRXJQ...
2025-12-30 22:27:27,083 - ERROR - Error processing document_id AHZZL323ORE6AHMIB2OCLYNCRXJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,085 - INFO - Processing document AF3RIXFFIORNQZTXHDKBYQXLAQMA...
2025-12-30 22:27:27,087 - ERROR - Error processing document_id AF3RIXFFIORNQZTXHDKBYQXLAQMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,088 - INFO - Processing document AFMSKPD5ZG6NEOIQCYATQKQQOTGQ...
2025-12-30 22:27:27,091 - ERROR - Error processing document_id AFMSKPD5ZG6NEOIQCYATQKQQOTGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,093 - INFO - Processing document AGMAXEWXXNUTIOS5ASX5QJTBC62A...
2025-12-30 22:27:27,095 - ERROR - Error processing document_id AGMAXEWXXNUTIOS5ASX5QJTBC62A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,097 - INFO - Processing document AHLJR5O6TV2CVP37E3DKH5FM77OA...
2025-12-30 22:27:27,101 - ERROR - Error processing document_id AHLJR5O6TV2CVP37E3DKH5FM77OA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,103 - INFO - Processing document AH5PNJABFY5OHWHZC3N77PJRQYQA...
2025-12-30 22:27:27,105 - ERROR - Error processing document_id AH5PNJABFY5OHWHZC3N77PJRQYQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,107 - INFO - Processing document AFPKA5PV777FKBSJBT4H7GTNVSOA...
2025-12-30 22:27:27,109 - ERROR - Error processing document_id AFPKA5PV777FKBSJBT4H7GTNVSOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,111 - INFO - Processing document AGIUTASQIZG4S2PUDUGAG7HQG3BQ...
2025-12-30 22:27:27,113 - ERROR - Error processing document_id AGIUTASQIZG4S2PUDUGAG7HQG3BQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,115 - INFO - Processing document AH4XJHWHDRC2TPHD72ZEAMRJ2GEA...
2025-12-30 22:27:27,117 - ERROR - Error processing document_id AH4XJHWHDRC2TPHD72ZEAMRJ2GEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,118 - INFO - Processing document AEU5PWQ52VQLMMZCUWJV7KO4UQLQ...
2025-12-30 22:27:27,121 - ERROR - Error processing document_id AEU5PWQ52VQLMMZCUWJV7KO4UQLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,122 - INFO - Processing document AHZKPBAPGV34K7XHWQM7L62B3A2A...
2025-12-30 22:27:27,125 - ERROR - Error processing document_id AHZKPBAPGV34K7XHWQM7L62B3A2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,126 - INFO - Processing document AEKJWZWOYGIWJT7VAY2TFPTTIK6A...
2025-12-30 22:27:27,130 - ERROR - Error processing document_id AEKJWZWOYGIWJT7VAY2TFPTTIK6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,132 - INFO - Processing document AE6VILPJGYTNMUZJCTFQPDPOPIGQ...
2025-12-30 22:27:27,134 - ERROR - Error processing document_id AE6VILPJGYTNMUZJCTFQPDPOPIGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,136 - INFO - Processing document AF5C22YLGYT3DCRWZ6HRGQSCMMQQ...
2025-12-30 22:27:27,139 - ERROR - Error processing document_id AF5C22YLGYT3DCRWZ6HRGQSCMMQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,141 - INFO - Processing document AHYF4IY3KHAJLA3ESZALLKZ733KA...
2025-12-30 22:27:27,143 - ERROR - Error processing document_id AHYF4IY3KHAJLA3ESZALLKZ733KA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,144 - INFO - Processing document AEECEOIXTTZUJRKR7KC3ULFDASVQ...
2025-12-30 22:27:27,146 - ERROR - Error processing document_id AEECEOIXTTZUJRKR7KC3ULFDASVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,148 - INFO - Processing document AH2RFTKPBJK75OOBORMYCRRZYWXA...
2025-12-30 22:27:27,151 - ERROR - Error processing document_id AH2RFTKPBJK75OOBORMYCRRZYWXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,153 - INFO - Processing document AEHXBFB55HGTTRH3D2MRMGRWBHDQ...
2025-12-30 22:27:27,155 - ERROR - Error processing document_id AEHXBFB55HGTTRH3D2MRMGRWBHDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,157 - INFO - Processing document AG3S4FROO422V5KP7DJCBXVUQLJQ...
2025-12-30 22:27:27,160 - ERROR - Error processing document_id AG3S4FROO422V5KP7DJCBXVUQLJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,161 - INFO - Processing document AFJSSKRIKK7DW5UVAQC6FZQGCAIA...
2025-12-30 22:27:27,163 - ERROR - Error processing document_id AFJSSKRIKK7DW5UVAQC6FZQGCAIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,165 - INFO - Processing document AHEP53NJQZHUXUJTXPUWGWTO4F4A...
2025-12-30 22:27:27,167 - ERROR - Error processing document_id AHEP53NJQZHUXUJTXPUWGWTO4F4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,168 - INFO - Processing document AFDK4L6EL763PPUAVQUIADC2QSGQ...
2025-12-30 22:27:27,171 - ERROR - Error processing document_id AFDK4L6EL763PPUAVQUIADC2QSGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,173 - INFO - Processing document AEP6VSEK3Y5MHGJCG2RGHM2EHCLA...
2025-12-30 22:27:27,175 - ERROR - Error processing document_id AEP6VSEK3Y5MHGJCG2RGHM2EHCLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,177 - INFO - Processing document AG7J6V7VY3ERX4NONJWA6FXNL7OA...
2025-12-30 22:27:27,179 - ERROR - Error processing document_id AG7J6V7VY3ERX4NONJWA6FXNL7OA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,181 - INFO - Processing document AHVNOAZKWDGUVJZJ47U2Y5GTMATA...
2025-12-30 22:27:27,183 - ERROR - Error processing document_id AHVNOAZKWDGUVJZJ47U2Y5GTMATA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,184 - INFO - Processing document AE5IFZFRVPQOL6P4VHCEXNNHJGSA...
2025-12-30 22:27:27,186 - ERROR - Error processing document_id AE5IFZFRVPQOL6P4VHCEXNNHJGSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,188 - INFO - Processing document AHQ3WA2UAESVMC4P7G5MEU6VBGNQ...
2025-12-30 22:27:27,191 - ERROR - Error processing document_id AHQ3WA2UAESVMC4P7G5MEU6VBGNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,193 - INFO - Processing document AFKEKTJUEFUMXYPGVP6I5X5FVFFQ...
2025-12-30 22:27:27,196 - ERROR - Error processing document_id AFKEKTJUEFUMXYPGVP6I5X5FVFFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,198 - INFO - Processing document AGTPTT52V7BOHLJS43HEJIJP7NUA...
2025-12-30 22:27:27,202 - ERROR - Error processing document_id AGTPTT52V7BOHLJS43HEJIJP7NUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,203 - INFO - Processing document AFDFG2H4BKFML22NULTWWTTRFWPA...
2025-12-30 22:27:27,206 - ERROR - Error processing document_id AFDFG2H4BKFML22NULTWWTTRFWPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,208 - INFO - Processing document AF4SGXHP4MHOZYFQGDBRYNKQ6SIQ...
2025-12-30 22:27:27,210 - ERROR - Error processing document_id AF4SGXHP4MHOZYFQGDBRYNKQ6SIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,212 - INFO - Processing document AEIOHVUW5LUCT6M5NL33LXGZ3DWQ...
2025-12-30 22:27:27,214 - ERROR - Error processing document_id AEIOHVUW5LUCT6M5NL33LXGZ3DWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,216 - INFO - Processing document AFHOBSRGTWQTQFXEBY6OVYBSWJFA...
2025-12-30 22:27:27,218 - ERROR - Error processing document_id AFHOBSRGTWQTQFXEBY6OVYBSWJFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,219 - INFO - Processing document AEISSDAHVQZBTKL5CW7FAVRKACJA...
2025-12-30 22:27:27,222 - ERROR - Error processing document_id AEISSDAHVQZBTKL5CW7FAVRKACJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,223 - INFO - Processing document AERQG2U5GKGCRVI2VVHK5DQZO3ZQ...
2025-12-30 22:27:27,227 - ERROR - Error processing document_id AERQG2U5GKGCRVI2VVHK5DQZO3ZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,229 - INFO - Processing document AG7GTPZXG7QZ3ZGTPPW6GVKOF2GA...
2025-12-30 22:27:27,231 - ERROR - Error processing document_id AG7GTPZXG7QZ3ZGTPPW6GVKOF2GA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,233 - INFO - Processing document AHN3ACLVEKKPT4HIE7TJGCUHSJ7Q...
2025-12-30 22:27:27,235 - ERROR - Error processing document_id AHN3ACLVEKKPT4HIE7TJGCUHSJ7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,237 - INFO - Processing document AHP55QOVFFB75GVIRV6EAAFH6ZSA...
2025-12-30 22:27:27,241 - ERROR - Error processing document_id AHP55QOVFFB75GVIRV6EAAFH6ZSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,243 - INFO - Processing document AFZRBHATWMMOZ75SQMQGZNOD7CRA...
2025-12-30 22:27:27,245 - ERROR - Error processing document_id AFZRBHATWMMOZ75SQMQGZNOD7CRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,246 - INFO - Processing document AECDGWGLTQLBUZYPHREOTEJZMQIQ...
2025-12-30 22:27:27,249 - ERROR - Error processing document_id AECDGWGLTQLBUZYPHREOTEJZMQIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,250 - INFO - Processing document AG4OIBF57KNQ2OETOKL7CLFQY4CQ...
2025-12-30 22:27:27,253 - ERROR - Error processing document_id AG4OIBF57KNQ2OETOKL7CLFQY4CQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,254 - INFO - Processing document AGQ5EJCIFESIS3VZU7JU5TPBS3CQ...
2025-12-30 22:27:27,256 - ERROR - Error processing document_id AGQ5EJCIFESIS3VZU7JU5TPBS3CQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,258 - INFO - Processing document AF2DJVRSQHT22UTAPMD4R6JFCBUA...
2025-12-30 22:27:27,260 - ERROR - Error processing document_id AF2DJVRSQHT22UTAPMD4R6JFCBUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,262 - INFO - Processing document AG5K74J2QFJC2L3RSJ2UADJTQRUQ...
2025-12-30 22:27:27,264 - ERROR - Error processing document_id AG5K74J2QFJC2L3RSJ2UADJTQRUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,265 - INFO - Processing document AECZSOSZLH2GCEOHHVSFV64UQBXA...
2025-12-30 22:27:27,267 - ERROR - Error processing document_id AECZSOSZLH2GCEOHHVSFV64UQBXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,269 - INFO - Processing document AHYMAA5CV6NQUEKBRSIHEUDQ3CAQ...
2025-12-30 22:27:27,270 - ERROR - Error processing document_id AHYMAA5CV6NQUEKBRSIHEUDQ3CAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,272 - INFO - Processing document AF3CPGJHLNFKJWASL5HS65TLJ6YQ...
2025-12-30 22:27:27,276 - ERROR - Error processing document_id AF3CPGJHLNFKJWASL5HS65TLJ6YQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,277 - INFO - Processing document AEBRN6DMK62MRILSWLLKSQLNOCDQ...
2025-12-30 22:27:27,279 - ERROR - Error processing document_id AEBRN6DMK62MRILSWLLKSQLNOCDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,280 - INFO - Processing document AFQEXC2FSW3QCS4O6GUPZLLNV34Q...
2025-12-30 22:27:27,282 - ERROR - Error processing document_id AFQEXC2FSW3QCS4O6GUPZLLNV34Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,284 - INFO - Processing document AHUN5VLMX6VUSLFW3VE5EYKO4YJA...
2025-12-30 22:27:27,287 - ERROR - Error processing document_id AHUN5VLMX6VUSLFW3VE5EYKO4YJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,288 - INFO - Processing document AFAEKIVSINXPCL5GHUC3HFP3RC5A...
2025-12-30 22:27:27,290 - ERROR - Error processing document_id AFAEKIVSINXPCL5GHUC3HFP3RC5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,292 - INFO - Processing document AGHZBVUB365Y7GPCQOJPGNOSE5PQ...
2025-12-30 22:27:27,294 - ERROR - Error processing document_id AGHZBVUB365Y7GPCQOJPGNOSE5PQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,295 - INFO - Processing document AF2YMA33EUBQTMUGIS2AXMEUTHXA...
2025-12-30 22:27:27,298 - ERROR - Error processing document_id AF2YMA33EUBQTMUGIS2AXMEUTHXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,300 - INFO - Processing document AEMVLQ4LOG542INGGSPCBUCDTVRA...
2025-12-30 22:27:27,306 - ERROR - Error processing document_id AEMVLQ4LOG542INGGSPCBUCDTVRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,308 - INFO - Processing document AH6ANPCHFVIES3CJRYIPOID4RFRA...
2025-12-30 22:27:27,310 - ERROR - Error processing document_id AH6ANPCHFVIES3CJRYIPOID4RFRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,311 - INFO - Processing document AF32DVZGQ6YIYL6V2H54HAPJXLFQ...
2025-12-30 22:27:27,314 - ERROR - Error processing document_id AF32DVZGQ6YIYL6V2H54HAPJXLFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,315 - INFO - Processing document AG7BOX7LSHKRG37TUNSUWXNYY6EQ...
2025-12-30 22:27:27,319 - ERROR - Error processing document_id AG7BOX7LSHKRG37TUNSUWXNYY6EQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,320 - INFO - Processing document AH2KQVGPXDJZEG2BTSLBAOWUZARQ...
2025-12-30 22:27:27,323 - ERROR - Error processing document_id AH2KQVGPXDJZEG2BTSLBAOWUZARQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,324 - INFO - Processing document AHYCJGF2DYH4CUEZFE4VVNG2DWBA...
2025-12-30 22:27:27,328 - ERROR - Error processing document_id AHYCJGF2DYH4CUEZFE4VVNG2DWBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,329 - INFO - Processing document AES2HBFQUWBGH57Y4W5AY3SHWXPQ...
2025-12-30 22:27:27,331 - ERROR - Error processing document_id AES2HBFQUWBGH57Y4W5AY3SHWXPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,333 - INFO - Processing document AG2DS6RVU3U4YZTESLLKGDZO75SA...
2025-12-30 22:27:27,336 - ERROR - Error processing document_id AG2DS6RVU3U4YZTESLLKGDZO75SA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,338 - INFO - Processing document AH7PHQNFMWHHE4TBGMN76XRNNDGQ...
2025-12-30 22:27:27,340 - ERROR - Error processing document_id AH7PHQNFMWHHE4TBGMN76XRNNDGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,341 - INFO - Processing document AGET7MY7E4JMJFOABHYDYIYBNB3A...
2025-12-30 22:27:27,344 - ERROR - Error processing document_id AGET7MY7E4JMJFOABHYDYIYBNB3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,345 - INFO - Processing document AEYVPPWR4CIKWX4BGYKCBCDL2CZQ...
2025-12-30 22:27:27,347 - ERROR - Error processing document_id AEYVPPWR4CIKWX4BGYKCBCDL2CZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,349 - INFO - Processing document AGEAL7JAW54GZFVYYWAKGBJX66KA...
2025-12-30 22:27:27,351 - ERROR - Error processing document_id AGEAL7JAW54GZFVYYWAKGBJX66KA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,352 - INFO - Processing document AEBZFMNIRS5U4DQN2BVEYZFKRXEA...
2025-12-30 22:27:27,356 - ERROR - Error processing document_id AEBZFMNIRS5U4DQN2BVEYZFKRXEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,357 - INFO - Processing document AEIIRIHLIYKQGI7ZOCIJTRDF5NPQ...
2025-12-30 22:27:27,359 - ERROR - Error processing document_id AEIIRIHLIYKQGI7ZOCIJTRDF5NPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,361 - INFO - Processing document AFRRN4I7TOEWWI7HOVTWUFTFRPDQ...
2025-12-30 22:27:27,363 - ERROR - Error processing document_id AFRRN4I7TOEWWI7HOVTWUFTFRPDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,364 - INFO - Processing document AE5SK7TOFENFDV2TWOM2UIRZDB6Q...
2025-12-30 22:27:27,367 - ERROR - Error processing document_id AE5SK7TOFENFDV2TWOM2UIRZDB6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,368 - INFO - Processing document AFERRZEU3Y7DC6YZVZWNVHWCG7KA...
2025-12-30 22:27:27,370 - ERROR - Error processing document_id AFERRZEU3Y7DC6YZVZWNVHWCG7KA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,372 - INFO - Processing document AHOGZA266ZDLYDNRXMRXMQKGY6GA...
2025-12-30 22:27:27,375 - ERROR - Error processing document_id AHOGZA266ZDLYDNRXMRXMQKGY6GA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,376 - INFO - Processing document AHVOAO4BG2KEPKOUUHI6RRQMDWYA...
2025-12-30 22:27:27,380 - ERROR - Error processing document_id AHVOAO4BG2KEPKOUUHI6RRQMDWYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,382 - INFO - Processing document AHU2SIEOSIFCR7UKRYRQ5BKRJCNA...
2025-12-30 22:27:27,385 - ERROR - Error processing document_id AHU2SIEOSIFCR7UKRYRQ5BKRJCNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,387 - INFO - Processing document AGSWHKAW4GEIYLRCTDDEDTUL7J5Q...
2025-12-30 22:27:27,393 - ERROR - Error processing document_id AGSWHKAW4GEIYLRCTDDEDTUL7J5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,395 - INFO - Processing document AG5YUZR7LEFKAOLOMEAGB3BGCFNQ...
2025-12-30 22:27:27,399 - ERROR - Error processing document_id AG5YUZR7LEFKAOLOMEAGB3BGCFNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,401 - INFO - Processing document AFGSM2IFWCMZCPCORIQEO7SGXIRQ...
2025-12-30 22:27:27,403 - ERROR - Error processing document_id AFGSM2IFWCMZCPCORIQEO7SGXIRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,405 - INFO - Processing document AHOR6J2GZSBMBJRWYTNDXGCVKJ4A...
2025-12-30 22:27:27,411 - ERROR - Error processing document_id AHOR6J2GZSBMBJRWYTNDXGCVKJ4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,413 - INFO - Processing document AF3E5BUP55DPPTHU7THO3EUHQROQ...
2025-12-30 22:27:27,417 - ERROR - Error processing document_id AF3E5BUP55DPPTHU7THO3EUHQROQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,419 - INFO - Processing document AHG35E5FHNF746BKLNB7SIQCXIOQ...
2025-12-30 22:27:27,421 - ERROR - Error processing document_id AHG35E5FHNF746BKLNB7SIQCXIOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,423 - INFO - Processing document AHBMQUED3IHU4XXUYBHCVHSZOO6Q...
2025-12-30 22:27:27,425 - ERROR - Error processing document_id AHBMQUED3IHU4XXUYBHCVHSZOO6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,427 - INFO - Processing document AGZXNXUWL7CHLHNL4JSBL52FHCRQ...
2025-12-30 22:27:27,429 - ERROR - Error processing document_id AGZXNXUWL7CHLHNL4JSBL52FHCRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,431 - INFO - Processing document AEJTSF2B4DO7IZDL5YY544KSOJ6Q...
2025-12-30 22:27:27,434 - ERROR - Error processing document_id AEJTSF2B4DO7IZDL5YY544KSOJ6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,435 - INFO - Processing document AHYXAUFEMITXGZWLXZW2HPCERATA...
2025-12-30 22:27:27,439 - ERROR - Error processing document_id AHYXAUFEMITXGZWLXZW2HPCERATA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,440 - INFO - Processing document AGJ3XWQI46FTJ77PQHHVT5D56FTA...
2025-12-30 22:27:27,442 - ERROR - Error processing document_id AGJ3XWQI46FTJ77PQHHVT5D56FTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,444 - INFO - Processing document AFAHC2OYAUZKKKEDGFVRN45PQ2HA...
2025-12-30 22:27:27,446 - ERROR - Error processing document_id AFAHC2OYAUZKKKEDGFVRN45PQ2HA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,448 - INFO - Processing document AF3HDVNZORGY7BUF7VEANTMKRPJA...
2025-12-30 22:27:27,449 - ERROR - Error processing document_id AF3HDVNZORGY7BUF7VEANTMKRPJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,451 - INFO - Processing document AF7XL5524TASMVWOR4UX5RGS6INA...
2025-12-30 22:27:27,453 - ERROR - Error processing document_id AF7XL5524TASMVWOR4UX5RGS6INA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,455 - INFO - Processing document AFJPFJDJY7I3BI7LNHR5SJ456O6Q...
2025-12-30 22:27:27,457 - ERROR - Error processing document_id AFJPFJDJY7I3BI7LNHR5SJ456O6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,459 - INFO - Processing document AHVDL5MXSYNX6KHCNLTH47XP4QJQ...
2025-12-30 22:27:27,461 - ERROR - Error processing document_id AHVDL5MXSYNX6KHCNLTH47XP4QJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,462 - INFO - Processing document AEHMKX7H5OTTIKYECNPGVTXKT4EA...
2025-12-30 22:27:27,466 - ERROR - Error processing document_id AEHMKX7H5OTTIKYECNPGVTXKT4EA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,468 - INFO - Processing document AHEDJDOIHBKRCODYL4HTIH2URG3A...
2025-12-30 22:27:27,471 - ERROR - Error processing document_id AHEDJDOIHBKRCODYL4HTIH2URG3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,473 - INFO - Processing document AFIJLAW3HIOMRUFSWNH54IJ3XQAA...
2025-12-30 22:27:27,475 - ERROR - Error processing document_id AFIJLAW3HIOMRUFSWNH54IJ3XQAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,477 - INFO - Processing document AHPUCLW72C2F6OVEAUN7OD5EYCWQ...
2025-12-30 22:27:27,479 - ERROR - Error processing document_id AHPUCLW72C2F6OVEAUN7OD5EYCWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,481 - INFO - Processing document AGGFYIQ464XNB3QDRLQNAD5H7XTQ...
2025-12-30 22:27:27,483 - ERROR - Error processing document_id AGGFYIQ464XNB3QDRLQNAD5H7XTQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,484 - INFO - Processing document AGVZWLUTG2UWO4KY7OCP23JGKZHA...
2025-12-30 22:27:27,486 - ERROR - Error processing document_id AGVZWLUTG2UWO4KY7OCP23JGKZHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,488 - INFO - Processing document AETKJ7XC7M5LALVTBUS64VBQMDLQ...
2025-12-30 22:27:27,491 - ERROR - Error processing document_id AETKJ7XC7M5LALVTBUS64VBQMDLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,492 - INFO - Processing document AGK4IGFGJCVMKCLPHTVYDMI25QFA...
2025-12-30 22:27:27,494 - ERROR - Error processing document_id AGK4IGFGJCVMKCLPHTVYDMI25QFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,496 - INFO - Processing document AG74SLCMARX4WRAIIHEDXAXYKICA...
2025-12-30 22:27:27,499 - ERROR - Error processing document_id AG74SLCMARX4WRAIIHEDXAXYKICA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,500 - INFO - Processing document AG7VPYNHYVYDF6KKEF444CIJIAKA...
2025-12-30 22:27:27,502 - ERROR - Error processing document_id AG7VPYNHYVYDF6KKEF444CIJIAKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,504 - INFO - Processing document AF5ORKGZ3R5XRY4IJLN3A3T6PB3A...
2025-12-30 22:27:27,506 - ERROR - Error processing document_id AF5ORKGZ3R5XRY4IJLN3A3T6PB3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,507 - INFO - Processing document AHXUJUPQBKYZWKJXHNMI6MPVXKRA...
2025-12-30 22:27:27,510 - ERROR - Error processing document_id AHXUJUPQBKYZWKJXHNMI6MPVXKRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,511 - INFO - Processing document AF5DH6UOPFS7Z7TVH2QSHSWABCZQ...
2025-12-30 22:27:27,514 - ERROR - Error processing document_id AF5DH6UOPFS7Z7TVH2QSHSWABCZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,516 - INFO - Processing document AE626ZTKMZHYPJX3IJLTUXHJ5QDA...
2025-12-30 22:27:27,518 - ERROR - Error processing document_id AE626ZTKMZHYPJX3IJLTUXHJ5QDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,520 - INFO - Processing document AGUVYAOIWBA6FIDGLSGHCSPFUFQQ...
2025-12-30 22:27:27,523 - ERROR - Error processing document_id AGUVYAOIWBA6FIDGLSGHCSPFUFQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,524 - INFO - Processing document AFGQ6UN5SLEZ6XSTUQ6NKVRUKHRQ...
2025-12-30 22:27:27,528 - ERROR - Error processing document_id AFGQ6UN5SLEZ6XSTUQ6NKVRUKHRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,532 - INFO - Processing document AHE5QEC5ITOUT77GQE6GMC4CUWFQ...
2025-12-30 22:27:27,536 - ERROR - Error processing document_id AHE5QEC5ITOUT77GQE6GMC4CUWFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,537 - INFO - Processing document AECQQBG6YRYCOJL2NCB2H3V6LD6Q...
2025-12-30 22:27:27,540 - ERROR - Error processing document_id AECQQBG6YRYCOJL2NCB2H3V6LD6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,541 - INFO - Processing document AFRRDFUZUM6XJJVGHKGGH2WE7FDQ...
2025-12-30 22:27:27,546 - ERROR - Error processing document_id AFRRDFUZUM6XJJVGHKGGH2WE7FDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,548 - INFO - Processing document AGTZU6GZZJWFTAS3WS4ZYFVCPLDQ...
2025-12-30 22:27:27,551 - ERROR - Error processing document_id AGTZU6GZZJWFTAS3WS4ZYFVCPLDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,552 - INFO - Processing document AG3E73Y2ITXQR27YAIAHS7HLYYKA...
2025-12-30 22:27:27,554 - ERROR - Error processing document_id AG3E73Y2ITXQR27YAIAHS7HLYYKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,556 - INFO - Processing document AFTIM5BUL5WYGMH2W26QABZI6YHA...
2025-12-30 22:27:27,558 - ERROR - Error processing document_id AFTIM5BUL5WYGMH2W26QABZI6YHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,559 - INFO - Processing document AF7KEOF6HSJOQMT5OKYBD7OGJR6A...
2025-12-30 22:27:27,562 - ERROR - Error processing document_id AF7KEOF6HSJOQMT5OKYBD7OGJR6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,564 - INFO - Processing document AEM6DIFOJFM7LR7YZWE7NWF26ELQ...
2025-12-30 22:27:27,566 - ERROR - Error processing document_id AEM6DIFOJFM7LR7YZWE7NWF26ELQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,568 - INFO - Processing document AEN4SX7AUF644S7SICXNWOTRQ6WQ...
2025-12-30 22:27:27,570 - ERROR - Error processing document_id AEN4SX7AUF644S7SICXNWOTRQ6WQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,572 - INFO - Processing document AGXX73HOQ7RSYCWLBF4OFBTLN7WQ...
2025-12-30 22:27:27,574 - ERROR - Error processing document_id AGXX73HOQ7RSYCWLBF4OFBTLN7WQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,575 - INFO - Processing document AEAOJCFOPGJZMOXEWBWIC7GFEL6Q...
2025-12-30 22:27:27,579 - ERROR - Error processing document_id AEAOJCFOPGJZMOXEWBWIC7GFEL6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,580 - INFO - Processing document AEOXNGC63NRG2OVZPG7NSJTCPZZA...
2025-12-30 22:27:27,584 - ERROR - Error processing document_id AEOXNGC63NRG2OVZPG7NSJTCPZZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,585 - INFO - Processing document AGIUG5WQNUYQCXSTO6NC5U7WTCBA...
2025-12-30 22:27:27,588 - ERROR - Error processing document_id AGIUG5WQNUYQCXSTO6NC5U7WTCBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,589 - INFO - Processing document AGDSUDAGTYL6TULLUZQI6LW6Q5LA...
2025-12-30 22:27:27,592 - ERROR - Error processing document_id AGDSUDAGTYL6TULLUZQI6LW6Q5LA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,593 - INFO - Processing document AFGB67N26ZRFLYV47EK7LZOOJ5OQ...
2025-12-30 22:27:27,596 - ERROR - Error processing document_id AFGB67N26ZRFLYV47EK7LZOOJ5OQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,598 - INFO - Processing document AGHCDSDMCEB23LYKRPOEUOR6DQWA...
2025-12-30 22:27:27,600 - ERROR - Error processing document_id AGHCDSDMCEB23LYKRPOEUOR6DQWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,601 - INFO - Processing document AFALQ3QDYUFATMPAW4QG3F7YV2OQ...
2025-12-30 22:27:27,604 - ERROR - Error processing document_id AFALQ3QDYUFATMPAW4QG3F7YV2OQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,606 - INFO - Processing document AHP2GXUTRDSMAOXVU6EL6WW4C3PQ...
2025-12-30 22:27:27,606 - ERROR - Error processing document_id AHP2GXUTRDSMAOXVU6EL6WW4C3PQ: Expecting value: line 1 column 1 (char 0)
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 44, in precreate_nodes
    reviews = json.load(f)
  File "/root/miniconda3/envs/amazon/lib/python3.14/json/__init__.py", line 298, in load
    return loads(fp.read(),
        cls=cls, object_hook=object_hook,
        parse_float=parse_float, parse_int=parse_int,
        parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
  File "/root/miniconda3/envs/amazon/lib/python3.14/json/__init__.py", line 352, in loads
    return _default_decoder.decode(s)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/json/decoder.py", line 345, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/json/decoder.py", line 363, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2025-12-30 22:27:27,608 - INFO - Processing document AFIDNJPKWQAS5TG2AE73IRW7OTGQ...
2025-12-30 22:27:27,611 - ERROR - Error processing document_id AFIDNJPKWQAS5TG2AE73IRW7OTGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,612 - INFO - Processing document AFROG6KWT4XLA7ZUOBDJSUQDJHAA...
2025-12-30 22:27:27,616 - ERROR - Error processing document_id AFROG6KWT4XLA7ZUOBDJSUQDJHAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,618 - INFO - Processing document AGVIM5THT3A3SXTZPUQCZXRQHX4A...
2025-12-30 22:27:27,620 - ERROR - Error processing document_id AGVIM5THT3A3SXTZPUQCZXRQHX4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,622 - INFO - Processing document AE7YUU7AZAAMROEZQQ6BCA2ZJU2Q...
2025-12-30 22:27:27,624 - ERROR - Error processing document_id AE7YUU7AZAAMROEZQQ6BCA2ZJU2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,625 - INFO - Processing document AFXCVVIZKGYX4MMEL2AVY5ZGCZQQ...
2025-12-30 22:27:27,630 - ERROR - Error processing document_id AFXCVVIZKGYX4MMEL2AVY5ZGCZQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,631 - INFO - Processing document AGZK7ZQ2OG65JCGG3HOIY6KOAX6A...
2025-12-30 22:27:27,633 - ERROR - Error processing document_id AGZK7ZQ2OG65JCGG3HOIY6KOAX6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,635 - INFO - Processing document AH74YZQJEUF55CYLLS5FIGGEXKLQ...
2025-12-30 22:27:27,638 - ERROR - Error processing document_id AH74YZQJEUF55CYLLS5FIGGEXKLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,640 - INFO - Processing document AEAK4E5GPG3T64LTNYMCXGOVTKNA...
2025-12-30 22:27:27,642 - ERROR - Error processing document_id AEAK4E5GPG3T64LTNYMCXGOVTKNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,644 - INFO - Processing document AFO6ITMA3UJW3TG7DMRXQ5P4HHCQ...
2025-12-30 22:27:27,647 - ERROR - Error processing document_id AFO6ITMA3UJW3TG7DMRXQ5P4HHCQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,648 - INFO - Processing document AHFD5AHZ6V6TZ5AIY542YTX62QMQ...
2025-12-30 22:27:27,651 - ERROR - Error processing document_id AHFD5AHZ6V6TZ5AIY542YTX62QMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,653 - INFO - Processing document AFAU7PESJ2FXNR6RBPJZVY566GQA...
2025-12-30 22:27:27,655 - ERROR - Error processing document_id AFAU7PESJ2FXNR6RBPJZVY566GQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,657 - INFO - Processing document AGEWLGCIPVPMLJ3DQ3TCNPDLJ2NQ...
2025-12-30 22:27:27,659 - ERROR - Error processing document_id AGEWLGCIPVPMLJ3DQ3TCNPDLJ2NQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,660 - INFO - Processing document AEB2KGGJANOW27FHME4SJ3IIFGIA...
2025-12-30 22:27:27,662 - ERROR - Error processing document_id AEB2KGGJANOW27FHME4SJ3IIFGIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,664 - INFO - Processing document AHIEDI2MZHNP7YTTZR2T22JPEDXA...
2025-12-30 22:27:27,667 - ERROR - Error processing document_id AHIEDI2MZHNP7YTTZR2T22JPEDXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,668 - INFO - Processing document AGOXSG7ZSGA5JCPP3HA3GM5M53QQ...
2025-12-30 22:27:27,671 - ERROR - Error processing document_id AGOXSG7ZSGA5JCPP3HA3GM5M53QQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,672 - INFO - Processing document AERUVDULIT26X2WWUKHF4MOCD3KQ...
2025-12-30 22:27:27,674 - ERROR - Error processing document_id AERUVDULIT26X2WWUKHF4MOCD3KQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,676 - INFO - Processing document AE3EAPQJCJJ4AIY7I7SDAWLVSZOQ...
2025-12-30 22:27:27,679 - ERROR - Error processing document_id AE3EAPQJCJJ4AIY7I7SDAWLVSZOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,680 - INFO - Processing document AHL7GFZXX7E3WMQXFZZXTXYGD5FA...
2025-12-30 22:27:27,683 - ERROR - Error processing document_id AHL7GFZXX7E3WMQXFZZXTXYGD5FA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,685 - INFO - Processing document AFGG66BFZGDGXFVUK7X7A2MT6YHA...
2025-12-30 22:27:27,687 - ERROR - Error processing document_id AFGG66BFZGDGXFVUK7X7A2MT6YHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,689 - INFO - Processing document AEGROGZ2YRIIIBD56KDLWQ7XUN4Q...
2025-12-30 22:27:27,691 - ERROR - Error processing document_id AEGROGZ2YRIIIBD56KDLWQ7XUN4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,693 - INFO - Processing document AHUU6RD27REMQBXY2CSOMKACYLZQ...
2025-12-30 22:27:27,696 - ERROR - Error processing document_id AHUU6RD27REMQBXY2CSOMKACYLZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,697 - INFO - Processing document AGAZEA4RMMZSUCSE226V2HBWCFXA...
2025-12-30 22:27:27,700 - ERROR - Error processing document_id AGAZEA4RMMZSUCSE226V2HBWCFXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,702 - INFO - Processing document AHNPLHSXTMSDKELBA22KMPNC3OGA...
2025-12-30 22:27:27,705 - ERROR - Error processing document_id AHNPLHSXTMSDKELBA22KMPNC3OGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,707 - INFO - Processing document AGRUZT5FWO6DIJESXG7OWPX7XWVQ...
2025-12-30 22:27:27,709 - ERROR - Error processing document_id AGRUZT5FWO6DIJESXG7OWPX7XWVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,710 - INFO - Processing document AH6RQJ4WEU4OVFUFZNCQYLKRJ22Q...
2025-12-30 22:27:27,715 - ERROR - Error processing document_id AH6RQJ4WEU4OVFUFZNCQYLKRJ22Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,717 - INFO - Processing document AENBSJOBQIUYW5UK3ESTAF7ECSGA...
2025-12-30 22:27:27,722 - ERROR - Error processing document_id AENBSJOBQIUYW5UK3ESTAF7ECSGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,723 - INFO - Processing document AEXECL4LEK6VKG2KJDGLIWD7XB7Q...
2025-12-30 22:27:27,725 - ERROR - Error processing document_id AEXECL4LEK6VKG2KJDGLIWD7XB7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,727 - INFO - Processing document AETUBLKBW5GFAPS4CXF574L55DQA...
2025-12-30 22:27:27,732 - ERROR - Error processing document_id AETUBLKBW5GFAPS4CXF574L55DQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,734 - INFO - Processing document AFEMRH3L23GRAQXPYWM2EHUOAB7A...
2025-12-30 22:27:27,737 - ERROR - Error processing document_id AFEMRH3L23GRAQXPYWM2EHUOAB7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,738 - INFO - Processing document AFC4C5LOJIYCCV5HAC3W46DZZP6Q...
2025-12-30 22:27:27,742 - ERROR - Error processing document_id AFC4C5LOJIYCCV5HAC3W46DZZP6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,743 - INFO - Processing document AE2T5U7KR3YPVGJZGMKW4PCNMGFA...
2025-12-30 22:27:27,746 - ERROR - Error processing document_id AE2T5U7KR3YPVGJZGMKW4PCNMGFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,747 - INFO - Processing document AELOWOWNVQ3SO7JG5HA2OZFRJAFQ...
2025-12-30 22:27:27,750 - ERROR - Error processing document_id AELOWOWNVQ3SO7JG5HA2OZFRJAFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,751 - INFO - Processing document AGGQUZ5SUOAXCYPDI6OJU63EQXVA...
2025-12-30 22:27:27,753 - ERROR - Error processing document_id AGGQUZ5SUOAXCYPDI6OJU63EQXVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,755 - INFO - Processing document AFKJYGOKOHG2JGPRUVTVGLNEOKTA...
2025-12-30 22:27:27,757 - ERROR - Error processing document_id AFKJYGOKOHG2JGPRUVTVGLNEOKTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,759 - INFO - Processing document AFWVN52MRBWOTIK7UGXBWGOY4HBA...
2025-12-30 22:27:27,761 - ERROR - Error processing document_id AFWVN52MRBWOTIK7UGXBWGOY4HBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,763 - INFO - Processing document AHUP62DKKY25MHVX5XWT52J2X3KQ...
2025-12-30 22:27:27,765 - ERROR - Error processing document_id AHUP62DKKY25MHVX5XWT52J2X3KQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,767 - INFO - Processing document AFKROS6I2DRPA3N7FJABAM7LYMCA...
2025-12-30 22:27:27,770 - ERROR - Error processing document_id AFKROS6I2DRPA3N7FJABAM7LYMCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,771 - INFO - Processing document AEYQSKUYZIOPKJ3QCFQXKSMTDN5Q...
2025-12-30 22:27:27,777 - ERROR - Error processing document_id AEYQSKUYZIOPKJ3QCFQXKSMTDN5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,778 - INFO - Processing document AFSXZRVMK6TNCI7SMA6B2FR2NLOQ...
2025-12-30 22:27:27,781 - ERROR - Error processing document_id AFSXZRVMK6TNCI7SMA6B2FR2NLOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,782 - INFO - Processing document AG4OAEALEOJ4TRA2CK33UKT4F4FQ...
2025-12-30 22:27:27,786 - ERROR - Error processing document_id AG4OAEALEOJ4TRA2CK33UKT4F4FQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,787 - INFO - Processing document AGE522R3Q7EE2UQ5VLYH4CT4VBBQ...
2025-12-30 22:27:27,790 - ERROR - Error processing document_id AGE522R3Q7EE2UQ5VLYH4CT4VBBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,792 - INFO - Processing document AG75Q2O6BSZILOD3AUGTMANKNJLQ...
2025-12-30 22:27:27,794 - ERROR - Error processing document_id AG75Q2O6BSZILOD3AUGTMANKNJLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,796 - INFO - Processing document AG37RQGYWU3Y7LIYF6YTUFWOREYA...
2025-12-30 22:27:27,799 - ERROR - Error processing document_id AG37RQGYWU3Y7LIYF6YTUFWOREYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,801 - INFO - Processing document AHXMYHPI2LLQVYHBYAXL7X76OYLA...
2025-12-30 22:27:27,803 - ERROR - Error processing document_id AHXMYHPI2LLQVYHBYAXL7X76OYLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,805 - INFO - Processing document AGQX4A4M74JBIYFVWRW3Y65I36FA...
2025-12-30 22:27:27,808 - ERROR - Error processing document_id AGQX4A4M74JBIYFVWRW3Y65I36FA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,810 - INFO - Processing document AHJLNXCPHJIJL6H3QU7RIWJJZMQA...
2025-12-30 22:27:27,812 - ERROR - Error processing document_id AHJLNXCPHJIJL6H3QU7RIWJJZMQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,814 - INFO - Processing document AGCLY2ZNMINDEQPWIXDGCL5TOMCA...
2025-12-30 22:27:27,817 - ERROR - Error processing document_id AGCLY2ZNMINDEQPWIXDGCL5TOMCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,819 - INFO - Processing document AH7FYFJJYCFZALUELNBIJGFJA32A...
2025-12-30 22:27:27,822 - ERROR - Error processing document_id AH7FYFJJYCFZALUELNBIJGFJA32A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,823 - INFO - Processing document AGFZU5HNJZ3C2V5LQ63GVXZIHBFA...
2025-12-30 22:27:27,826 - ERROR - Error processing document_id AGFZU5HNJZ3C2V5LQ63GVXZIHBFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,828 - INFO - Processing document AGFQTPB3FITFVDFYRQFLH75FSUYA...
2025-12-30 22:27:27,830 - ERROR - Error processing document_id AGFQTPB3FITFVDFYRQFLH75FSUYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,832 - INFO - Processing document AED4IST7JDJIVE5SSZU2FLZKU2NA...
2025-12-30 22:27:27,835 - ERROR - Error processing document_id AED4IST7JDJIVE5SSZU2FLZKU2NA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,837 - INFO - Processing document AFYY5VD23XXZYBUSM5GK5LV2W74Q...
2025-12-30 22:27:27,840 - ERROR - Error processing document_id AFYY5VD23XXZYBUSM5GK5LV2W74Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,842 - INFO - Processing document AH43ILG2DRPKGJIF7773IZO3QHGA...
2025-12-30 22:27:27,844 - ERROR - Error processing document_id AH43ILG2DRPKGJIF7773IZO3QHGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,846 - INFO - Processing document AF65LW2PC7JGMUOAHO4KORPG7LZA...
2025-12-30 22:27:27,848 - ERROR - Error processing document_id AF65LW2PC7JGMUOAHO4KORPG7LZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,849 - INFO - Processing document AF63G54MYDJC46WGTXVR6UXX3L7A...
2025-12-30 22:27:27,852 - ERROR - Error processing document_id AF63G54MYDJC46WGTXVR6UXX3L7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,854 - INFO - Processing document AG5QZC6WJ3MPNRZURT4DCBBTAQWQ...
2025-12-30 22:27:27,856 - ERROR - Error processing document_id AG5QZC6WJ3MPNRZURT4DCBBTAQWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,857 - INFO - Processing document AEAIYXOFA72SHDGDTFR2V3NA6TAQ...
2025-12-30 22:27:27,859 - ERROR - Error processing document_id AEAIYXOFA72SHDGDTFR2V3NA6TAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,861 - INFO - Processing document AG7KU7LSRS26JXG2ZXGI3AI37L7A...
2025-12-30 22:27:27,863 - ERROR - Error processing document_id AG7KU7LSRS26JXG2ZXGI3AI37L7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,865 - INFO - Processing document AEAP27BJWYXVNRZ7BXG2SHERBIFA...
2025-12-30 22:27:27,868 - ERROR - Error processing document_id AEAP27BJWYXVNRZ7BXG2SHERBIFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,870 - INFO - Processing document AHHTX5WNS5MPMBVC32JKJCK7IVUA...
2025-12-30 22:27:27,873 - ERROR - Error processing document_id AHHTX5WNS5MPMBVC32JKJCK7IVUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,874 - INFO - Processing document AHB5TSX45U4EY5COVOKOKTVNQ2PA...
2025-12-30 22:27:27,877 - ERROR - Error processing document_id AHB5TSX45U4EY5COVOKOKTVNQ2PA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,879 - INFO - Processing document AHNC7NKXQNEKFO2CEDTGP6FXCWDQ...
2025-12-30 22:27:27,881 - ERROR - Error processing document_id AHNC7NKXQNEKFO2CEDTGP6FXCWDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,882 - INFO - Processing document AHV6QCNBJNSGLATP56JAWJ3C4G2A...
2025-12-30 22:27:27,884 - ERROR - Error processing document_id AHV6QCNBJNSGLATP56JAWJ3C4G2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,886 - INFO - Processing document AG73BVBKUOH22USSFJA5ZWL7AKXA...
2025-12-30 22:27:27,888 - ERROR - Error processing document_id AG73BVBKUOH22USSFJA5ZWL7AKXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,890 - INFO - Processing document AFL2ZDLKD6WRY5AG35RAWWKQTJFA...
2025-12-30 22:27:27,893 - ERROR - Error processing document_id AFL2ZDLKD6WRY5AG35RAWWKQTJFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,894 - INFO - Processing document AFLGU4JUBA2ERTQCF4BBFQ7ZL62Q...
2025-12-30 22:27:27,897 - ERROR - Error processing document_id AFLGU4JUBA2ERTQCF4BBFQ7ZL62Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,899 - INFO - Processing document AFVV7CLXY43P3Z3F7SROBZATZVSQ...
2025-12-30 22:27:27,902 - ERROR - Error processing document_id AFVV7CLXY43P3Z3F7SROBZATZVSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,904 - INFO - Processing document AGZH2ST3APBDSEOUJ6MDBMGVQ4BA...
2025-12-30 22:27:27,908 - ERROR - Error processing document_id AGZH2ST3APBDSEOUJ6MDBMGVQ4BA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,910 - INFO - Processing document AHAJ4J5MISSFTGUHK4MQTBAYGO3A...
2025-12-30 22:27:27,913 - ERROR - Error processing document_id AHAJ4J5MISSFTGUHK4MQTBAYGO3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,915 - INFO - Processing document AEVF7GTEQUQAQATNH2PKZTUU6A5A...
2025-12-30 22:27:27,917 - ERROR - Error processing document_id AEVF7GTEQUQAQATNH2PKZTUU6A5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,918 - INFO - Processing document AGVOF7OSJ4BKNC5IUYD44PAHU5IA...
2025-12-30 22:27:27,921 - ERROR - Error processing document_id AGVOF7OSJ4BKNC5IUYD44PAHU5IA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,922 - INFO - Processing document AF4SB2PZVLAAJWQ3GF35ZIG5VSKA...
2025-12-30 22:27:27,925 - ERROR - Error processing document_id AF4SB2PZVLAAJWQ3GF35ZIG5VSKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,927 - INFO - Processing document AHGM555CGE3AASBGT6EEA2L3Y3KA...
2025-12-30 22:27:27,929 - ERROR - Error processing document_id AHGM555CGE3AASBGT6EEA2L3Y3KA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,931 - INFO - Processing document AGQR36E3KKM56CXIE2SLRRSOEDWQ...
2025-12-30 22:27:27,936 - ERROR - Error processing document_id AGQR36E3KKM56CXIE2SLRRSOEDWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,937 - INFO - Processing document AEARSX3FBN434QRFUMPL24UR7P3Q...
2025-12-30 22:27:27,941 - ERROR - Error processing document_id AEARSX3FBN434QRFUMPL24UR7P3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,942 - INFO - Processing document AH5J7W6RJV5H6TB57IMVKRXRRVNA...
2025-12-30 22:27:27,945 - ERROR - Error processing document_id AH5J7W6RJV5H6TB57IMVKRXRRVNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,947 - INFO - Processing document AGSJG63GSPGNDCOQFYZVFJV5VBIA...
2025-12-30 22:27:27,951 - ERROR - Error processing document_id AGSJG63GSPGNDCOQFYZVFJV5VBIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,953 - INFO - Processing document AG3M4LFA4ZK7WESDNEN5ZCOA33WQ...
2025-12-30 22:27:27,956 - ERROR - Error processing document_id AG3M4LFA4ZK7WESDNEN5ZCOA33WQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,958 - INFO - Processing document AETGSRRECMAHGHEOHULFU4PCVTOQ...
2025-12-30 22:27:27,963 - ERROR - Error processing document_id AETGSRRECMAHGHEOHULFU4PCVTOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,965 - INFO - Processing document AGBCK74AYAHWAW4SAXYB63D7IXGQ...
2025-12-30 22:27:27,969 - ERROR - Error processing document_id AGBCK74AYAHWAW4SAXYB63D7IXGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,970 - INFO - Processing document AFR532HGCCAZAJ4HNV2ONGIPUMFA...
2025-12-30 22:27:27,975 - ERROR - Error processing document_id AFR532HGCCAZAJ4HNV2ONGIPUMFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,977 - INFO - Processing document AEGLNZN5B3KTDFL4ZHCME5LFIEVQ...
2025-12-30 22:27:27,979 - ERROR - Error processing document_id AEGLNZN5B3KTDFL4ZHCME5LFIEVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,980 - INFO - Processing document AEW3YGCKRG3JAIFD45DRMJSUZ5MQ...
2025-12-30 22:27:27,983 - ERROR - Error processing document_id AEW3YGCKRG3JAIFD45DRMJSUZ5MQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,985 - INFO - Processing document AFJEODQMM36TI3QRL7DUMWUC7KWQ...
2025-12-30 22:27:27,987 - ERROR - Error processing document_id AFJEODQMM36TI3QRL7DUMWUC7KWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,988 - INFO - Processing document AEQISLFHSIZPDSAJCOJBYED7PHXA...
2025-12-30 22:27:27,991 - ERROR - Error processing document_id AEQISLFHSIZPDSAJCOJBYED7PHXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,992 - INFO - Processing document AEJIPQY4UD6ZS2ATZPPUSQYEVSQA...
2025-12-30 22:27:27,998 - ERROR - Error processing document_id AEJIPQY4UD6ZS2ATZPPUSQYEVSQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:27,999 - INFO - Processing document AGUOVV45SLJF5XLZ6QQPIRGLIVLA...
2025-12-30 22:27:28,001 - ERROR - Error processing document_id AGUOVV45SLJF5XLZ6QQPIRGLIVLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,003 - INFO - Processing document AFGMBCMV5XTWDOHBKXZYWXPXOPMA...
2025-12-30 22:27:28,006 - ERROR - Error processing document_id AFGMBCMV5XTWDOHBKXZYWXPXOPMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,007 - INFO - Processing document AE3ZNOCKADEH5ZIJRSJWY73ZE3NA...
2025-12-30 22:27:28,010 - ERROR - Error processing document_id AE3ZNOCKADEH5ZIJRSJWY73ZE3NA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,012 - INFO - Processing document AFRLFHCW5IRVRLS7WJJHRMYKJSGA...
2025-12-30 22:27:28,015 - ERROR - Error processing document_id AFRLFHCW5IRVRLS7WJJHRMYKJSGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,017 - INFO - Processing document AF74JFZHO5XE32SZNXWHEZ673UUQ...
2025-12-30 22:27:28,020 - ERROR - Error processing document_id AF74JFZHO5XE32SZNXWHEZ673UUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,021 - INFO - Processing document AHROJTB7NURG6ENWG67W5KDY5JVA...
2025-12-30 22:27:28,025 - ERROR - Error processing document_id AHROJTB7NURG6ENWG67W5KDY5JVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,027 - INFO - Processing document AEZWTPQ6DFCOM3W4JWY2WUQ7XSIQ...
2025-12-30 22:27:28,029 - ERROR - Error processing document_id AEZWTPQ6DFCOM3W4JWY2WUQ7XSIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,030 - INFO - Processing document AHME6Y36XVBFNXYSOX6LCAQPUZZQ...
2025-12-30 22:27:28,033 - ERROR - Error processing document_id AHME6Y36XVBFNXYSOX6LCAQPUZZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,034 - INFO - Processing document AH7C5ALYLATXPPSFZ4XYMKVYJJ5A...
2025-12-30 22:27:28,038 - ERROR - Error processing document_id AH7C5ALYLATXPPSFZ4XYMKVYJJ5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,040 - INFO - Processing document AFMCVDMOC4AOXTXEF42RNZQIYOGA...
2025-12-30 22:27:28,042 - ERROR - Error processing document_id AFMCVDMOC4AOXTXEF42RNZQIYOGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,043 - INFO - Processing document AHU4NFN3A47XGGNWMQBUC3AT2MBQ...
2025-12-30 22:27:28,046 - ERROR - Error processing document_id AHU4NFN3A47XGGNWMQBUC3AT2MBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,048 - INFO - Processing document AFUJ4KVERHGJMOEU5CN3ERR5YCEA...
2025-12-30 22:27:28,050 - ERROR - Error processing document_id AFUJ4KVERHGJMOEU5CN3ERR5YCEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,051 - INFO - Processing document AF6SM2ESLD534XWS7P6PFSL257CA...
2025-12-30 22:27:28,053 - ERROR - Error processing document_id AF6SM2ESLD534XWS7P6PFSL257CA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,055 - INFO - Processing document AHGDVTFSAAIENYAIAZWJG427I3EA...
2025-12-30 22:27:28,057 - ERROR - Error processing document_id AHGDVTFSAAIENYAIAZWJG427I3EA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,059 - INFO - Processing document AEVRSSUFTYBYOVAIDX26SHBWCCKA...
2025-12-30 22:27:28,062 - ERROR - Error processing document_id AEVRSSUFTYBYOVAIDX26SHBWCCKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,063 - INFO - Processing document AHIILRF3CJGYF3LXJSE4EVAIP7TA...
2025-12-30 22:27:28,066 - ERROR - Error processing document_id AHIILRF3CJGYF3LXJSE4EVAIP7TA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,067 - INFO - Processing document AHF2B6SMLWPQ4RW7FFSHZ6YJBSCA...
2025-12-30 22:27:28,077 - ERROR - Error processing document_id AHF2B6SMLWPQ4RW7FFSHZ6YJBSCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,079 - INFO - Processing document AH6IG7MXDFBAO5YGLOIKAHY3A77Q...
2025-12-30 22:27:28,083 - ERROR - Error processing document_id AH6IG7MXDFBAO5YGLOIKAHY3A77Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,084 - INFO - Processing document AFOQD7COIVG754AZOGASX4PO43YA...
2025-12-30 22:27:28,088 - ERROR - Error processing document_id AFOQD7COIVG754AZOGASX4PO43YA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,089 - INFO - Processing document AEIV6M6B6XVPFRIBTB3J4RDNMLFA...
2025-12-30 22:27:28,092 - ERROR - Error processing document_id AEIV6M6B6XVPFRIBTB3J4RDNMLFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,093 - INFO - Processing document AHWORBK2X6VTOAJSB3WBFT2Q6L6Q...
2025-12-30 22:27:28,095 - ERROR - Error processing document_id AHWORBK2X6VTOAJSB3WBFT2Q6L6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,097 - INFO - Processing document AFXCZW7XUGH562I4AXDZI52WCI3A...
2025-12-30 22:27:28,099 - ERROR - Error processing document_id AFXCZW7XUGH562I4AXDZI52WCI3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,100 - INFO - Processing document AHJLWB7JLTOM4CE2D7S72K4YYOBA...
2025-12-30 22:27:28,103 - ERROR - Error processing document_id AHJLWB7JLTOM4CE2D7S72K4YYOBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,105 - INFO - Processing document AFHV3PVYATVIKHCMSRWAQQ7EW5GA...
2025-12-30 22:27:28,107 - ERROR - Error processing document_id AFHV3PVYATVIKHCMSRWAQQ7EW5GA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,109 - INFO - Processing document AEHJK7DAT5IGG5ZN6SNLAIHEQYIQ...
2025-12-30 22:27:28,111 - ERROR - Error processing document_id AEHJK7DAT5IGG5ZN6SNLAIHEQYIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,113 - INFO - Processing document AFERANXTNMSY3OQUKFAJ6X7KIFPQ...
2025-12-30 22:27:28,119 - ERROR - Error processing document_id AFERANXTNMSY3OQUKFAJ6X7KIFPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,120 - INFO - Processing document AHXWAAV2S6SCWPM2FVB62HHYVSQA...
2025-12-30 22:27:28,123 - ERROR - Error processing document_id AHXWAAV2S6SCWPM2FVB62HHYVSQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,125 - INFO - Processing document AGRKSQT7TTMAT4BROWT3MIEOEJ4A...
2025-12-30 22:27:28,131 - ERROR - Error processing document_id AGRKSQT7TTMAT4BROWT3MIEOEJ4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,132 - INFO - Processing document AHT45PBYNFGO5CXSCMI7B4CYJNMQ...
2025-12-30 22:27:28,134 - ERROR - Error processing document_id AHT45PBYNFGO5CXSCMI7B4CYJNMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,136 - INFO - Processing document AFC4XQOS6VCTP2HLCKVFBR65I5OA...
2025-12-30 22:27:28,138 - ERROR - Error processing document_id AFC4XQOS6VCTP2HLCKVFBR65I5OA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,140 - INFO - Processing document AFQNEQRDS3NIBHOROEHE5S3N3BBA...
2025-12-30 22:27:28,142 - ERROR - Error processing document_id AFQNEQRDS3NIBHOROEHE5S3N3BBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,144 - INFO - Processing document AELVU3HQT42V3KIF5XXRRELIYQKQ...
2025-12-30 22:27:28,147 - ERROR - Error processing document_id AELVU3HQT42V3KIF5XXRRELIYQKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,149 - INFO - Processing document AFH66TPYPQ6WMVLINTBDIG7T7IYA...
2025-12-30 22:27:28,151 - ERROR - Error processing document_id AFH66TPYPQ6WMVLINTBDIG7T7IYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,153 - INFO - Processing document AHCOV6ID7GFK6ABVU35ZNEJAXT6A...
2025-12-30 22:27:28,155 - ERROR - Error processing document_id AHCOV6ID7GFK6ABVU35ZNEJAXT6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,156 - INFO - Processing document AGF542WBRFOZ3DBLBTJPNDKTQ2UA...
2025-12-30 22:27:28,158 - ERROR - Error processing document_id AGF542WBRFOZ3DBLBTJPNDKTQ2UA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,160 - INFO - Processing document AHLBJFLJSM4DQN2CIXQYO3VYJMQQ...
2025-12-30 22:27:28,163 - ERROR - Error processing document_id AHLBJFLJSM4DQN2CIXQYO3VYJMQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,165 - INFO - Processing document AF4OXJN7MOTXN5CW3OTB6U4S45TA...
2025-12-30 22:27:28,167 - ERROR - Error processing document_id AF4OXJN7MOTXN5CW3OTB6U4S45TA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,169 - INFO - Processing document AHO6FLGEPTVR76G7FYUCMZHXS4DA...
2025-12-30 22:27:28,171 - ERROR - Error processing document_id AHO6FLGEPTVR76G7FYUCMZHXS4DA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,172 - INFO - Processing document AGR4V67NQMGIWTESVZZYT4WMK5MA...
2025-12-30 22:27:28,176 - ERROR - Error processing document_id AGR4V67NQMGIWTESVZZYT4WMK5MA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,177 - INFO - Processing document AFMJZODOLXGTSJ7EOBXPGZVQZJJQ...
2025-12-30 22:27:28,180 - ERROR - Error processing document_id AFMJZODOLXGTSJ7EOBXPGZVQZJJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,182 - INFO - Processing document AHJRFB4GANZD2WSWA62TIVW6IVUA...
2025-12-30 22:27:28,184 - ERROR - Error processing document_id AHJRFB4GANZD2WSWA62TIVW6IVUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,185 - INFO - Processing document AENQ6S5QMDPQNZMVSB2NUCDOCC4A...
2025-12-30 22:27:28,188 - ERROR - Error processing document_id AENQ6S5QMDPQNZMVSB2NUCDOCC4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,190 - INFO - Processing document AHU7KB4W3KYREAGZEDOYHOZCP46A...
2025-12-30 22:27:28,193 - ERROR - Error processing document_id AHU7KB4W3KYREAGZEDOYHOZCP46A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,194 - INFO - Processing document AFXDK6ZEBQHCOUMKTAYND535OGSA...
2025-12-30 22:27:28,198 - ERROR - Error processing document_id AFXDK6ZEBQHCOUMKTAYND535OGSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,200 - INFO - Processing document AH5AO5JQJ3D2ZJPNZB5CMQCKEUHQ...
2025-12-30 22:27:28,202 - ERROR - Error processing document_id AH5AO5JQJ3D2ZJPNZB5CMQCKEUHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,204 - INFO - Processing document AFCLMS2MVNZB5DZJQBLWIDXWVYSA...
2025-12-30 22:27:28,206 - ERROR - Error processing document_id AFCLMS2MVNZB5DZJQBLWIDXWVYSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,208 - INFO - Processing document AGYVC7KVHP2AWM7BDCEYNHFA6F3Q...
2025-12-30 22:27:28,210 - ERROR - Error processing document_id AGYVC7KVHP2AWM7BDCEYNHFA6F3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,212 - INFO - Processing document AF7SOKFGGA7A2I4NCH6RUAOD6HFA...
2025-12-30 22:27:28,214 - ERROR - Error processing document_id AF7SOKFGGA7A2I4NCH6RUAOD6HFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,216 - INFO - Processing document AHJXZY3OJX6BUIVV346UY2SQUHNQ...
2025-12-30 22:27:28,218 - ERROR - Error processing document_id AHJXZY3OJX6BUIVV346UY2SQUHNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,220 - INFO - Processing document AH2XWCFNTLCT5OZBU6DI5E7VGTHA...
2025-12-30 22:27:28,222 - ERROR - Error processing document_id AH2XWCFNTLCT5OZBU6DI5E7VGTHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,224 - INFO - Processing document AFSLPLOUAJZZCHZMWCOGTTLHQZXA...
2025-12-30 22:27:28,226 - ERROR - Error processing document_id AFSLPLOUAJZZCHZMWCOGTTLHQZXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,228 - INFO - Processing document AHZPMMJMHZYEUYUU7W6CHN24IPYA...
2025-12-30 22:27:28,232 - ERROR - Error processing document_id AHZPMMJMHZYEUYUU7W6CHN24IPYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,233 - INFO - Processing document AFVUJQ4EZGPYIOTFC2GFXAZRSYEA...
2025-12-30 22:27:28,237 - ERROR - Error processing document_id AFVUJQ4EZGPYIOTFC2GFXAZRSYEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,238 - INFO - Processing document AFFZ7GXPGA57RGABH3OVWTKOII2A...
2025-12-30 22:27:28,241 - ERROR - Error processing document_id AFFZ7GXPGA57RGABH3OVWTKOII2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,242 - INFO - Processing document AE2ANSF4RJFCMWL52H5HHDD6YQMA...
2025-12-30 22:27:28,246 - ERROR - Error processing document_id AE2ANSF4RJFCMWL52H5HHDD6YQMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,248 - INFO - Processing document AHJXRWE3XOEGQ22S7ORQ5WXPK27Q...
2025-12-30 22:27:28,253 - ERROR - Error processing document_id AHJXRWE3XOEGQ22S7ORQ5WXPK27Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,255 - INFO - Processing document AGRN7P3NXUINRLMALUHN2C3NSAVA...
2025-12-30 22:27:28,258 - ERROR - Error processing document_id AGRN7P3NXUINRLMALUHN2C3NSAVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,259 - INFO - Processing document AF4KW2BX3W4B7NGTALLHIDFHZ5AA...
2025-12-30 22:27:28,261 - ERROR - Error processing document_id AF4KW2BX3W4B7NGTALLHIDFHZ5AA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,263 - INFO - Processing document AFI4WVBJCH7WKORYTD74S42NACMQ...
2025-12-30 22:27:28,265 - ERROR - Error processing document_id AFI4WVBJCH7WKORYTD74S42NACMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,267 - INFO - Processing document AHJTDF6MZUPLWUBVUT6TZOGJJV5A...
2025-12-30 22:27:28,270 - ERROR - Error processing document_id AHJTDF6MZUPLWUBVUT6TZOGJJV5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,271 - INFO - Processing document AG4G532CUH7B4F4BXQFXENT756RA...
2025-12-30 22:27:28,273 - ERROR - Error processing document_id AG4G532CUH7B4F4BXQFXENT756RA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,274 - INFO - Processing document AHB5FYXIBASG4H6HCKN4LSEPPLBA...
2025-12-30 22:27:28,277 - ERROR - Error processing document_id AHB5FYXIBASG4H6HCKN4LSEPPLBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,279 - INFO - Processing document AH4YBVO724V3BJUUFP4DBFQGNEHQ...
2025-12-30 22:27:28,282 - ERROR - Error processing document_id AH4YBVO724V3BJUUFP4DBFQGNEHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,284 - INFO - Processing document AH4PHUQB5OQ4ALDY7RQVLIC67M7A...
2025-12-30 22:27:28,287 - ERROR - Error processing document_id AH4PHUQB5OQ4ALDY7RQVLIC67M7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,289 - INFO - Processing document AFPOCQQCQ2EQYIDGYMFHLVAN6UZA...
2025-12-30 22:27:28,291 - ERROR - Error processing document_id AFPOCQQCQ2EQYIDGYMFHLVAN6UZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,293 - INFO - Processing document AHB63HJQYFDHLYNGJ4YF55R65QMQ...
2025-12-30 22:27:28,295 - ERROR - Error processing document_id AHB63HJQYFDHLYNGJ4YF55R65QMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,297 - INFO - Processing document AGZDI3CZXV6EB6YIUTLJOUCB2P4Q...
2025-12-30 22:27:28,300 - ERROR - Error processing document_id AGZDI3CZXV6EB6YIUTLJOUCB2P4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,301 - INFO - Processing document AFEURQMGWI3LXF5Y5SAFUT5CUMOA...
2025-12-30 22:27:28,304 - ERROR - Error processing document_id AFEURQMGWI3LXF5Y5SAFUT5CUMOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,305 - INFO - Processing document AGV2YOLD4L2ZE4XWOUOJYSZ7ILLQ...
2025-12-30 22:27:28,308 - ERROR - Error processing document_id AGV2YOLD4L2ZE4XWOUOJYSZ7ILLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,310 - INFO - Processing document AHVI3SUWFXE6Y4BMFC6BHVARKVQQ...
2025-12-30 22:27:28,312 - ERROR - Error processing document_id AHVI3SUWFXE6Y4BMFC6BHVARKVQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,314 - INFO - Processing document AGY3VL4UDUNFZQ4FLED6Y3QVXNYQ...
2025-12-30 22:27:28,317 - ERROR - Error processing document_id AGY3VL4UDUNFZQ4FLED6Y3QVXNYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,318 - INFO - Processing document AHQNJY73ZLZU3XIYZHV6I7KTDIUA...
2025-12-30 22:27:28,322 - ERROR - Error processing document_id AHQNJY73ZLZU3XIYZHV6I7KTDIUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,323 - INFO - Processing document AGJQCPYJGJED5X7FUXZXSN5VIYQQ...
2025-12-30 22:27:28,325 - ERROR - Error processing document_id AGJQCPYJGJED5X7FUXZXSN5VIYQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,327 - INFO - Processing document AEVFBW5NOVVLLN3YA45DUU2FNNWQ...
2025-12-30 22:27:28,329 - ERROR - Error processing document_id AEVFBW5NOVVLLN3YA45DUU2FNNWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,331 - INFO - Processing document AE6ZSPD3EZPV7NYJIDOYSU2J7RUA...
2025-12-30 22:27:28,333 - ERROR - Error processing document_id AE6ZSPD3EZPV7NYJIDOYSU2J7RUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,335 - INFO - Processing document AEWTJWYHBDUWXQJO6EGZPQ5ULNCA...
2025-12-30 22:27:28,337 - ERROR - Error processing document_id AEWTJWYHBDUWXQJO6EGZPQ5ULNCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,339 - INFO - Processing document AF2HN54O5XA2HZ2C7KMNSIXCKZLA...
2025-12-30 22:27:28,342 - ERROR - Error processing document_id AF2HN54O5XA2HZ2C7KMNSIXCKZLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,343 - INFO - Processing document AFVSSWYM7KNMX6EAFMF6HCERQJ3Q...
2025-12-30 22:27:28,345 - ERROR - Error processing document_id AFVSSWYM7KNMX6EAFMF6HCERQJ3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,347 - INFO - Processing document AFNLJA4ENTHOPTATXJDQYPGBONJA...
2025-12-30 22:27:28,350 - ERROR - Error processing document_id AFNLJA4ENTHOPTATXJDQYPGBONJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,352 - INFO - Processing document AEHGCBDKAVKSK5UW6PJQL6JJWG7A...
2025-12-30 22:27:28,356 - ERROR - Error processing document_id AEHGCBDKAVKSK5UW6PJQL6JJWG7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,357 - INFO - Processing document AGP7CJRNZKI7ZYDFQYBTNAVNOMMQ...
2025-12-30 22:27:28,360 - ERROR - Error processing document_id AGP7CJRNZKI7ZYDFQYBTNAVNOMMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,361 - INFO - Processing document AHR2KQP6BZR3NNRFFUDIN73EJWCA...
2025-12-30 22:27:28,364 - ERROR - Error processing document_id AHR2KQP6BZR3NNRFFUDIN73EJWCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,365 - INFO - Processing document AH2PAPU6UMYNZMK6P5QOFFDNLCKA...
2025-12-30 22:27:28,367 - ERROR - Error processing document_id AH2PAPU6UMYNZMK6P5QOFFDNLCKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,369 - INFO - Processing document AGZ62KRUABYJC32R3UEDLOZVZOPA...
2025-12-30 22:27:28,372 - ERROR - Error processing document_id AGZ62KRUABYJC32R3UEDLOZVZOPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,373 - INFO - Processing document AGFVXHJAH3OAYKCMXDEY3DAJXQ3A...
2025-12-30 22:27:28,377 - ERROR - Error processing document_id AGFVXHJAH3OAYKCMXDEY3DAJXQ3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,379 - INFO - Processing document AHXMHA2PB23LOXI3WB5N5CZBTTTA...
2025-12-30 22:27:28,382 - ERROR - Error processing document_id AHXMHA2PB23LOXI3WB5N5CZBTTTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,384 - INFO - Processing document AG6MAIJGKQE62RFN256TSKRCINPA...
2025-12-30 22:27:28,387 - ERROR - Error processing document_id AG6MAIJGKQE62RFN256TSKRCINPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,388 - INFO - Processing document AGVWBLNPA6DZDGRJ2GFB4JAGYQSQ...
2025-12-30 22:27:28,391 - ERROR - Error processing document_id AGVWBLNPA6DZDGRJ2GFB4JAGYQSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,393 - INFO - Processing document AHUVP4OPEYHZZWTA24SZA5ONT4UQ...
2025-12-30 22:27:28,396 - ERROR - Error processing document_id AHUVP4OPEYHZZWTA24SZA5ONT4UQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,397 - INFO - Processing document AEM3RMN355LXMSJYJHO5PGA4SWUA...
2025-12-30 22:27:28,400 - ERROR - Error processing document_id AEM3RMN355LXMSJYJHO5PGA4SWUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,402 - INFO - Processing document AE5DXOKOQKQ6N5GVF572QZRRRGLA...
2025-12-30 22:27:28,405 - ERROR - Error processing document_id AE5DXOKOQKQ6N5GVF572QZRRRGLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,407 - INFO - Processing document AHVSKOZXKLW3C4UQA3ACRWHBHFBA...
2025-12-30 22:27:28,409 - ERROR - Error processing document_id AHVSKOZXKLW3C4UQA3ACRWHBHFBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,411 - INFO - Processing document AFGV6IPXJWTOP4UIGMQ3Y4EPJ7XQ...
2025-12-30 22:27:28,413 - ERROR - Error processing document_id AFGV6IPXJWTOP4UIGMQ3Y4EPJ7XQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,415 - INFO - Processing document AE3MR2EVAQWHB7PZCHM2NMMVQBAQ...
2025-12-30 22:27:28,418 - ERROR - Error processing document_id AE3MR2EVAQWHB7PZCHM2NMMVQBAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,419 - INFO - Processing document AHD6UELVTOCVC6BXBEUMFUGDOKCQ...
2025-12-30 22:27:28,423 - ERROR - Error processing document_id AHD6UELVTOCVC6BXBEUMFUGDOKCQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,425 - INFO - Processing document AGUMBSUXRFTU3ZR76XSHQYKWZMGA...
2025-12-30 22:27:28,427 - ERROR - Error processing document_id AGUMBSUXRFTU3ZR76XSHQYKWZMGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,429 - INFO - Processing document AE25MJXNW6C5SBWOTFT3SMXIIZMQ...
2025-12-30 22:27:28,431 - ERROR - Error processing document_id AE25MJXNW6C5SBWOTFT3SMXIIZMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,433 - INFO - Processing document AGRK5PENDBXX6BZ3ZDDDYEAP4IAA...
2025-12-30 22:27:28,435 - ERROR - Error processing document_id AGRK5PENDBXX6BZ3ZDDDYEAP4IAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,437 - INFO - Processing document AF3Y5JZBLG56FLUVJDGOWGDDIF2Q...
2025-12-30 22:27:28,439 - ERROR - Error processing document_id AF3Y5JZBLG56FLUVJDGOWGDDIF2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,441 - INFO - Processing document AEXDN26YS437DFIKMOVCVYNMNIRA...
2025-12-30 22:27:28,443 - ERROR - Error processing document_id AEXDN26YS437DFIKMOVCVYNMNIRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,445 - INFO - Processing document AFLAPKCHS5SC63WIVLILLOY4YAFA...
2025-12-30 22:27:28,449 - ERROR - Error processing document_id AFLAPKCHS5SC63WIVLILLOY4YAFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,451 - INFO - Processing document AFUHSUIBCUP5UAGCMSLLB2RGIY2Q...
2025-12-30 22:27:28,456 - ERROR - Error processing document_id AFUHSUIBCUP5UAGCMSLLB2RGIY2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,458 - INFO - Processing document AG473D27NAWRW3N2RMWVYZ6ICBTA...
2025-12-30 22:27:28,460 - ERROR - Error processing document_id AG473D27NAWRW3N2RMWVYZ6ICBTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,462 - INFO - Processing document AHA5AWMIANNM2JP777HFWJWH4FYA...
2025-12-30 22:27:28,464 - ERROR - Error processing document_id AHA5AWMIANNM2JP777HFWJWH4FYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,466 - INFO - Processing document AE5KJRYPW6QG3OEPH7QVQPGEZSBQ...
2025-12-30 22:27:28,468 - ERROR - Error processing document_id AE5KJRYPW6QG3OEPH7QVQPGEZSBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,470 - INFO - Processing document AH4YVGECVECIHAVCI7QO6X54473A...
2025-12-30 22:27:28,472 - ERROR - Error processing document_id AH4YVGECVECIHAVCI7QO6X54473A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,473 - INFO - Processing document AGYHPYV5IZF4RQMLMJ3ZRIRBOH5Q...
2025-12-30 22:27:28,476 - ERROR - Error processing document_id AGYHPYV5IZF4RQMLMJ3ZRIRBOH5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,480 - INFO - Processing document AEOWTX2YPTWULWL42VN5LDB7HKPA...
2025-12-30 22:27:28,483 - ERROR - Error processing document_id AEOWTX2YPTWULWL42VN5LDB7HKPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,484 - INFO - Processing document AET4TW5LTAQY3KIXK56LZLD4XIYQ...
2025-12-30 22:27:28,487 - ERROR - Error processing document_id AET4TW5LTAQY3KIXK56LZLD4XIYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,488 - INFO - Processing document AFZL2GTKPEOGFIWLXANM6JQVSXRQ...
2025-12-30 22:27:28,491 - ERROR - Error processing document_id AFZL2GTKPEOGFIWLXANM6JQVSXRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,492 - INFO - Processing document AFJQCDA6ITIZLB7BB7I4CYXHJU2A...
2025-12-30 22:27:28,494 - ERROR - Error processing document_id AFJQCDA6ITIZLB7BB7I4CYXHJU2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,496 - INFO - Processing document AETWFNQNH3T6DFE5X66H7YDQ7UOQ...
2025-12-30 22:27:28,498 - ERROR - Error processing document_id AETWFNQNH3T6DFE5X66H7YDQ7UOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,500 - INFO - Processing document AGGL7OM65242RDX4LDJIDPUH24GQ...
2025-12-30 22:27:28,502 - ERROR - Error processing document_id AGGL7OM65242RDX4LDJIDPUH24GQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,504 - INFO - Processing document AGEX6IPOJ64D2U7VPY7RAR7UGCJA...
2025-12-30 22:27:28,506 - ERROR - Error processing document_id AGEX6IPOJ64D2U7VPY7RAR7UGCJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,507 - INFO - Processing document AGGNVC4NJ45KIJKXO2MCK72T7QOA...
2025-12-30 22:27:28,510 - ERROR - Error processing document_id AGGNVC4NJ45KIJKXO2MCK72T7QOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,511 - INFO - Processing document AFVFFO2R5RCW7KH2ENNWMS7FL36A...
2025-12-30 22:27:28,514 - ERROR - Error processing document_id AFVFFO2R5RCW7KH2ENNWMS7FL36A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,516 - INFO - Processing document AHJHSITWPAYSTJWXPEOHYOTHYGDQ...
2025-12-30 22:27:28,518 - ERROR - Error processing document_id AHJHSITWPAYSTJWXPEOHYOTHYGDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,520 - INFO - Processing document AGJMHO36BA4T2MFGABF7E36VY53A...
2025-12-30 22:27:28,523 - ERROR - Error processing document_id AGJMHO36BA4T2MFGABF7E36VY53A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,525 - INFO - Processing document AGPL54ZLAVN3ORD3GJWWAQ6S2JOA...
2025-12-30 22:27:28,528 - ERROR - Error processing document_id AGPL54ZLAVN3ORD3GJWWAQ6S2JOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,530 - INFO - Processing document AHD2JUCHOMDSV7AVHREPDIPIS4AA...
2025-12-30 22:27:28,532 - ERROR - Error processing document_id AHD2JUCHOMDSV7AVHREPDIPIS4AA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,534 - INFO - Processing document AFL7IZKOK5XU4GX5SXGEPXWNOBPQ...
2025-12-30 22:27:28,537 - ERROR - Error processing document_id AFL7IZKOK5XU4GX5SXGEPXWNOBPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,538 - INFO - Processing document AELNTZKIM4QVHNDDK625WSEADAEA...
2025-12-30 22:27:28,541 - ERROR - Error processing document_id AELNTZKIM4QVHNDDK625WSEADAEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,543 - INFO - Processing document AGU3S2K7SE62MAEDMAV64EWYU3DQ...
2025-12-30 22:27:28,545 - ERROR - Error processing document_id AGU3S2K7SE62MAEDMAV64EWYU3DQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,547 - INFO - Processing document AG2TW4TSOZQFLV2JNX35TGJTMSEQ...
2025-12-30 22:27:28,550 - ERROR - Error processing document_id AG2TW4TSOZQFLV2JNX35TGJTMSEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,551 - INFO - Processing document AH3QNNM6B5FDM4DZZH4KFUXP74XQ...
2025-12-30 22:27:28,553 - ERROR - Error processing document_id AH3QNNM6B5FDM4DZZH4KFUXP74XQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,555 - INFO - Processing document AGHKKGFZAJLQC7AR33Y23GN3I2RQ...
2025-12-30 22:27:28,557 - ERROR - Error processing document_id AGHKKGFZAJLQC7AR33Y23GN3I2RQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,559 - INFO - Processing document AF2LZKZEDQKGQMIFLQQAOIVL5BHQ...
2025-12-30 22:27:28,562 - ERROR - Error processing document_id AF2LZKZEDQKGQMIFLQQAOIVL5BHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,564 - INFO - Processing document AH6MYDC3NTRP3OMCLEXO2PNXXCEQ...
2025-12-30 22:27:28,566 - ERROR - Error processing document_id AH6MYDC3NTRP3OMCLEXO2PNXXCEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,567 - INFO - Processing document AGFJ3XSUQ74BTPJOWY5LL36CK5HQ...
2025-12-30 22:27:28,572 - ERROR - Error processing document_id AGFJ3XSUQ74BTPJOWY5LL36CK5HQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,573 - INFO - Processing document AE4ACJAS2JESKVSTFEGR77WFC2NQ...
2025-12-30 22:27:28,576 - ERROR - Error processing document_id AE4ACJAS2JESKVSTFEGR77WFC2NQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,577 - INFO - Processing document AHX5NYNCJFSG74JNGLNB64OV4NLA...
2025-12-30 22:27:28,580 - ERROR - Error processing document_id AHX5NYNCJFSG74JNGLNB64OV4NLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,582 - INFO - Processing document AHKXDPBPGC3Z7ZQVDZNMS6HOOR6Q...
2025-12-30 22:27:28,585 - ERROR - Error processing document_id AHKXDPBPGC3Z7ZQVDZNMS6HOOR6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,586 - INFO - Processing document AHO5NLQHOYOTLC56VELJ23S4O3XA...
2025-12-30 22:27:28,589 - ERROR - Error processing document_id AHO5NLQHOYOTLC56VELJ23S4O3XA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,590 - INFO - Processing document AFYDPT7M7PE4QBBZLIHGWTKTUP5Q...
2025-12-30 22:27:28,593 - ERROR - Error processing document_id AFYDPT7M7PE4QBBZLIHGWTKTUP5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,594 - INFO - Processing document AHBSAEV65SZJFHDTPWDS47KKC2YA...
2025-12-30 22:27:28,597 - ERROR - Error processing document_id AHBSAEV65SZJFHDTPWDS47KKC2YA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,598 - INFO - Processing document AEEJJHUK2QBGET2LHJGECF2SAA3Q...
2025-12-30 22:27:28,601 - ERROR - Error processing document_id AEEJJHUK2QBGET2LHJGECF2SAA3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,603 - INFO - Processing document AEB6Z2NTX7KVVKQXWHFOOSSWIL5Q...
2025-12-30 22:27:28,605 - ERROR - Error processing document_id AEB6Z2NTX7KVVKQXWHFOOSSWIL5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,607 - INFO - Processing document AH7LXSQFN4MM2TWUB2HZOLPKZRFA...
2025-12-30 22:27:28,610 - ERROR - Error processing document_id AH7LXSQFN4MM2TWUB2HZOLPKZRFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,611 - INFO - Processing document AF6VFLCMTVJCTMSATTT5NSM64SEQ...
2025-12-30 22:27:28,614 - ERROR - Error processing document_id AF6VFLCMTVJCTMSATTT5NSM64SEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,615 - INFO - Processing document AG4JDGQREZU2VL7SH3QAA6MDJKSQ...
2025-12-30 22:27:28,618 - ERROR - Error processing document_id AG4JDGQREZU2VL7SH3QAA6MDJKSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,620 - INFO - Processing document AFGCI2BHLQXQECSJVMKQH5POZO6Q...
2025-12-30 22:27:28,623 - ERROR - Error processing document_id AFGCI2BHLQXQECSJVMKQH5POZO6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,624 - INFO - Processing document AGYM5FOW3VQDZXPKZCR2O7CPGVFQ...
2025-12-30 22:27:28,627 - ERROR - Error processing document_id AGYM5FOW3VQDZXPKZCR2O7CPGVFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,629 - INFO - Processing document AERKWOGXMBKGKSEZWT5CDWRDHQ3Q...
2025-12-30 22:27:28,632 - ERROR - Error processing document_id AERKWOGXMBKGKSEZWT5CDWRDHQ3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,634 - INFO - Processing document AG7JCEMC64AM7JPATDVGP6YZOTXA...
2025-12-30 22:27:28,637 - ERROR - Error processing document_id AG7JCEMC64AM7JPATDVGP6YZOTXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,638 - INFO - Processing document AFPNZMXZYJU3GBYOK2W2KAGHRTBQ...
2025-12-30 22:27:28,641 - ERROR - Error processing document_id AFPNZMXZYJU3GBYOK2W2KAGHRTBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,642 - INFO - Processing document AEAXAJACFMXIAAH4WOHRMXPSZWFA...
2025-12-30 22:27:28,645 - ERROR - Error processing document_id AEAXAJACFMXIAAH4WOHRMXPSZWFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,646 - INFO - Processing document AHQ3ZHGTBCINHQAU2NNYFY6AYU5Q...
2025-12-30 22:27:28,648 - ERROR - Error processing document_id AHQ3ZHGTBCINHQAU2NNYFY6AYU5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,650 - INFO - Processing document AEKBUS7DE3NZ4YXJA3QKCGNDZGAQ...
2025-12-30 22:27:28,653 - ERROR - Error processing document_id AEKBUS7DE3NZ4YXJA3QKCGNDZGAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,655 - INFO - Processing document AFQDQUAB7UXDT6EMXBTJ3VUOUKFQ...
2025-12-30 22:27:28,658 - ERROR - Error processing document_id AFQDQUAB7UXDT6EMXBTJ3VUOUKFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,659 - INFO - Processing document AFVXVU4ZRGPSNAYORUXPOV7R46AA...
2025-12-30 22:27:28,662 - ERROR - Error processing document_id AFVXVU4ZRGPSNAYORUXPOV7R46AA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,663 - INFO - Processing document AHM7JJRH2NRF6EINCZ5XC7BBEJCA...
2025-12-30 22:27:28,667 - ERROR - Error processing document_id AHM7JJRH2NRF6EINCZ5XC7BBEJCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,669 - INFO - Processing document AE7ES2UA5N2JCEHUR5MZ6IU5RZ6A...
2025-12-30 22:27:28,672 - ERROR - Error processing document_id AE7ES2UA5N2JCEHUR5MZ6IU5RZ6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,673 - INFO - Processing document AFIAFLWS7GPATJDG3SSWL7AFRLEA...
2025-12-30 22:27:28,676 - ERROR - Error processing document_id AFIAFLWS7GPATJDG3SSWL7AFRLEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,678 - INFO - Processing document AF24BGD6GW3HOPVC3DCMZOLMMQ7Q...
2025-12-30 22:27:28,679 - ERROR - Error processing document_id AF24BGD6GW3HOPVC3DCMZOLMMQ7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,681 - INFO - Processing document AFVV3PU2DXKRDEJ4DO22IMOFX2UQ...
2025-12-30 22:27:28,683 - ERROR - Error processing document_id AFVV3PU2DXKRDEJ4DO22IMOFX2UQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,685 - INFO - Processing document AHCMK74OFSPCOFDB7JLGWMWWIKNA...
2025-12-30 22:27:28,688 - ERROR - Error processing document_id AHCMK74OFSPCOFDB7JLGWMWWIKNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,689 - INFO - Processing document AHPOHKN4PU4W3V5PGFL7AGTAD2AA...
2025-12-30 22:27:28,691 - ERROR - Error processing document_id AHPOHKN4PU4W3V5PGFL7AGTAD2AA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,693 - INFO - Processing document AHOGE76TY4MRPZOESSHLBO6O26GQ...
2025-12-30 22:27:28,695 - ERROR - Error processing document_id AHOGE76TY4MRPZOESSHLBO6O26GQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,697 - INFO - Processing document AHQ3KVRVRD62GDZNDQEQB6MPULOQ...
2025-12-30 22:27:28,699 - ERROR - Error processing document_id AHQ3KVRVRD62GDZNDQEQB6MPULOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,701 - INFO - Processing document AHH3SRVZRAONGSVBB6GQ2HLOHPZQ...
2025-12-30 22:27:28,703 - ERROR - Error processing document_id AHH3SRVZRAONGSVBB6GQ2HLOHPZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,705 - INFO - Processing document AEIGXZULXNTPHNKUFUZTD2PIFZXA...
2025-12-30 22:27:28,707 - ERROR - Error processing document_id AEIGXZULXNTPHNKUFUZTD2PIFZXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,709 - INFO - Processing document AFTTS6KLEA7QJDLUQVY22B2B733A...
2025-12-30 22:27:28,711 - ERROR - Error processing document_id AFTTS6KLEA7QJDLUQVY22B2B733A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,713 - INFO - Processing document AHPCFVWVVIWGYA3CJ626SEFIWJ2A...
2025-12-30 22:27:28,716 - ERROR - Error processing document_id AHPCFVWVVIWGYA3CJ626SEFIWJ2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,717 - INFO - Processing document AF5C5NIMX4CR2L2U3SJEX4IA4A2A...
2025-12-30 22:27:28,721 - ERROR - Error processing document_id AF5C5NIMX4CR2L2U3SJEX4IA4A2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,722 - INFO - Processing document AFH7WM5TULLDJQMUD6NJFSVVUBAQ...
2025-12-30 22:27:28,725 - ERROR - Error processing document_id AFH7WM5TULLDJQMUD6NJFSVVUBAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,727 - INFO - Processing document AETWF2INRCH3LK27F7XKLBJ2H33Q...
2025-12-30 22:27:28,729 - ERROR - Error processing document_id AETWF2INRCH3LK27F7XKLBJ2H33Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,730 - INFO - Processing document AHR7ZC2G4VF4MEJMXTZANR23OJ6Q...
2025-12-30 22:27:28,734 - ERROR - Error processing document_id AHR7ZC2G4VF4MEJMXTZANR23OJ6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,736 - INFO - Processing document AHGKO5AAC4VEB6I6HOD5ICB5J63Q...
2025-12-30 22:27:28,740 - ERROR - Error processing document_id AHGKO5AAC4VEB6I6HOD5ICB5J63Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,741 - INFO - Processing document AGLGX7DGPF5AUXRUA3OPADMXGPRA...
2025-12-30 22:27:28,745 - ERROR - Error processing document_id AGLGX7DGPF5AUXRUA3OPADMXGPRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,747 - INFO - Processing document AFS5WG7LHUZJVOZHPMPESFULFK3Q...
2025-12-30 22:27:28,749 - ERROR - Error processing document_id AFS5WG7LHUZJVOZHPMPESFULFK3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,751 - INFO - Processing document AFXF3EGQTQDXMRLDWFU7UBFQZB7Q...
2025-12-30 22:27:28,755 - ERROR - Error processing document_id AFXF3EGQTQDXMRLDWFU7UBFQZB7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,757 - INFO - Processing document AGVBX4FJILTJ5P6327RMYKJ54ZFA...
2025-12-30 22:27:28,760 - ERROR - Error processing document_id AGVBX4FJILTJ5P6327RMYKJ54ZFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,761 - INFO - Processing document AEY5U4ICNOZL6LY5F4N2CH2CM67Q...
2025-12-30 22:27:28,764 - ERROR - Error processing document_id AEY5U4ICNOZL6LY5F4N2CH2CM67Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,765 - INFO - Processing document AEC4LWGM32JINEZ55HRIUV3OFW2Q...
2025-12-30 22:27:28,767 - ERROR - Error processing document_id AEC4LWGM32JINEZ55HRIUV3OFW2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,769 - INFO - Processing document AGDULJ3O7D3R4H6Y3NPZ3IWWGIQA...
2025-12-30 22:27:28,771 - ERROR - Error processing document_id AGDULJ3O7D3R4H6Y3NPZ3IWWGIQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,773 - INFO - Processing document AHUN3W6JFIJEB73J4EWVRRC3IFDQ...
2025-12-30 22:27:28,778 - ERROR - Error processing document_id AHUN3W6JFIJEB73J4EWVRRC3IFDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,779 - INFO - Processing document AEM6YTC7OJLLLL7Y5DDQOXBTGOZA...
2025-12-30 22:27:28,782 - ERROR - Error processing document_id AEM6YTC7OJLLLL7Y5DDQOXBTGOZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,783 - INFO - Processing document AECMMWFRKTOFQM2KIG6Y5LU3IHEQ...
2025-12-30 22:27:28,785 - ERROR - Error processing document_id AECMMWFRKTOFQM2KIG6Y5LU3IHEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,787 - INFO - Processing document AGH4ETVLHF7WQLKSEJS627BCWZJQ...
2025-12-30 22:27:28,789 - ERROR - Error processing document_id AGH4ETVLHF7WQLKSEJS627BCWZJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,791 - INFO - Processing document AEYBWG3YAF4FHXHCPQJ2G4FJAJXA...
2025-12-30 22:27:28,794 - ERROR - Error processing document_id AEYBWG3YAF4FHXHCPQJ2G4FJAJXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,796 - INFO - Processing document AHH3A7A6TTIHSL5R2PDFB3ZVWU4A...
2025-12-30 22:27:28,800 - ERROR - Error processing document_id AHH3A7A6TTIHSL5R2PDFB3ZVWU4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,802 - INFO - Processing document AE747CPTU3RCQNGPY4OYSSKJHKQQ...
2025-12-30 22:27:28,804 - ERROR - Error processing document_id AE747CPTU3RCQNGPY4OYSSKJHKQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,806 - INFO - Processing document AECF75SEE7CNRKYFSO3LWLKNAW2A...
2025-12-30 22:27:28,808 - ERROR - Error processing document_id AECF75SEE7CNRKYFSO3LWLKNAW2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,809 - INFO - Processing document AFM6NMN4Y2BRHDTFFZHDWC7KNAKA...
2025-12-30 22:27:28,812 - ERROR - Error processing document_id AFM6NMN4Y2BRHDTFFZHDWC7KNAKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,813 - INFO - Processing document AHGGGBXEXMK7JABWORIZIGXRLRNQ...
2025-12-30 22:27:28,816 - ERROR - Error processing document_id AHGGGBXEXMK7JABWORIZIGXRLRNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,818 - INFO - Processing document AGEVDPAUU2YC4WKAX6GZLB3HWM6A...
2025-12-30 22:27:28,820 - ERROR - Error processing document_id AGEVDPAUU2YC4WKAX6GZLB3HWM6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,821 - INFO - Processing document AEYGPUCRKH7G4VM22FM3VAKSQ23Q...
2025-12-30 22:27:28,825 - ERROR - Error processing document_id AEYGPUCRKH7G4VM22FM3VAKSQ23Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,826 - INFO - Processing document AHT7FUSRNTWENIJWHS35VBU6X5CA...
2025-12-30 22:27:28,829 - ERROR - Error processing document_id AHT7FUSRNTWENIJWHS35VBU6X5CA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,831 - INFO - Processing document AFM7GDLMHM3KL6OLT364UPDI4YEA...
2025-12-30 22:27:28,834 - ERROR - Error processing document_id AFM7GDLMHM3KL6OLT364UPDI4YEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,835 - INFO - Processing document AEACCISG637S76W6CJDKPEDH4HYA...
2025-12-30 22:27:28,837 - ERROR - Error processing document_id AEACCISG637S76W6CJDKPEDH4HYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,839 - INFO - Processing document AGK6WBB4NS3NWCQQL7KQVU7XVLMA...
2025-12-30 22:27:28,841 - ERROR - Error processing document_id AGK6WBB4NS3NWCQQL7KQVU7XVLMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,843 - INFO - Processing document AFVRRXH7SFVMLOH3E72LOV7MVOEA...
2025-12-30 22:27:28,847 - ERROR - Error processing document_id AFVRRXH7SFVMLOH3E72LOV7MVOEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,848 - INFO - Processing document AFXEPFYWQMUVYXOQUZ6DTJ3CBW4A...
2025-12-30 22:27:28,851 - ERROR - Error processing document_id AFXEPFYWQMUVYXOQUZ6DTJ3CBW4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,853 - INFO - Processing document AG6WXCJ6RBOXIOU6ZU6JVSD2HRJQ...
2025-12-30 22:27:28,855 - ERROR - Error processing document_id AG6WXCJ6RBOXIOU6ZU6JVSD2HRJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,857 - INFO - Processing document AFNKLE7EH44ND5IUJHNQUJQO2LQQ...
2025-12-30 22:27:28,859 - ERROR - Error processing document_id AFNKLE7EH44ND5IUJHNQUJQO2LQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,861 - INFO - Processing document AH3TC4OTBOMAGY6D6P3VSNACBUGQ...
2025-12-30 22:27:28,863 - ERROR - Error processing document_id AH3TC4OTBOMAGY6D6P3VSNACBUGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,864 - INFO - Processing document AGQDL65BAHH322EWAPHOC6FUAIBA...
2025-12-30 22:27:28,866 - ERROR - Error processing document_id AGQDL65BAHH322EWAPHOC6FUAIBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,868 - INFO - Processing document AHTJLXSFOXVNA5WLBJZ5L6BYTHDA...
2025-12-30 22:27:28,871 - ERROR - Error processing document_id AHTJLXSFOXVNA5WLBJZ5L6BYTHDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,872 - INFO - Processing document AGAE5ACPRYSLAMTPOXYM6JZXWLKA...
2025-12-30 22:27:28,875 - ERROR - Error processing document_id AGAE5ACPRYSLAMTPOXYM6JZXWLKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,876 - INFO - Processing document AGMVZHOKKGA7PW5B35X5FZECPFUQ...
2025-12-30 22:27:28,878 - ERROR - Error processing document_id AGMVZHOKKGA7PW5B35X5FZECPFUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,880 - INFO - Processing document AERYSLQNC4KI7CMI7DJWFF324P4Q...
2025-12-30 22:27:28,883 - ERROR - Error processing document_id AERYSLQNC4KI7CMI7DJWFF324P4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,885 - INFO - Processing document AEOTH6LNFFIZOLAJ4FBVDAC6H4AA...
2025-12-30 22:27:28,888 - ERROR - Error processing document_id AEOTH6LNFFIZOLAJ4FBVDAC6H4AA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,889 - INFO - Processing document AH7VL4L4Y5NV6UZJFBABBVA35FLQ...
2025-12-30 22:27:28,892 - ERROR - Error processing document_id AH7VL4L4Y5NV6UZJFBABBVA35FLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,893 - INFO - Processing document AHA6AJCTR44XPTV2423CMNJEJQXA...
2025-12-30 22:27:28,896 - ERROR - Error processing document_id AHA6AJCTR44XPTV2423CMNJEJQXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,897 - INFO - Processing document AEHY3646YHMCXGYJ6I7SVGCPVEMQ...
2025-12-30 22:27:28,899 - ERROR - Error processing document_id AEHY3646YHMCXGYJ6I7SVGCPVEMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,901 - INFO - Processing document AESRDGGB3MCUC447D2OPIQ27OQPQ...
2025-12-30 22:27:28,903 - ERROR - Error processing document_id AESRDGGB3MCUC447D2OPIQ27OQPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,904 - INFO - Processing document AGFNWLAN7XVFFQEUOG32CPTCSKWQ...
2025-12-30 22:27:28,907 - ERROR - Error processing document_id AGFNWLAN7XVFFQEUOG32CPTCSKWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,909 - INFO - Processing document AFWOXG3JRFERL3UIJ3XYL4OD2PNA...
2025-12-30 22:27:28,911 - ERROR - Error processing document_id AFWOXG3JRFERL3UIJ3XYL4OD2PNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,913 - INFO - Processing document AFC4II37P4YXYROE4ZMGROLJZPDQ...
2025-12-30 22:27:28,916 - ERROR - Error processing document_id AFC4II37P4YXYROE4ZMGROLJZPDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,917 - INFO - Processing document AEPCSZVWVFOERFGK27XCESCFWIGA...
2025-12-30 22:27:28,920 - ERROR - Error processing document_id AEPCSZVWVFOERFGK27XCESCFWIGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,922 - INFO - Processing document AEHDBJOHAEF25PAVBTVK5MBABEYQ...
2025-12-30 22:27:28,924 - ERROR - Error processing document_id AEHDBJOHAEF25PAVBTVK5MBABEYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,926 - INFO - Processing document AHDLDCRRNGEYRY5LEXFAHVTZ46HA...
2025-12-30 22:27:28,928 - ERROR - Error processing document_id AHDLDCRRNGEYRY5LEXFAHVTZ46HA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,930 - INFO - Processing document AGAS2DGZ5KVPDVX5CRHIZCKCYJGQ...
2025-12-30 22:27:28,933 - ERROR - Error processing document_id AGAS2DGZ5KVPDVX5CRHIZCKCYJGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,935 - INFO - Processing document AFMZCUS5G5GXYZJ437VVESTONVIQ...
2025-12-30 22:27:28,939 - ERROR - Error processing document_id AFMZCUS5G5GXYZJ437VVESTONVIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,941 - INFO - Processing document AESTZFRPD6A7QPNBCMAEZDNVZT4A...
2025-12-30 22:27:28,943 - ERROR - Error processing document_id AESTZFRPD6A7QPNBCMAEZDNVZT4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,945 - INFO - Processing document AE6TDCUOKCSCRL5ZHWKEL7QHOA3A...
2025-12-30 22:27:28,947 - ERROR - Error processing document_id AE6TDCUOKCSCRL5ZHWKEL7QHOA3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,949 - INFO - Processing document AFTANVQEIAVLXHD44T52OYPDJZHQ...
2025-12-30 22:27:28,951 - ERROR - Error processing document_id AFTANVQEIAVLXHD44T52OYPDJZHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,953 - INFO - Processing document AGLWJVVVYCTF5KCFME4NVBHHCX3Q...
2025-12-30 22:27:28,956 - ERROR - Error processing document_id AGLWJVVVYCTF5KCFME4NVBHHCX3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,958 - INFO - Processing document AFQXFZ64W6Q2MRSDOJZUSDOUHXIA...
2025-12-30 22:27:28,961 - ERROR - Error processing document_id AFQXFZ64W6Q2MRSDOJZUSDOUHXIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,963 - INFO - Processing document AFETUGSA3BZ4PCXTTF73SUQFODQQ...
2025-12-30 22:27:28,966 - ERROR - Error processing document_id AFETUGSA3BZ4PCXTTF73SUQFODQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,968 - INFO - Processing document AEURGND3CRTGIKSEHZNJF5KGDW2Q...
2025-12-30 22:27:28,971 - ERROR - Error processing document_id AEURGND3CRTGIKSEHZNJF5KGDW2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,972 - INFO - Processing document AGVJFTV2OCTCWFBMNDW6ZOBVXATQ...
2025-12-30 22:27:28,974 - ERROR - Error processing document_id AGVJFTV2OCTCWFBMNDW6ZOBVXATQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,976 - INFO - Processing document AGLHEGFNVILJ5SM25XOSIR7HLMOA...
2025-12-30 22:27:28,978 - ERROR - Error processing document_id AGLHEGFNVILJ5SM25XOSIR7HLMOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,979 - INFO - Processing document AG2YJWMXWKLFZGVDPSXO4LHCA3PA...
2025-12-30 22:27:28,982 - ERROR - Error processing document_id AG2YJWMXWKLFZGVDPSXO4LHCA3PA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,984 - INFO - Processing document AERHW4GJ4YNVCCIDSFU3IGPZ5AEQ...
2025-12-30 22:27:28,986 - ERROR - Error processing document_id AERHW4GJ4YNVCCIDSFU3IGPZ5AEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,987 - INFO - Processing document AGDKFRWSE2HV2QUGKIBYZQ4PEP2A...
2025-12-30 22:27:28,990 - ERROR - Error processing document_id AGDKFRWSE2HV2QUGKIBYZQ4PEP2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,992 - INFO - Processing document AEKSN5TFTCDYXN2GD2INHTINJJNQ...
2025-12-30 22:27:28,995 - ERROR - Error processing document_id AEKSN5TFTCDYXN2GD2INHTINJJNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:28,996 - INFO - Processing document AEKVAZOYZKBHZCJWHM7U3L2HY3FQ...
2025-12-30 22:27:28,999 - ERROR - Error processing document_id AEKVAZOYZKBHZCJWHM7U3L2HY3FQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,001 - INFO - Processing document AHIJ3EKTADWQTLZ6KYQ3JBJNNAXQ...
2025-12-30 22:27:29,004 - ERROR - Error processing document_id AHIJ3EKTADWQTLZ6KYQ3JBJNNAXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,005 - INFO - Processing document AGORQW36MQIG5GVY2AUOBQSJWYPA...
2025-12-30 22:27:29,008 - ERROR - Error processing document_id AGORQW36MQIG5GVY2AUOBQSJWYPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,009 - INFO - Processing document AHJSLFGBX6N76BU6W74M5244WDIQ...
2025-12-30 22:27:29,012 - ERROR - Error processing document_id AHJSLFGBX6N76BU6W74M5244WDIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,014 - INFO - Processing document AHG5EBGIFK4GZVS6TJEZQSB7MOBQ...
2025-12-30 22:27:29,016 - ERROR - Error processing document_id AHG5EBGIFK4GZVS6TJEZQSB7MOBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,017 - INFO - Processing document AHJHV5R773UVUAGDTIIHULFY2VZQ...
2025-12-30 22:27:29,020 - ERROR - Error processing document_id AHJHV5R773UVUAGDTIIHULFY2VZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,022 - INFO - Processing document AGY3WTXVAKWR4KGCW3DJHZR5Y76A...
2025-12-30 22:27:29,023 - ERROR - Error processing document_id AGY3WTXVAKWR4KGCW3DJHZR5Y76A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,025 - INFO - Processing document AGOU6VS6GPDH3OVV2UZJWQXP6TLA...
2025-12-30 22:27:29,027 - ERROR - Error processing document_id AGOU6VS6GPDH3OVV2UZJWQXP6TLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,028 - INFO - Processing document AEBAIBBQHIRBLTRXNNZ5WJ6GAFBA...
2025-12-30 22:27:29,031 - ERROR - Error processing document_id AEBAIBBQHIRBLTRXNNZ5WJ6GAFBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,032 - INFO - Processing document AGJYSJTDBIIWS3VMAVIRRVIVMNQQ...
2025-12-30 22:27:29,035 - ERROR - Error processing document_id AGJYSJTDBIIWS3VMAVIRRVIVMNQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,037 - INFO - Processing document AH4NQPJWDUGX55NOZ2BSO2TYSQYQ...
2025-12-30 22:27:29,040 - ERROR - Error processing document_id AH4NQPJWDUGX55NOZ2BSO2TYSQYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,042 - INFO - Processing document AFN7NY2U26CGENJSZOZTV3RVRIXQ...
2025-12-30 22:27:29,044 - ERROR - Error processing document_id AFN7NY2U26CGENJSZOZTV3RVRIXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,046 - INFO - Processing document AHW76YSJHEJ6RR6XCWFQ6BH4HLZA...
2025-12-30 22:27:29,048 - ERROR - Error processing document_id AHW76YSJHEJ6RR6XCWFQ6BH4HLZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,049 - INFO - Processing document AGZZXSMMS4WRHHJRBUJZI4FZDHKQ...
2025-12-30 22:27:29,051 - ERROR - Error processing document_id AGZZXSMMS4WRHHJRBUJZI4FZDHKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,053 - INFO - Processing document AHVLOGY53HCMLTNEJZ6MHTJ2PYJQ...
2025-12-30 22:27:29,055 - ERROR - Error processing document_id AHVLOGY53HCMLTNEJZ6MHTJ2PYJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,057 - INFO - Processing document AHTPQ7DWPE5GBEZMNOCFAPXP53SQ...
2025-12-30 22:27:29,059 - ERROR - Error processing document_id AHTPQ7DWPE5GBEZMNOCFAPXP53SQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,060 - INFO - Processing document AFAOM7N3GRIKAH7MFY27AHBDIVNQ...
2025-12-30 22:27:29,064 - ERROR - Error processing document_id AFAOM7N3GRIKAH7MFY27AHBDIVNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,066 - INFO - Processing document AFDQ3R5JQM4SZZMGIDTH3JZ7G7WQ...
2025-12-30 22:27:29,068 - ERROR - Error processing document_id AFDQ3R5JQM4SZZMGIDTH3JZ7G7WQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,069 - INFO - Processing document AGND4Z2SJ3A7YKTOM6EOAOLZM25Q...
2025-12-30 22:27:29,073 - ERROR - Error processing document_id AGND4Z2SJ3A7YKTOM6EOAOLZM25Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,074 - INFO - Processing document AGP4B64EVQMT7WNXQXVGLVHHBSLQ...
2025-12-30 22:27:29,076 - ERROR - Error processing document_id AGP4B64EVQMT7WNXQXVGLVHHBSLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,078 - INFO - Processing document AG7NORBR5XEPKQ2ZTERI2IQPYOGQ...
2025-12-30 22:27:29,080 - ERROR - Error processing document_id AG7NORBR5XEPKQ2ZTERI2IQPYOGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,081 - INFO - Processing document AEBTHXVMGWA3AZJJ6O5AZJJSLYCA...
2025-12-30 22:27:29,084 - ERROR - Error processing document_id AEBTHXVMGWA3AZJJ6O5AZJJSLYCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,086 - INFO - Processing document AGJJWQXRBCHL745C3NATSDMASWWA...
2025-12-30 22:27:29,089 - ERROR - Error processing document_id AGJJWQXRBCHL745C3NATSDMASWWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,091 - INFO - Processing document AEUVCVI4VCSL4X3QSZ2V7SFIYUQQ...
2025-12-30 22:27:29,094 - ERROR - Error processing document_id AEUVCVI4VCSL4X3QSZ2V7SFIYUQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,096 - INFO - Processing document AEBIU6MFJY2UBUUKRANNJ6GWGBPQ...
2025-12-30 22:27:29,098 - ERROR - Error processing document_id AEBIU6MFJY2UBUUKRANNJ6GWGBPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,099 - INFO - Processing document AHCSNDHDLX7RGX5JYG43HDSHZFYQ...
2025-12-30 22:27:29,103 - ERROR - Error processing document_id AHCSNDHDLX7RGX5JYG43HDSHZFYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,104 - INFO - Processing document AFYS3KBCPS6L5TTARMUUQM4X5ULQ...
2025-12-30 22:27:29,106 - ERROR - Error processing document_id AFYS3KBCPS6L5TTARMUUQM4X5ULQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,108 - INFO - Processing document AFDGIDERNSQNEVRXP42DJLOUF7VQ...
2025-12-30 22:27:29,111 - ERROR - Error processing document_id AFDGIDERNSQNEVRXP42DJLOUF7VQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,112 - INFO - Processing document AEYCJ7L3CPYCHQDE27MILDXGGPLA...
2025-12-30 22:27:29,114 - ERROR - Error processing document_id AEYCJ7L3CPYCHQDE27MILDXGGPLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,116 - INFO - Processing document AFAQXADCADEJPRTEBV6YN5BQELWQ...
2025-12-30 22:27:29,118 - ERROR - Error processing document_id AFAQXADCADEJPRTEBV6YN5BQELWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,120 - INFO - Processing document AEVRCS2PW4A5LDOHWQUY65RYO6FQ...
2025-12-30 22:27:29,122 - ERROR - Error processing document_id AEVRCS2PW4A5LDOHWQUY65RYO6FQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,124 - INFO - Processing document AEJMZWFQBPXWVDXHJ7RSDRY3XOCA...
2025-12-30 22:27:29,127 - ERROR - Error processing document_id AEJMZWFQBPXWVDXHJ7RSDRY3XOCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,129 - INFO - Processing document AFNEDKFHJRVDA2YCW3OTAQOLJDCA...
2025-12-30 22:27:29,131 - ERROR - Error processing document_id AFNEDKFHJRVDA2YCW3OTAQOLJDCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,133 - INFO - Processing document AHTYRNNKHJBVAQYRO3WAOQXX44ZQ...
2025-12-30 22:27:29,135 - ERROR - Error processing document_id AHTYRNNKHJBVAQYRO3WAOQXX44ZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,137 - INFO - Processing document AFHQILLS27MRG72IMSVRF27SMOTA...
2025-12-30 22:27:29,139 - ERROR - Error processing document_id AFHQILLS27MRG72IMSVRF27SMOTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,141 - INFO - Processing document AF4A5EX4BKGDBE2IJJ2DGZOKCDQA...
2025-12-30 22:27:29,143 - ERROR - Error processing document_id AF4A5EX4BKGDBE2IJJ2DGZOKCDQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,145 - INFO - Processing document AFXLW6CIDUZQQFRGVAWUKZKJQF5Q...
2025-12-30 22:27:29,147 - ERROR - Error processing document_id AFXLW6CIDUZQQFRGVAWUKZKJQF5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,149 - INFO - Processing document AF5NZBQHOLUERYRO2CUAR76PHFLQ...
2025-12-30 22:27:29,151 - ERROR - Error processing document_id AF5NZBQHOLUERYRO2CUAR76PHFLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,153 - INFO - Processing document AHRG42QISV23C6JMCZQUMNHXE26Q...
2025-12-30 22:27:29,155 - ERROR - Error processing document_id AHRG42QISV23C6JMCZQUMNHXE26Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,156 - INFO - Processing document AH2ENAH2MUCHWBSJHZIP6GGXB4GA...
2025-12-30 22:27:29,158 - ERROR - Error processing document_id AH2ENAH2MUCHWBSJHZIP6GGXB4GA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,160 - INFO - Processing document AHPUNNPYLLQZSV56MUEOGSHANASQ...
2025-12-30 22:27:29,163 - ERROR - Error processing document_id AHPUNNPYLLQZSV56MUEOGSHANASQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,164 - INFO - Processing document AGKL4X2KXMMOLLQFDVPJKO44WTNA...
2025-12-30 22:27:29,167 - ERROR - Error processing document_id AGKL4X2KXMMOLLQFDVPJKO44WTNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,168 - INFO - Processing document AEBHJNY4EN2PRLCZMIPYZFHH3RUQ...
2025-12-30 22:27:29,172 - ERROR - Error processing document_id AEBHJNY4EN2PRLCZMIPYZFHH3RUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,173 - INFO - Processing document AGW3DUR4C3B32EOZSE3NQXW2KNEA...
2025-12-30 22:27:29,175 - ERROR - Error processing document_id AGW3DUR4C3B32EOZSE3NQXW2KNEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,177 - INFO - Processing document AHIOJEXHJBLQERB7RW755WYQFZ7Q...
2025-12-30 22:27:29,180 - ERROR - Error processing document_id AHIOJEXHJBLQERB7RW755WYQFZ7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,182 - INFO - Processing document AFEHQ4BNQV3PFLED3UKS4PHBYGMA...
2025-12-30 22:27:29,184 - ERROR - Error processing document_id AFEHQ4BNQV3PFLED3UKS4PHBYGMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,186 - INFO - Processing document AEGEPZON5NTN5H33IU63R3A56FJQ...
2025-12-30 22:27:29,189 - ERROR - Error processing document_id AEGEPZON5NTN5H33IU63R3A56FJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,190 - INFO - Processing document AHHBAGQYRP4XWVCDF6LWHCGQHOQQ...
2025-12-30 22:27:29,193 - ERROR - Error processing document_id AHHBAGQYRP4XWVCDF6LWHCGQHOQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,194 - INFO - Processing document AEZDKITNZJRZMINPYHSPKL7HEGIQ...
2025-12-30 22:27:29,196 - ERROR - Error processing document_id AEZDKITNZJRZMINPYHSPKL7HEGIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,198 - INFO - Processing document AGB443SPTKLEBARVE2DAZIINUKEQ...
2025-12-30 22:27:29,200 - ERROR - Error processing document_id AGB443SPTKLEBARVE2DAZIINUKEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,202 - INFO - Processing document AE3XHOX3OBSVFXACAPNFMKSUWR5Q...
2025-12-30 22:27:29,205 - ERROR - Error processing document_id AE3XHOX3OBSVFXACAPNFMKSUWR5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,206 - INFO - Processing document AFL3L2GRBX3AM4JQZZXSSP3AGCWQ...
2025-12-30 22:27:29,208 - ERROR - Error processing document_id AFL3L2GRBX3AM4JQZZXSSP3AGCWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,210 - INFO - Processing document AGGR5IJZ4V5ODBJEKHB3JZITTTYA...
2025-12-30 22:27:29,212 - ERROR - Error processing document_id AGGR5IJZ4V5ODBJEKHB3JZITTTYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,214 - INFO - Processing document AG2TEOMNHC7C6ABC7LN4BCGJY6AQ...
2025-12-30 22:27:29,216 - ERROR - Error processing document_id AG2TEOMNHC7C6ABC7LN4BCGJY6AQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,218 - INFO - Processing document AHXHIYNIP7PJTZAZPYL7Z7FWLSSA...
2025-12-30 22:27:29,220 - ERROR - Error processing document_id AHXHIYNIP7PJTZAZPYL7Z7FWLSSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,222 - INFO - Processing document AH3JFU65CZRULF7KCJKUXMV37XKQ...
2025-12-30 22:27:29,225 - ERROR - Error processing document_id AH3JFU65CZRULF7KCJKUXMV37XKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,226 - INFO - Processing document AFJ6HRHDMONUH5CE32QV4ECNW5QQ...
2025-12-30 22:27:29,230 - ERROR - Error processing document_id AFJ6HRHDMONUH5CE32QV4ECNW5QQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,231 - INFO - Processing document AHDVCRAJVWO63SJAHKJOPG562GGA...
2025-12-30 22:27:29,233 - ERROR - Error processing document_id AHDVCRAJVWO63SJAHKJOPG562GGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,235 - INFO - Processing document AEJNB2KJ7VFBSXAEZD74D5VS2M6Q...
2025-12-30 22:27:29,240 - ERROR - Error processing document_id AEJNB2KJ7VFBSXAEZD74D5VS2M6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,241 - INFO - Processing document AGCWIYZRLTQL56L2UAEGRRXAWD2Q...
2025-12-30 22:27:29,244 - ERROR - Error processing document_id AGCWIYZRLTQL56L2UAEGRRXAWD2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,246 - INFO - Processing document AGFVNGZJ4XEFX2ULJNDCWJYRHQVA...
2025-12-30 22:27:29,248 - ERROR - Error processing document_id AGFVNGZJ4XEFX2ULJNDCWJYRHQVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,250 - INFO - Processing document AHGNLKKFIJKADROIJYI47XH6HPLA...
2025-12-30 22:27:29,252 - ERROR - Error processing document_id AHGNLKKFIJKADROIJYI47XH6HPLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,254 - INFO - Processing document AH5XZOS67FPLGO44AVSZZY6L6OWA...
2025-12-30 22:27:29,257 - ERROR - Error processing document_id AH5XZOS67FPLGO44AVSZZY6L6OWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,259 - INFO - Processing document AHVZWILJS72IVXSIKGCUOMR6TBLQ...
2025-12-30 22:27:29,261 - ERROR - Error processing document_id AHVZWILJS72IVXSIKGCUOMR6TBLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,263 - INFO - Processing document AG6JIPV3LM3SMQ6SZSNYXQQQIL7Q...
2025-12-30 22:27:29,265 - ERROR - Error processing document_id AG6JIPV3LM3SMQ6SZSNYXQQQIL7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,267 - INFO - Processing document AHZSMWV3VT37YAUMKYRKJUTWDVAQ...
2025-12-30 22:27:29,270 - ERROR - Error processing document_id AHZSMWV3VT37YAUMKYRKJUTWDVAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,271 - INFO - Processing document AFR24O6WTLHWAC6FJX4OLVBZPZZA...
2025-12-30 22:27:29,274 - ERROR - Error processing document_id AFR24O6WTLHWAC6FJX4OLVBZPZZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,276 - INFO - Processing document AHHZFNZ7XVQU4ZA3OSWPR6Z5TWYA...
2025-12-30 22:27:29,279 - ERROR - Error processing document_id AHHZFNZ7XVQU4ZA3OSWPR6Z5TWYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,280 - INFO - Processing document AGFL37QFQSMST4NUU76NWB5KHTAA...
2025-12-30 22:27:29,284 - ERROR - Error processing document_id AGFL37QFQSMST4NUU76NWB5KHTAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,286 - INFO - Processing document AEJW2TSRVIYHQOE5WJQUD5X4MAMQ...
2025-12-30 22:27:29,289 - ERROR - Error processing document_id AEJW2TSRVIYHQOE5WJQUD5X4MAMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,291 - INFO - Processing document AGP7UZVT6IJ5A7IXB63GQKY64BZQ...
2025-12-30 22:27:29,298 - ERROR - Error processing document_id AGP7UZVT6IJ5A7IXB63GQKY64BZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,300 - INFO - Processing document AHQUMRE7ILPPWCTNA7GPWBYBEU2Q...
2025-12-30 22:27:29,303 - ERROR - Error processing document_id AHQUMRE7ILPPWCTNA7GPWBYBEU2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,304 - INFO - Processing document AGK2BNFE3YRLWRDRHRQS5VUVY4XA...
2025-12-30 22:27:29,308 - ERROR - Error processing document_id AGK2BNFE3YRLWRDRHRQS5VUVY4XA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,309 - INFO - Processing document AHNXL5JY2AP5AJPBDGM4SSUBJNKQ...
2025-12-30 22:27:29,311 - ERROR - Error processing document_id AHNXL5JY2AP5AJPBDGM4SSUBJNKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,313 - INFO - Processing document AF6B4PFCKIAY7JNDXNMN73BSD7WA...
2025-12-30 22:27:29,314 - ERROR - Error processing document_id AF6B4PFCKIAY7JNDXNMN73BSD7WA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,316 - INFO - Processing document AGCWDKIATRJJB7QSRKWICHCKWQHQ...
2025-12-30 22:27:29,320 - ERROR - Error processing document_id AGCWDKIATRJJB7QSRKWICHCKWQHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,322 - INFO - Processing document AHAHIV4SOEBVY2MQESDHP3UWS5QQ...
2025-12-30 22:27:29,324 - ERROR - Error processing document_id AHAHIV4SOEBVY2MQESDHP3UWS5QQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,326 - INFO - Processing document AERSBXUONEBBTU6A4JH3K6IZSHPA...
2025-12-30 22:27:29,328 - ERROR - Error processing document_id AERSBXUONEBBTU6A4JH3K6IZSHPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,330 - INFO - Processing document AEZZTWEQ2P3UQ2NIEIFXNKA62PPA...
2025-12-30 22:27:29,332 - ERROR - Error processing document_id AEZZTWEQ2P3UQ2NIEIFXNKA62PPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,334 - INFO - Processing document AH4XAWAJQFII6XFBFTKJBFWK2OVA...
2025-12-30 22:27:29,338 - ERROR - Error processing document_id AH4XAWAJQFII6XFBFTKJBFWK2OVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,339 - INFO - Processing document AFCOZJ664UPLDHFUIOG25ZOBGR5Q...
2025-12-30 22:27:29,341 - ERROR - Error processing document_id AFCOZJ664UPLDHFUIOG25ZOBGR5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,343 - INFO - Processing document AGDBWAKQKYGDUAVWTAXAE5AINLFQ...
2025-12-30 22:27:29,346 - ERROR - Error processing document_id AGDBWAKQKYGDUAVWTAXAE5AINLFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,348 - INFO - Processing document AEBHAAZSNVD6OPUEQTA7KJCHNXKQ...
2025-12-30 22:27:29,351 - ERROR - Error processing document_id AEBHAAZSNVD6OPUEQTA7KJCHNXKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,352 - INFO - Processing document AFI7YILO4JRFKRXQXGDBHDIYZVVQ...
2025-12-30 22:27:29,356 - ERROR - Error processing document_id AFI7YILO4JRFKRXQXGDBHDIYZVVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,358 - INFO - Processing document AFLX66DKF6R3H6OEOC3TIVAYXZIQ...
2025-12-30 22:27:29,361 - ERROR - Error processing document_id AFLX66DKF6R3H6OEOC3TIVAYXZIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,362 - INFO - Processing document AF32SBFDNCSXDYI6L77FDGOFCZ4Q...
2025-12-30 22:27:29,364 - ERROR - Error processing document_id AF32SBFDNCSXDYI6L77FDGOFCZ4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,366 - INFO - Processing document AENSUCQ2DMXTZENRPBMK6HLVAGDA...
2025-12-30 22:27:29,368 - ERROR - Error processing document_id AENSUCQ2DMXTZENRPBMK6HLVAGDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,370 - INFO - Processing document AHMIFJZEHOTAJPMH42C5KLOE2TJQ...
2025-12-30 22:27:29,372 - ERROR - Error processing document_id AHMIFJZEHOTAJPMH42C5KLOE2TJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,374 - INFO - Processing document AEK52FCBO4W7TAO5ZUEYHMZXANZA...
2025-12-30 22:27:29,377 - ERROR - Error processing document_id AEK52FCBO4W7TAO5ZUEYHMZXANZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,379 - INFO - Processing document AG4TTVN7TZCCL7XNHB5MCTPYCZFA...
2025-12-30 22:27:29,381 - ERROR - Error processing document_id AG4TTVN7TZCCL7XNHB5MCTPYCZFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,382 - INFO - Processing document AHUY4DQ5LWFEKNKCCURJERSPGRFQ...
2025-12-30 22:27:29,384 - ERROR - Error processing document_id AHUY4DQ5LWFEKNKCCURJERSPGRFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,386 - INFO - Processing document AFZ22HQOP7JDL7ZYJ6KBQW2S6QFA...
2025-12-30 22:27:29,389 - ERROR - Error processing document_id AFZ22HQOP7JDL7ZYJ6KBQW2S6QFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,390 - INFO - Processing document AFS33KNKX7EMYLXENV37EW6XUW3A...
2025-12-30 22:27:29,392 - ERROR - Error processing document_id AFS33KNKX7EMYLXENV37EW6XUW3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,394 - INFO - Processing document AFVSBBLRMF77R3DTS4IX5J5FA6ZA...
2025-12-30 22:27:29,399 - ERROR - Error processing document_id AFVSBBLRMF77R3DTS4IX5J5FA6ZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,400 - INFO - Processing document AEWL6F6ODH3GS3I62T3UGF6VEP2Q...
2025-12-30 22:27:29,404 - ERROR - Error processing document_id AEWL6F6ODH3GS3I62T3UGF6VEP2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,405 - INFO - Processing document AFVDQJZ7AMDQNKC7IIQRPSG6F2QQ...
2025-12-30 22:27:29,408 - ERROR - Error processing document_id AFVDQJZ7AMDQNKC7IIQRPSG6F2QQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,409 - INFO - Processing document AEJKSHDPUBJPE6MAP2DNJFZC7I2Q...
2025-12-30 22:27:29,411 - ERROR - Error processing document_id AEJKSHDPUBJPE6MAP2DNJFZC7I2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,413 - INFO - Processing document AGQEK3TE4QUJT7LRGVPTZBSTVQ3A...
2025-12-30 22:27:29,415 - ERROR - Error processing document_id AGQEK3TE4QUJT7LRGVPTZBSTVQ3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,417 - INFO - Processing document AHSC6EBPSXZWNOMZH4LUCABWVEZA...
2025-12-30 22:27:29,419 - ERROR - Error processing document_id AHSC6EBPSXZWNOMZH4LUCABWVEZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,420 - INFO - Processing document AHZ5AMCVZGPJV6YM46IILINMFG7Q...
2025-12-30 22:27:29,423 - ERROR - Error processing document_id AHZ5AMCVZGPJV6YM46IILINMFG7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,424 - INFO - Processing document AESPA56DVQQZHNE3CPMPVAJJVZDA...
2025-12-30 22:27:29,428 - ERROR - Error processing document_id AESPA56DVQQZHNE3CPMPVAJJVZDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,430 - INFO - Processing document AGBG2H55LSS2NH4XS5SSNEHKTMAA...
2025-12-30 22:27:29,432 - ERROR - Error processing document_id AGBG2H55LSS2NH4XS5SSNEHKTMAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,433 - INFO - Processing document AFDUE76MNHVCHP52Q36NA5CE56DA...
2025-12-30 22:27:29,436 - ERROR - Error processing document_id AFDUE76MNHVCHP52Q36NA5CE56DA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,437 - INFO - Processing document AEVRYVDRSVO5K54IXGQHR5D6X52A...
2025-12-30 22:27:29,440 - ERROR - Error processing document_id AEVRYVDRSVO5K54IXGQHR5D6X52A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,441 - INFO - Processing document AFUMQ2RPMGGRTMFZIKNMLWYROPYA...
2025-12-30 22:27:29,444 - ERROR - Error processing document_id AFUMQ2RPMGGRTMFZIKNMLWYROPYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,445 - INFO - Processing document AHBHKMKIKGGYFTNP7QDIF3G6XGQA...
2025-12-30 22:27:29,448 - ERROR - Error processing document_id AHBHKMKIKGGYFTNP7QDIF3G6XGQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,450 - INFO - Processing document AELPPIG47SNDDYX5YRLXTEBEO3RQ...
2025-12-30 22:27:29,453 - ERROR - Error processing document_id AELPPIG47SNDDYX5YRLXTEBEO3RQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,454 - INFO - Processing document AGMN2GMFHRT6LZ5IMH4BINO73NIA...
2025-12-30 22:27:29,457 - ERROR - Error processing document_id AGMN2GMFHRT6LZ5IMH4BINO73NIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,458 - INFO - Processing document AFSWAVJ3CKLATK55OMWEE66WFBVQ...
2025-12-30 22:27:29,461 - ERROR - Error processing document_id AFSWAVJ3CKLATK55OMWEE66WFBVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,463 - INFO - Processing document AHLKYQCI3Z42OK7QMG4KAVILKVOQ...
2025-12-30 22:27:29,465 - ERROR - Error processing document_id AHLKYQCI3Z42OK7QMG4KAVILKVOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,466 - INFO - Processing document AHD5JZ7A5RVZGWOFLOXNMKVJPOYA...
2025-12-30 22:27:29,468 - ERROR - Error processing document_id AHD5JZ7A5RVZGWOFLOXNMKVJPOYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,470 - INFO - Processing document AEU236XPTICUEGGX5OGRFD5XSSYA...
2025-12-30 22:27:29,474 - ERROR - Error processing document_id AEU236XPTICUEGGX5OGRFD5XSSYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,476 - INFO - Processing document AGCA64OUVM2TJGNN3IUXMEU7KF6Q...
2025-12-30 22:27:29,478 - ERROR - Error processing document_id AGCA64OUVM2TJGNN3IUXMEU7KF6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,480 - INFO - Processing document AG5H6IDZPBUWVTQ252TYAZ2EPETQ...
2025-12-30 22:27:29,483 - ERROR - Error processing document_id AG5H6IDZPBUWVTQ252TYAZ2EPETQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,485 - INFO - Processing document AHMSMFZQIIGKSIY7PKJGRHXM65GQ...
2025-12-30 22:27:29,488 - ERROR - Error processing document_id AHMSMFZQIIGKSIY7PKJGRHXM65GQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,490 - INFO - Processing document AEYY4ZZL5TDJBNZH7X2VHAUWDO5Q...
2025-12-30 22:27:29,492 - ERROR - Error processing document_id AEYY4ZZL5TDJBNZH7X2VHAUWDO5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,494 - INFO - Processing document AE6VU2732JB62WCGUA3WKJCHEEYQ...
2025-12-30 22:27:29,497 - ERROR - Error processing document_id AE6VU2732JB62WCGUA3WKJCHEEYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,498 - INFO - Processing document AFPKKBIDROYZAHLJ3X57PGZRF6JQ...
2025-12-30 22:27:29,500 - ERROR - Error processing document_id AFPKKBIDROYZAHLJ3X57PGZRF6JQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,502 - INFO - Processing document AHFYVQ5QRTI57IPEBDURHWBF252Q...
2025-12-30 22:27:29,504 - ERROR - Error processing document_id AHFYVQ5QRTI57IPEBDURHWBF252Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,506 - INFO - Processing document AGXZ6ELCUJSWJGZ4JINSX25LWMPQ...
2025-12-30 22:27:29,508 - ERROR - Error processing document_id AGXZ6ELCUJSWJGZ4JINSX25LWMPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,510 - INFO - Processing document AG3DAWJAUXTIMXBJHURX2DSVN7BA...
2025-12-30 22:27:29,514 - ERROR - Error processing document_id AG3DAWJAUXTIMXBJHURX2DSVN7BA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,515 - INFO - Processing document AEYYWOWJI352UQT56FGOZRDPLZDQ...
2025-12-30 22:27:29,517 - ERROR - Error processing document_id AEYYWOWJI352UQT56FGOZRDPLZDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,519 - INFO - Processing document AHD6WJLS7PAJ6MSG6EX4J4W2CSQA...
2025-12-30 22:27:29,521 - ERROR - Error processing document_id AHD6WJLS7PAJ6MSG6EX4J4W2CSQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,523 - INFO - Processing document AHQ5TWMBVKG2MYDVPA4CIMANX44A...
2025-12-30 22:27:29,525 - ERROR - Error processing document_id AHQ5TWMBVKG2MYDVPA4CIMANX44A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,527 - INFO - Processing document AHAOS7R725KSAMTIAULM24XOP4BA...
2025-12-30 22:27:29,529 - ERROR - Error processing document_id AHAOS7R725KSAMTIAULM24XOP4BA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,530 - INFO - Processing document AHP3QLR76ONZXMJBEA5RHAKYU3IQ...
2025-12-30 22:27:29,533 - ERROR - Error processing document_id AHP3QLR76ONZXMJBEA5RHAKYU3IQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,534 - INFO - Processing document AGKAD43XFII2G3IJ34PC7PNBHU5A...
2025-12-30 22:27:29,537 - ERROR - Error processing document_id AGKAD43XFII2G3IJ34PC7PNBHU5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,538 - INFO - Processing document AH7ZALFGQYYXVOFQ25PUE4JOQMNA...
2025-12-30 22:27:29,541 - ERROR - Error processing document_id AH7ZALFGQYYXVOFQ25PUE4JOQMNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,542 - INFO - Processing document AETZECHENB6OEZ6B46IGDBBGTF4A...
2025-12-30 22:27:29,544 - ERROR - Error processing document_id AETZECHENB6OEZ6B46IGDBBGTF4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,546 - INFO - Processing document AFPUL5RJ5MASRADXIDPWN56OERAQ...
2025-12-30 22:27:29,548 - ERROR - Error processing document_id AFPUL5RJ5MASRADXIDPWN56OERAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,550 - INFO - Processing document AFBJQJWDPLG5CN5OUDJ6VM7QP2UA...
2025-12-30 22:27:29,552 - ERROR - Error processing document_id AFBJQJWDPLG5CN5OUDJ6VM7QP2UA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,554 - INFO - Processing document AFCUBTEDDUYJ7ULIUULVK7PCFCXA...
2025-12-30 22:27:29,556 - ERROR - Error processing document_id AFCUBTEDDUYJ7ULIUULVK7PCFCXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,558 - INFO - Processing document AHPNLULNBOBQWSFUOWNITLMA2SFQ...
2025-12-30 22:27:29,560 - ERROR - Error processing document_id AHPNLULNBOBQWSFUOWNITLMA2SFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,562 - INFO - Processing document AEQNTXHJEF4VTGNBW7NPRVKXFOHQ...
2025-12-30 22:27:29,564 - ERROR - Error processing document_id AEQNTXHJEF4VTGNBW7NPRVKXFOHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,566 - INFO - Processing document AEVAETDGXQST2IFTMNCPFW75GQZQ...
2025-12-30 22:27:29,569 - ERROR - Error processing document_id AEVAETDGXQST2IFTMNCPFW75GQZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,571 - INFO - Processing document AFKAI5DF3HURXFRSGPCT3OJX527Q...
2025-12-30 22:27:29,573 - ERROR - Error processing document_id AFKAI5DF3HURXFRSGPCT3OJX527Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,575 - INFO - Processing document AFGBRQLSYKUX2ACFY24FDV65EXEQ...
2025-12-30 22:27:29,577 - ERROR - Error processing document_id AFGBRQLSYKUX2ACFY24FDV65EXEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,578 - INFO - Processing document AHONRDENWMMGSO3ODVAKHIWXIHGQ...
2025-12-30 22:27:29,581 - ERROR - Error processing document_id AHONRDENWMMGSO3ODVAKHIWXIHGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,582 - INFO - Processing document AE2UT2YIBEJRJHVURFT3BWO3INXA...
2025-12-30 22:27:29,585 - ERROR - Error processing document_id AE2UT2YIBEJRJHVURFT3BWO3INXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,586 - INFO - Processing document AF7DFPCPBRMJFNWHTZXSVM72IWQQ...
2025-12-30 22:27:29,588 - ERROR - Error processing document_id AF7DFPCPBRMJFNWHTZXSVM72IWQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,590 - INFO - Processing document AHLWVYHYZJ3CWEGX25GVQR4MOE7Q...
2025-12-30 22:27:29,592 - ERROR - Error processing document_id AHLWVYHYZJ3CWEGX25GVQR4MOE7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,594 - INFO - Processing document AFEURZCGDJTOG4PNIBM73XEXXLAQ...
2025-12-30 22:27:29,595 - ERROR - Error processing document_id AFEURZCGDJTOG4PNIBM73XEXXLAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,597 - INFO - Processing document AFVHGHJ3SGVP46IFWUQ7QW2EV3HQ...
2025-12-30 22:27:29,599 - ERROR - Error processing document_id AFVHGHJ3SGVP46IFWUQ7QW2EV3HQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,601 - INFO - Processing document AFONQ6K55TEGFZ56IABR357GIX5A...
2025-12-30 22:27:29,604 - ERROR - Error processing document_id AFONQ6K55TEGFZ56IABR357GIX5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,606 - INFO - Processing document AEOQRVILVYPLTMRW6Y4VE4N6ZZJA...
2025-12-30 22:27:29,608 - ERROR - Error processing document_id AEOQRVILVYPLTMRW6Y4VE4N6ZZJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,610 - INFO - Processing document AFCPIUKZSBSUR66KRPU7QPYTTXZA...
2025-12-30 22:27:29,613 - ERROR - Error processing document_id AFCPIUKZSBSUR66KRPU7QPYTTXZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,615 - INFO - Processing document AFV6IZT7YYSDWQLW5P6HGH7QBUIQ...
2025-12-30 22:27:29,617 - ERROR - Error processing document_id AFV6IZT7YYSDWQLW5P6HGH7QBUIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,618 - INFO - Processing document AECSHKQ3FAAINVZZL4REOOWYIJGA...
2025-12-30 22:27:29,622 - ERROR - Error processing document_id AECSHKQ3FAAINVZZL4REOOWYIJGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,624 - INFO - Processing document AEC43KVCM5KOG5PXBS2LJ7XRSUQQ...
2025-12-30 22:27:29,627 - ERROR - Error processing document_id AEC43KVCM5KOG5PXBS2LJ7XRSUQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,628 - INFO - Processing document AGZNNSMD6ZMF4LN3PTUNPBBLTIUQ...
2025-12-30 22:27:29,631 - ERROR - Error processing document_id AGZNNSMD6ZMF4LN3PTUNPBBLTIUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,632 - INFO - Processing document AHAQ3KBT7S2ZMCYKSOTCQYGHCMEQ...
2025-12-30 22:27:29,636 - ERROR - Error processing document_id AHAQ3KBT7S2ZMCYKSOTCQYGHCMEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,637 - INFO - Processing document AEXXWCSKFBOJW66ZP2RCNHACMYEQ...
2025-12-30 22:27:29,640 - ERROR - Error processing document_id AEXXWCSKFBOJW66ZP2RCNHACMYEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,641 - INFO - Processing document AHAN2NYYR2ZF2KNBXUP47NCOGAHA...
2025-12-30 22:27:29,643 - ERROR - Error processing document_id AHAN2NYYR2ZF2KNBXUP47NCOGAHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,645 - INFO - Processing document AHHRV6OZNUVOX2GYWPNYZFJZUHFQ...
2025-12-30 22:27:29,647 - ERROR - Error processing document_id AHHRV6OZNUVOX2GYWPNYZFJZUHFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,649 - INFO - Processing document AGXWFXZEKUAZLTHSLLY4JFFVQPRA...
2025-12-30 22:27:29,651 - ERROR - Error processing document_id AGXWFXZEKUAZLTHSLLY4JFFVQPRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,653 - INFO - Processing document AGV5AQXTNYEQHNMCWTWQOPNW4AVQ...
2025-12-30 22:27:29,657 - ERROR - Error processing document_id AGV5AQXTNYEQHNMCWTWQOPNW4AVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,659 - INFO - Processing document AHSUP3VDJAK3BPATGB5RHOQSOIJA...
2025-12-30 22:27:29,662 - ERROR - Error processing document_id AHSUP3VDJAK3BPATGB5RHOQSOIJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,663 - INFO - Processing document AH5DZVIHBJ2DOZSAD7WLLLXWL22Q...
2025-12-30 22:27:29,665 - ERROR - Error processing document_id AH5DZVIHBJ2DOZSAD7WLLLXWL22Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,667 - INFO - Processing document AFBQEDGOE7DKHRKZGC5JSFB4V7GA...
2025-12-30 22:27:29,670 - ERROR - Error processing document_id AFBQEDGOE7DKHRKZGC5JSFB4V7GA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,672 - INFO - Processing document AGCGEQ73PNHY26ARLDMEMOZ6756A...
2025-12-30 22:27:29,674 - ERROR - Error processing document_id AGCGEQ73PNHY26ARLDMEMOZ6756A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,675 - INFO - Processing document AGMEGA7EEBIYOHXDYXTI64I7XYRQ...
2025-12-30 22:27:29,678 - ERROR - Error processing document_id AGMEGA7EEBIYOHXDYXTI64I7XYRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,679 - INFO - Processing document AHJN3CUKWKGI5YAVNDQRC37MYIPA...
2025-12-30 22:27:29,682 - ERROR - Error processing document_id AHJN3CUKWKGI5YAVNDQRC37MYIPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,683 - INFO - Processing document AGANB4XVONOB5NSFR4IGGAZQM44A...
2025-12-30 22:27:29,688 - ERROR - Error processing document_id AGANB4XVONOB5NSFR4IGGAZQM44A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,690 - INFO - Processing document AESFQEPQRIXE4VJD2DR3QWZJHJCQ...
2025-12-30 22:27:29,692 - ERROR - Error processing document_id AESFQEPQRIXE4VJD2DR3QWZJHJCQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,694 - INFO - Processing document AGSKM6R3LGSHR45N6ZVPTEA7TMYQ...
2025-12-30 22:27:29,698 - ERROR - Error processing document_id AGSKM6R3LGSHR45N6ZVPTEA7TMYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,700 - INFO - Processing document AFXLMTGSOD3SPC7SG4IQO5BMWJNQ...
2025-12-30 22:27:29,702 - ERROR - Error processing document_id AFXLMTGSOD3SPC7SG4IQO5BMWJNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,704 - INFO - Processing document AGBTBVTZLELI3JLWTBVFV5FPTCDA...
2025-12-30 22:27:29,706 - ERROR - Error processing document_id AGBTBVTZLELI3JLWTBVFV5FPTCDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,708 - INFO - Processing document AGRPLHGW2CR6WWOHT5TOWXDGIZEQ...
2025-12-30 22:27:29,710 - ERROR - Error processing document_id AGRPLHGW2CR6WWOHT5TOWXDGIZEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,712 - INFO - Processing document AFAVSUS5767HNSYDCCPZCSEEYZNA...
2025-12-30 22:27:29,714 - ERROR - Error processing document_id AFAVSUS5767HNSYDCCPZCSEEYZNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,716 - INFO - Processing document AGMDCTXLB645UUNLM7CTYPPY4D7Q...
2025-12-30 22:27:29,717 - ERROR - Error processing document_id AGMDCTXLB645UUNLM7CTYPPY4D7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,719 - INFO - Processing document AHSDPCF5FQF3HDOESVI7AUKSU3OA...
2025-12-30 22:27:29,722 - ERROR - Error processing document_id AHSDPCF5FQF3HDOESVI7AUKSU3OA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,724 - INFO - Processing document AHOYQ263ZGBZIBPQGKNVAWG35DNQ...
2025-12-30 22:27:29,726 - ERROR - Error processing document_id AHOYQ263ZGBZIBPQGKNVAWG35DNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,728 - INFO - Processing document AEALTL7SXNWI3N37UEYGUOHY6SPQ...
2025-12-30 22:27:29,730 - ERROR - Error processing document_id AEALTL7SXNWI3N37UEYGUOHY6SPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,732 - INFO - Processing document AFOIRY7SL52ZM4RW52YP2QIDWYNQ...
2025-12-30 22:27:29,735 - ERROR - Error processing document_id AFOIRY7SL52ZM4RW52YP2QIDWYNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,736 - INFO - Processing document AET7BX44YR7CWY4O5LZHMAEFUNQQ...
2025-12-30 22:27:29,739 - ERROR - Error processing document_id AET7BX44YR7CWY4O5LZHMAEFUNQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,740 - INFO - Processing document AFLOOZQX6J62V6ZQNX73VDEEH4EA...
2025-12-30 22:27:29,742 - ERROR - Error processing document_id AFLOOZQX6J62V6ZQNX73VDEEH4EA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,744 - INFO - Processing document AFLGBVEMLLK253RZFCAKA224C4GA...
2025-12-30 22:27:29,746 - ERROR - Error processing document_id AFLGBVEMLLK253RZFCAKA224C4GA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,748 - INFO - Processing document AFWMFJB32T36XS7SC6K7MNRFSGFA...
2025-12-30 22:27:29,750 - ERROR - Error processing document_id AFWMFJB32T36XS7SC6K7MNRFSGFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,751 - INFO - Processing document AE7TOMCDC2DAY3YFLXOQZW43AMZA...
2025-12-30 22:27:29,753 - ERROR - Error processing document_id AE7TOMCDC2DAY3YFLXOQZW43AMZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,754 - INFO - Processing document AGZJVLXJ5XW6TDPRQBWHZVNDHAJA...
2025-12-30 22:27:29,756 - ERROR - Error processing document_id AGZJVLXJ5XW6TDPRQBWHZVNDHAJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,757 - INFO - Processing document AFB3YSJKSPHVACQYAP5GS7DTGNWA...
2025-12-30 22:27:29,760 - ERROR - Error processing document_id AFB3YSJKSPHVACQYAP5GS7DTGNWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,761 - INFO - Processing document AHAEDQQXVJOLHMWV6BNKWZS36YCA...
2025-12-30 22:27:29,764 - ERROR - Error processing document_id AHAEDQQXVJOLHMWV6BNKWZS36YCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,765 - INFO - Processing document AGMR4FH7RLW2VAL23PEY7B4FBDWA...
2025-12-30 22:27:29,767 - ERROR - Error processing document_id AGMR4FH7RLW2VAL23PEY7B4FBDWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,768 - INFO - Processing document AGRMPNX7H5JIR3RKGMBWNN2Z6BAA...
2025-12-30 22:27:29,770 - ERROR - Error processing document_id AGRMPNX7H5JIR3RKGMBWNN2Z6BAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,771 - INFO - Processing document AG6HO2Z3YXUXPT5C53DQ5YSOIGUA...
2025-12-30 22:27:29,774 - ERROR - Error processing document_id AG6HO2Z3YXUXPT5C53DQ5YSOIGUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,775 - INFO - Processing document AEDEGXNI55CMSJOGB7MUCO6AVU2A...
2025-12-30 22:27:29,777 - ERROR - Error processing document_id AEDEGXNI55CMSJOGB7MUCO6AVU2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,779 - INFO - Processing document AHFBBXF42H32I7EJDYKRG7AKMINQ...
2025-12-30 22:27:29,780 - ERROR - Error processing document_id AHFBBXF42H32I7EJDYKRG7AKMINQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,782 - INFO - Processing document AH2HOZETFRP2X52YG3WC3EYV5ECQ...
2025-12-30 22:27:29,783 - ERROR - Error processing document_id AH2HOZETFRP2X52YG3WC3EYV5ECQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,784 - INFO - Processing document AFP7QLM7D6L2KPSTIFVDB6UCZ5TQ...
2025-12-30 22:27:29,786 - ERROR - Error processing document_id AFP7QLM7D6L2KPSTIFVDB6UCZ5TQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,788 - INFO - Processing document AGGQ734N7F7AJFKTRVDY3SIVT2BQ...
2025-12-30 22:27:29,790 - ERROR - Error processing document_id AGGQ734N7F7AJFKTRVDY3SIVT2BQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,791 - INFO - Processing document AG4ELVUDBHS5HRYEYCM4SLQF44DQ...
2025-12-30 22:27:29,793 - ERROR - Error processing document_id AG4ELVUDBHS5HRYEYCM4SLQF44DQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,794 - INFO - Processing document AFCESNTR7NIIC2NRTZBKOF6QX2QQ...
2025-12-30 22:27:29,796 - ERROR - Error processing document_id AFCESNTR7NIIC2NRTZBKOF6QX2QQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,797 - INFO - Processing document AHYDP3ATUHOHVCWJV7KFKEOXJFZQ...
2025-12-30 22:27:29,801 - ERROR - Error processing document_id AHYDP3ATUHOHVCWJV7KFKEOXJFZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,802 - INFO - Processing document AFOCWHTKHB7VCK44LF5MFSWGJVNA...
2025-12-30 22:27:29,804 - ERROR - Error processing document_id AFOCWHTKHB7VCK44LF5MFSWGJVNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,806 - INFO - Processing document AEGYU6OM2X66SMZN6AF3TSQ3N5PQ...
2025-12-30 22:27:29,808 - ERROR - Error processing document_id AEGYU6OM2X66SMZN6AF3TSQ3N5PQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,810 - INFO - Processing document AGEQ423FYL7OPUX2EE2XVIW6EWVA...
2025-12-30 22:27:29,811 - ERROR - Error processing document_id AGEQ423FYL7OPUX2EE2XVIW6EWVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,813 - INFO - Processing document AGAN3VWXL6UMM45GV4JJSEIDIXCA...
2025-12-30 22:27:29,815 - ERROR - Error processing document_id AGAN3VWXL6UMM45GV4JJSEIDIXCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,816 - INFO - Processing document AHMHWEOJAQ3F37Q2R2DQCX4JLDSA...
2025-12-30 22:27:29,818 - ERROR - Error processing document_id AHMHWEOJAQ3F37Q2R2DQCX4JLDSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,820 - INFO - Processing document AH665SQ6SQF6DXAGYIQFCX76LALA...
2025-12-30 22:27:29,822 - ERROR - Error processing document_id AH665SQ6SQF6DXAGYIQFCX76LALA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,823 - INFO - Processing document AFIPAPBVPN3HMIYZ3NRC7HH3FUMQ...
2025-12-30 22:27:29,825 - ERROR - Error processing document_id AFIPAPBVPN3HMIYZ3NRC7HH3FUMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,826 - INFO - Processing document AG5UI5TTO7NX6ZJKYALVOBIOYNYA...
2025-12-30 22:27:29,828 - ERROR - Error processing document_id AG5UI5TTO7NX6ZJKYALVOBIOYNYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,829 - INFO - Processing document AFK2YPS7A2E7JWR6PJTNWXHYWRBA...
2025-12-30 22:27:29,832 - ERROR - Error processing document_id AFK2YPS7A2E7JWR6PJTNWXHYWRBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,833 - INFO - Processing document AGFZ7YKT53VQRKIMZLRIUIZ2JZ3A...
2025-12-30 22:27:29,835 - ERROR - Error processing document_id AGFZ7YKT53VQRKIMZLRIUIZ2JZ3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,837 - INFO - Processing document AGUGXIJLHKXL2JQWVQ32CCXDBYFA...
2025-12-30 22:27:29,839 - ERROR - Error processing document_id AGUGXIJLHKXL2JQWVQ32CCXDBYFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,840 - INFO - Processing document AHS7M6A2AAH7ACG4Z3VVTGCIT5UQ...
2025-12-30 22:27:29,842 - ERROR - Error processing document_id AHS7M6A2AAH7ACG4Z3VVTGCIT5UQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,844 - INFO - Processing document AGJMZBJ4W3PHGNYUEEWAQA3TIH3A...
2025-12-30 22:27:29,846 - ERROR - Error processing document_id AGJMZBJ4W3PHGNYUEEWAQA3TIH3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,847 - INFO - Processing document AHULHJYB5NK773NY6DG5OUDQAIHQ...
2025-12-30 22:27:29,850 - ERROR - Error processing document_id AHULHJYB5NK773NY6DG5OUDQAIHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,851 - INFO - Processing document AGKQHBJ3DPV6WC2FM4DCKK5RILZA...
2025-12-30 22:27:29,854 - ERROR - Error processing document_id AGKQHBJ3DPV6WC2FM4DCKK5RILZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,855 - INFO - Processing document AHH3GHRCSLRZYBKR2TM55XV3RH3Q...
2025-12-30 22:27:29,857 - ERROR - Error processing document_id AHH3GHRCSLRZYBKR2TM55XV3RH3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,858 - INFO - Processing document AHZMMFJLP3475ZJPJFYU2BQRWVSQ...
2025-12-30 22:27:29,861 - ERROR - Error processing document_id AHZMMFJLP3475ZJPJFYU2BQRWVSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,862 - INFO - Processing document AED7BNOFTVRZZIQ57RCVX4MWLRYA...
2025-12-30 22:27:29,864 - ERROR - Error processing document_id AED7BNOFTVRZZIQ57RCVX4MWLRYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,866 - INFO - Processing document AEY36S3ZOJMNBC4DDZLTHD5E3M6Q...
2025-12-30 22:27:29,869 - ERROR - Error processing document_id AEY36S3ZOJMNBC4DDZLTHD5E3M6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,870 - INFO - Processing document AE2UO33DBPOLVWKDHSFSFUDWKCMA...
2025-12-30 22:27:29,872 - ERROR - Error processing document_id AE2UO33DBPOLVWKDHSFSFUDWKCMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,873 - INFO - Processing document AHMCCSSYGKGDJ7WPP6TYOYR4OULQ...
2025-12-30 22:27:29,875 - ERROR - Error processing document_id AHMCCSSYGKGDJ7WPP6TYOYR4OULQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,876 - INFO - Processing document AHMSKOYKE32N5P6E7AK7PQXGPA7A...
2025-12-30 22:27:29,879 - ERROR - Error processing document_id AHMSKOYKE32N5P6E7AK7PQXGPA7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,880 - INFO - Processing document AGOMZU3MHMYQHZMVJFDEGDGMBZXA...
2025-12-30 22:27:29,882 - ERROR - Error processing document_id AGOMZU3MHMYQHZMVJFDEGDGMBZXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,883 - INFO - Processing document AEOCNRTRQ3WTNPMXNC65X4J4KNGQ...
2025-12-30 22:27:29,885 - ERROR - Error processing document_id AEOCNRTRQ3WTNPMXNC65X4J4KNGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,886 - INFO - Processing document AFUI3PPQB5WGIHVYT4P5RNGIUMEQ...
2025-12-30 22:27:29,887 - ERROR - Error processing document_id AFUI3PPQB5WGIHVYT4P5RNGIUMEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,888 - INFO - Processing document AFII27BC4YGZDC4LD77P3JCCRUYQ...
2025-12-30 22:27:29,891 - ERROR - Error processing document_id AFII27BC4YGZDC4LD77P3JCCRUYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,892 - INFO - Processing document AFO2QIGBX2JGTJDARJIACJPVIVNA...
2025-12-30 22:27:29,894 - ERROR - Error processing document_id AFO2QIGBX2JGTJDARJIACJPVIVNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,895 - INFO - Processing document AH6ZA3BA4277U6PQIJT7MCKZMKYQ...
2025-12-30 22:27:29,897 - ERROR - Error processing document_id AH6ZA3BA4277U6PQIJT7MCKZMKYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,898 - INFO - Processing document AG3MGSNYFT3V2PD5OOYIXYRP4V5Q...
2025-12-30 22:27:29,901 - ERROR - Error processing document_id AG3MGSNYFT3V2PD5OOYIXYRP4V5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,902 - INFO - Processing document AFSEPFTJPNIIEQKDPCIEQSLSFBLQ...
2025-12-30 22:27:29,905 - ERROR - Error processing document_id AFSEPFTJPNIIEQKDPCIEQSLSFBLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,906 - INFO - Processing document AE2RLSWPDFQRKKED246OZZX3NQCA...
2025-12-30 22:27:29,908 - ERROR - Error processing document_id AE2RLSWPDFQRKKED246OZZX3NQCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,909 - INFO - Processing document AHNCVWGN3T4CU7E2MYKPMJ5ZRRGQ...
2025-12-30 22:27:29,911 - ERROR - Error processing document_id AHNCVWGN3T4CU7E2MYKPMJ5ZRRGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,912 - INFO - Processing document AFT6H4AKAYEBDNRY766V3OUYR5PQ...
2025-12-30 22:27:29,914 - ERROR - Error processing document_id AFT6H4AKAYEBDNRY766V3OUYR5PQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,915 - INFO - Processing document AEOEUAOS4ABYN66NQPJ3VXI44WIA...
2025-12-30 22:27:29,918 - ERROR - Error processing document_id AEOEUAOS4ABYN66NQPJ3VXI44WIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,920 - INFO - Processing document AE5AEF6MZZ4Q6QVYQI66U2KNP6IQ...
2025-12-30 22:27:29,922 - ERROR - Error processing document_id AE5AEF6MZZ4Q6QVYQI66U2KNP6IQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,923 - INFO - Processing document AGXP6XXVUHV5BHBSDT72JPWWLX7A...
2025-12-30 22:27:29,925 - ERROR - Error processing document_id AGXP6XXVUHV5BHBSDT72JPWWLX7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,926 - INFO - Processing document AHSNVZZID3DEDJKR4HPI2UPJU5FQ...
2025-12-30 22:27:29,927 - ERROR - Error processing document_id AHSNVZZID3DEDJKR4HPI2UPJU5FQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,929 - INFO - Processing document AHTX532TEL7VZ3QMO6BWKWO2KWOA...
2025-12-30 22:27:29,931 - ERROR - Error processing document_id AHTX532TEL7VZ3QMO6BWKWO2KWOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,932 - INFO - Processing document AGUVJSWOYIJC5CE2UHFJ5RLXXCOA...
2025-12-30 22:27:29,934 - ERROR - Error processing document_id AGUVJSWOYIJC5CE2UHFJ5RLXXCOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,935 - INFO - Processing document AEVJL2SFC2VZ3IQLU7AYGPINHOLQ...
2025-12-30 22:27:29,938 - ERROR - Error processing document_id AEVJL2SFC2VZ3IQLU7AYGPINHOLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,939 - INFO - Processing document AFSO2E4ZOB2IT76TKQN3OH3WAJCA...
2025-12-30 22:27:29,941 - ERROR - Error processing document_id AFSO2E4ZOB2IT76TKQN3OH3WAJCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,943 - INFO - Processing document AF62EO7CGCWJVEPPNTI22HOZSJ7A...
2025-12-30 22:27:29,944 - ERROR - Error processing document_id AF62EO7CGCWJVEPPNTI22HOZSJ7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,946 - INFO - Processing document AGN2JK7JZ7GAPOPDW3DUY44NCILA...
2025-12-30 22:27:29,947 - ERROR - Error processing document_id AGN2JK7JZ7GAPOPDW3DUY44NCILA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,949 - INFO - Processing document AERHR5Y4VNPU2MXVIORGYLQJYPEQ...
2025-12-30 22:27:29,950 - ERROR - Error processing document_id AERHR5Y4VNPU2MXVIORGYLQJYPEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,951 - INFO - Processing document AGBRSFUTITDVXT47M3DT77L3HR4A...
2025-12-30 22:27:29,955 - ERROR - Error processing document_id AGBRSFUTITDVXT47M3DT77L3HR4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,956 - INFO - Processing document AF56VFXXVVUIXVQXMKNFO7HKLOVA...
2025-12-30 22:27:29,959 - ERROR - Error processing document_id AF56VFXXVVUIXVQXMKNFO7HKLOVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,960 - INFO - Processing document AERACBGOQVFS34IM22YEMDJZHR5A...
2025-12-30 22:27:29,962 - ERROR - Error processing document_id AERACBGOQVFS34IM22YEMDJZHR5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,964 - INFO - Processing document AGJY444VSFTXWHXZUPXITWAE724Q...
2025-12-30 22:27:29,966 - ERROR - Error processing document_id AGJY444VSFTXWHXZUPXITWAE724Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,967 - INFO - Processing document AHDAZJ4O4C4LXFMPUGNAZE77IH6Q...
2025-12-30 22:27:29,969 - ERROR - Error processing document_id AHDAZJ4O4C4LXFMPUGNAZE77IH6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,970 - INFO - Processing document AGXISSDRKCFAXZLTGY7ON7G5O5NQ...
2025-12-30 22:27:29,972 - ERROR - Error processing document_id AGXISSDRKCFAXZLTGY7ON7G5O5NQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,974 - INFO - Processing document AEY7ON3TF4F6OTSQUUT4VVVN35VQ...
2025-12-30 22:27:29,976 - ERROR - Error processing document_id AEY7ON3TF4F6OTSQUUT4VVVN35VQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,977 - INFO - Processing document AG2GVOFM2ESHP5Y6ASGI3J3RJAJQ...
2025-12-30 22:27:29,979 - ERROR - Error processing document_id AG2GVOFM2ESHP5Y6ASGI3J3RJAJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,980 - INFO - Processing document AFKNDWQZ2BLY3FCHVCMBH6IU5JWA...
2025-12-30 22:27:29,982 - ERROR - Error processing document_id AFKNDWQZ2BLY3FCHVCMBH6IU5JWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,984 - INFO - Processing document AGNSUP65FG6GKSCQJKEH7A4YZLEQ...
2025-12-30 22:27:29,986 - ERROR - Error processing document_id AGNSUP65FG6GKSCQJKEH7A4YZLEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,988 - INFO - Processing document AH6YQLUPRLG4BR26W2BM2QM2TVUQ...
2025-12-30 22:27:29,991 - ERROR - Error processing document_id AH6YQLUPRLG4BR26W2BM2QM2TVUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,992 - INFO - Processing document AED6GVZUJMKRE3XRDNJFP3CAC6SA...
2025-12-30 22:27:29,994 - ERROR - Error processing document_id AED6GVZUJMKRE3XRDNJFP3CAC6SA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,995 - INFO - Processing document AG3BNACZ43MN3EENNKIQJIHOALXQ...
2025-12-30 22:27:29,997 - ERROR - Error processing document_id AG3BNACZ43MN3EENNKIQJIHOALXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:29,998 - INFO - Processing document AHAFU3RWXWOER3FFHPVAEE22IZFQ...
2025-12-30 22:27:30,001 - ERROR - Error processing document_id AHAFU3RWXWOER3FFHPVAEE22IZFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,003 - INFO - Processing document AGZWQP7WATTWVJFS6OXSEQNVZM7Q...
2025-12-30 22:27:30,005 - ERROR - Error processing document_id AGZWQP7WATTWVJFS6OXSEQNVZM7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,007 - INFO - Processing document AHUXNBCBPZFNW2NZYTXDMUSE2CVQ...
2025-12-30 22:27:30,009 - ERROR - Error processing document_id AHUXNBCBPZFNW2NZYTXDMUSE2CVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,011 - INFO - Processing document AEQDW3LUZCRBFMK2C4YPHHTFLNPQ...
2025-12-30 22:27:30,013 - ERROR - Error processing document_id AEQDW3LUZCRBFMK2C4YPHHTFLNPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,015 - INFO - Processing document AE4XJYGTAA5XFEGYSWLOEHAP2ROQ...
2025-12-30 22:27:30,017 - ERROR - Error processing document_id AE4XJYGTAA5XFEGYSWLOEHAP2ROQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,018 - INFO - Processing document AHMRXC5GFORXHLEPE43FGI6Y5MYA...
2025-12-30 22:27:30,020 - ERROR - Error processing document_id AHMRXC5GFORXHLEPE43FGI6Y5MYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,022 - INFO - Processing document AEAXJTNU6ILYLPKOVFQBIZAJNMJQ...
2025-12-30 22:27:30,024 - ERROR - Error processing document_id AEAXJTNU6ILYLPKOVFQBIZAJNMJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,026 - INFO - Processing document AEOKTI2H3TFICTMCFHPSO2BU24IQ...
2025-12-30 22:27:30,028 - ERROR - Error processing document_id AEOKTI2H3TFICTMCFHPSO2BU24IQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,030 - INFO - Processing document AHOAXR7BKLNSQQLNBL6WXVYP4G7A...
2025-12-30 22:27:30,033 - ERROR - Error processing document_id AHOAXR7BKLNSQQLNBL6WXVYP4G7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,035 - INFO - Processing document AE4NCPHTOMQS3KJS4KQLJVCLXEFA...
2025-12-30 22:27:30,039 - ERROR - Error processing document_id AE4NCPHTOMQS3KJS4KQLJVCLXEFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,041 - INFO - Processing document AFTTJMIV7F6YA2XNSKRA4JAUPGQA...
2025-12-30 22:27:30,044 - ERROR - Error processing document_id AFTTJMIV7F6YA2XNSKRA4JAUPGQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,045 - INFO - Processing document AH7S7X7XC762Q3TDZZT3RW2ZV7CA...
2025-12-30 22:27:30,048 - ERROR - Error processing document_id AH7S7X7XC762Q3TDZZT3RW2ZV7CA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,050 - INFO - Processing document AEZEFRB2AQ2EXQ3ZKPLAB7SSL6QA...
2025-12-30 22:27:30,053 - ERROR - Error processing document_id AEZEFRB2AQ2EXQ3ZKPLAB7SSL6QA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,054 - INFO - Processing document AHOCCVQGTWKTUVIDFK6DT7KPN6IQ...
2025-12-30 22:27:30,057 - ERROR - Error processing document_id AHOCCVQGTWKTUVIDFK6DT7KPN6IQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,058 - INFO - Processing document AHZVLPZINWT4GJNNK3YTEYLFPL6A...
2025-12-30 22:27:30,064 - ERROR - Error processing document_id AHZVLPZINWT4GJNNK3YTEYLFPL6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,065 - INFO - Processing document AFILVDDXN2HA3ICZOO7HUKKQWCWA...
2025-12-30 22:27:30,067 - ERROR - Error processing document_id AFILVDDXN2HA3ICZOO7HUKKQWCWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,069 - INFO - Processing document AEZEPIRF75CJDW4NHPFJOT3GX3GA...
2025-12-30 22:27:30,071 - ERROR - Error processing document_id AEZEPIRF75CJDW4NHPFJOT3GX3GA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,073 - INFO - Processing document AE6ADJORST74NWTPHZTYJV5IDUCQ...
2025-12-30 22:27:30,076 - ERROR - Error processing document_id AE6ADJORST74NWTPHZTYJV5IDUCQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,077 - INFO - Processing document AGS73LXVWGIRRYIR6DEA56ABZS7A...
2025-12-30 22:27:30,079 - ERROR - Error processing document_id AGS73LXVWGIRRYIR6DEA56ABZS7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,081 - INFO - Processing document AFFF4RLO2XJJ4YKGEIJPESXE5I2A...
2025-12-30 22:27:30,084 - ERROR - Error processing document_id AFFF4RLO2XJJ4YKGEIJPESXE5I2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,086 - INFO - Processing document AFS3I2BN5S3EIXK2FWH2MPTAVQPA...
2025-12-30 22:27:30,089 - ERROR - Error processing document_id AFS3I2BN5S3EIXK2FWH2MPTAVQPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,090 - INFO - Processing document AHR4R5R7SFCEMNVWWR2ZWE5N47AQ...
2025-12-30 22:27:30,092 - ERROR - Error processing document_id AHR4R5R7SFCEMNVWWR2ZWE5N47AQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,094 - INFO - Processing document AFKPJBMTXAKCUZFLGOYPZMOBZ4SQ...
2025-12-30 22:27:30,098 - ERROR - Error processing document_id AFKPJBMTXAKCUZFLGOYPZMOBZ4SQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,099 - INFO - Processing document AG6BMNG2JMPL5MLSBPNG42TL3XAQ...
2025-12-30 22:27:30,102 - ERROR - Error processing document_id AG6BMNG2JMPL5MLSBPNG42TL3XAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,104 - INFO - Processing document AE3TGLPVBPR2KSOGYZUKPKFNQSHA...
2025-12-30 22:27:30,108 - ERROR - Error processing document_id AE3TGLPVBPR2KSOGYZUKPKFNQSHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,109 - INFO - Processing document AEI6UAIIWSNUZEBIFXZ4G5PYEEOA...
2025-12-30 22:27:30,111 - ERROR - Error processing document_id AEI6UAIIWSNUZEBIFXZ4G5PYEEOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,113 - INFO - Processing document AGQRYJZDMS2C24XNBPMW4UEQP6PA...
2025-12-30 22:27:30,116 - ERROR - Error processing document_id AGQRYJZDMS2C24XNBPMW4UEQP6PA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,117 - INFO - Processing document AFYNT5QHZOSA7ZZY4IAAOQPU6O6A...
2025-12-30 22:27:30,119 - ERROR - Error processing document_id AFYNT5QHZOSA7ZZY4IAAOQPU6O6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,121 - INFO - Processing document AE76BUKSWUWMRIXDKUAUJA66JPZQ...
2025-12-30 22:27:30,123 - ERROR - Error processing document_id AE76BUKSWUWMRIXDKUAUJA66JPZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,125 - INFO - Processing document AH5NVZG4PXYYBA2SFDYTKEUQ7DJQ...
2025-12-30 22:27:30,128 - ERROR - Error processing document_id AH5NVZG4PXYYBA2SFDYTKEUQ7DJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,129 - INFO - Processing document AHEJNTJ4QV7NZAFIUNBYV6WXAPIA...
2025-12-30 22:27:30,131 - ERROR - Error processing document_id AHEJNTJ4QV7NZAFIUNBYV6WXAPIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,133 - INFO - Processing document AEW36YAFZIEZ225BUOFWZMGP5ZWQ...
2025-12-30 22:27:30,135 - ERROR - Error processing document_id AEW36YAFZIEZ225BUOFWZMGP5ZWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,137 - INFO - Processing document AGHGJ3YAYSJFOKVHV4WMBFLYRQNA...
2025-12-30 22:27:30,139 - ERROR - Error processing document_id AGHGJ3YAYSJFOKVHV4WMBFLYRQNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,140 - INFO - Processing document AGKVXFICEZUXX3FRPNCNJBUCMXLQ...
2025-12-30 22:27:30,143 - ERROR - Error processing document_id AGKVXFICEZUXX3FRPNCNJBUCMXLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,144 - INFO - Processing document AGZINKF2CUZKVTW5GN77IPT5DL7A...
2025-12-30 22:27:30,147 - ERROR - Error processing document_id AGZINKF2CUZKVTW5GN77IPT5DL7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,148 - INFO - Processing document AEDQFDIYJV6IZSINRCKGWGYAR5AA...
2025-12-30 22:27:30,151 - ERROR - Error processing document_id AEDQFDIYJV6IZSINRCKGWGYAR5AA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,152 - INFO - Processing document AEZAWB33IGGB6DL5ZZGQLK4LQ2VQ...
2025-12-30 22:27:30,155 - ERROR - Error processing document_id AEZAWB33IGGB6DL5ZZGQLK4LQ2VQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,157 - INFO - Processing document AFHMOLHTSEULOYAE6GTWCC2DXCSQ...
2025-12-30 22:27:30,159 - ERROR - Error processing document_id AFHMOLHTSEULOYAE6GTWCC2DXCSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,160 - INFO - Processing document AEXQSEBICCFHTPK5LRVJXNHLLVOA...
2025-12-30 22:27:30,162 - ERROR - Error processing document_id AEXQSEBICCFHTPK5LRVJXNHLLVOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,164 - INFO - Processing document AGYEUWECWULYH4TFRYYSLKWHK66Q...
2025-12-30 22:27:30,167 - ERROR - Error processing document_id AGYEUWECWULYH4TFRYYSLKWHK66Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,169 - INFO - Processing document AHXD7LOX5F5KSLYQF5KNRYOKY6SQ...
2025-12-30 22:27:30,171 - ERROR - Error processing document_id AHXD7LOX5F5KSLYQF5KNRYOKY6SQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,173 - INFO - Processing document AFQAPPYFCP72MRP75CHNTUD7SNBA...
2025-12-30 22:27:30,176 - ERROR - Error processing document_id AFQAPPYFCP72MRP75CHNTUD7SNBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,177 - INFO - Processing document AGTIAHEGRDAFY2CJ7TQV3MOQIMEQ...
2025-12-30 22:27:30,180 - ERROR - Error processing document_id AGTIAHEGRDAFY2CJ7TQV3MOQIMEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,182 - INFO - Processing document AHYEPWP7J2KSK4HRCA7HWM42ZFBA...
2025-12-30 22:27:30,186 - ERROR - Error processing document_id AHYEPWP7J2KSK4HRCA7HWM42ZFBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,188 - INFO - Processing document AGSWTRJPZ7AVS2C33JGND5BQ6SMQ...
2025-12-30 22:27:30,191 - ERROR - Error processing document_id AGSWTRJPZ7AVS2C33JGND5BQ6SMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,193 - INFO - Processing document AGEHSZ5MULUYPSHGJDWAUNPP5NMQ...
2025-12-30 22:27:30,196 - ERROR - Error processing document_id AGEHSZ5MULUYPSHGJDWAUNPP5NMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,197 - INFO - Processing document AFEAQ5UGNJX4P3UVNFQV2M65EWOA...
2025-12-30 22:27:30,200 - ERROR - Error processing document_id AFEAQ5UGNJX4P3UVNFQV2M65EWOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,201 - INFO - Processing document AGLSC3ZESDAQWTZ3JNTDE7TUBU2A...
2025-12-30 22:27:30,205 - ERROR - Error processing document_id AGLSC3ZESDAQWTZ3JNTDE7TUBU2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,207 - INFO - Processing document AGIOE3S4WV2RHS57NGRMQTWZVQQQ...
2025-12-30 22:27:30,210 - ERROR - Error processing document_id AGIOE3S4WV2RHS57NGRMQTWZVQQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,211 - INFO - Processing document AFPF2CONJ33FNFKOEQ2TUNWQJTFQ...
2025-12-30 22:27:30,214 - ERROR - Error processing document_id AFPF2CONJ33FNFKOEQ2TUNWQJTFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,215 - INFO - Processing document AHOILU3TMQNUE7R26FU5HXJO7LPQ...
2025-12-30 22:27:30,218 - ERROR - Error processing document_id AHOILU3TMQNUE7R26FU5HXJO7LPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,220 - INFO - Processing document AEKBA2QZJQE25BSJMEAFNXCFP3AQ...
2025-12-30 22:27:30,222 - ERROR - Error processing document_id AEKBA2QZJQE25BSJMEAFNXCFP3AQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,224 - INFO - Processing document AES4H5QBFVU4IJHH77XH3EII7BUQ...
2025-12-30 22:27:30,226 - ERROR - Error processing document_id AES4H5QBFVU4IJHH77XH3EII7BUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,228 - INFO - Processing document AGWDYYVVWM3DC3CASUZKXK67G6IA...
2025-12-30 22:27:30,231 - ERROR - Error processing document_id AGWDYYVVWM3DC3CASUZKXK67G6IA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,232 - INFO - Processing document AEWLQYBQDYWWUWK6UHHTNWO5AHYA...
2025-12-30 22:27:30,236 - ERROR - Error processing document_id AEWLQYBQDYWWUWK6UHHTNWO5AHYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,238 - INFO - Processing document AHUOGDPK4YYWBBGV4P6L5GXARCRA...
2025-12-30 22:27:30,240 - ERROR - Error processing document_id AHUOGDPK4YYWBBGV4P6L5GXARCRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,242 - INFO - Processing document AFWWZ37DQAXXG2EG3JUVWYRQZE3Q...
2025-12-30 22:27:30,244 - ERROR - Error processing document_id AFWWZ37DQAXXG2EG3JUVWYRQZE3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,245 - INFO - Processing document AEDKSC5QX4EYXTKHXKGBY4RVMOOQ...
2025-12-30 22:27:30,247 - ERROR - Error processing document_id AEDKSC5QX4EYXTKHXKGBY4RVMOOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,249 - INFO - Processing document AE7ARNGEI22UM2WWFSF2LYTBYWLA...
2025-12-30 22:27:30,251 - ERROR - Error processing document_id AE7ARNGEI22UM2WWFSF2LYTBYWLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,253 - INFO - Processing document AGKKI25PHHUEFASISDXNMN7VZUNQ...
2025-12-30 22:27:30,254 - ERROR - Error processing document_id AGKKI25PHHUEFASISDXNMN7VZUNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,256 - INFO - Processing document AH2QGEDRSNFIYR5GUDP6IHQYBLGA...
2025-12-30 22:27:30,258 - ERROR - Error processing document_id AH2QGEDRSNFIYR5GUDP6IHQYBLGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,260 - INFO - Processing document AHJOEDMMKLQ6B3Z3SVDKXDIOIRTA...
2025-12-30 22:27:30,263 - ERROR - Error processing document_id AHJOEDMMKLQ6B3Z3SVDKXDIOIRTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,264 - INFO - Processing document AGOVEEK7OQXMIF4I4UOXGD732XWA...
2025-12-30 22:27:30,267 - ERROR - Error processing document_id AGOVEEK7OQXMIF4I4UOXGD732XWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,269 - INFO - Processing document AHYFLXIL7CMIVNU42WE3EL43OWZA...
2025-12-30 22:27:30,271 - ERROR - Error processing document_id AHYFLXIL7CMIVNU42WE3EL43OWZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,272 - INFO - Processing document AESR577IGEW3VNCWVNFJ4OKQFYVA...
2025-12-30 22:27:30,274 - ERROR - Error processing document_id AESR577IGEW3VNCWVNFJ4OKQFYVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,276 - INFO - Processing document AEO4GAMHGDCK7LFNSFBQMENKBAMQ...
2025-12-30 22:27:30,278 - ERROR - Error processing document_id AEO4GAMHGDCK7LFNSFBQMENKBAMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,279 - INFO - Processing document AHLCSNCD6HJLUF6R6UFXQTPT5N4Q...
2025-12-30 22:27:30,282 - ERROR - Error processing document_id AHLCSNCD6HJLUF6R6UFXQTPT5N4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,283 - INFO - Processing document AHBIE5OSER6FR4IPI64AWDJZIAOA...
2025-12-30 22:27:30,286 - ERROR - Error processing document_id AHBIE5OSER6FR4IPI64AWDJZIAOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,288 - INFO - Processing document AHQWX7COIJC2V3MCXBC3MUWAE63A...
2025-12-30 22:27:30,290 - ERROR - Error processing document_id AHQWX7COIJC2V3MCXBC3MUWAE63A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,292 - INFO - Processing document AFKQOH5UOFIZUE4R5ITELBUZ6HJA...
2025-12-30 22:27:30,295 - ERROR - Error processing document_id AFKQOH5UOFIZUE4R5ITELBUZ6HJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,297 - INFO - Processing document AGEA4U6DELLG3KONVQ74U5HEC5ZQ...
2025-12-30 22:27:30,299 - ERROR - Error processing document_id AGEA4U6DELLG3KONVQ74U5HEC5ZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,301 - INFO - Processing document AEKFPNURJ6ABIGPI7CO3LN6H3Y7A...
2025-12-30 22:27:30,304 - ERROR - Error processing document_id AEKFPNURJ6ABIGPI7CO3LN6H3Y7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,305 - INFO - Processing document AHYHY7AOS5HHOE2EPDF24C5UEBAA...
2025-12-30 22:27:30,307 - ERROR - Error processing document_id AHYHY7AOS5HHOE2EPDF24C5UEBAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,309 - INFO - Processing document AHPHB42X5G2Q4BIDEAB5PGRQBNHQ...
2025-12-30 22:27:30,311 - ERROR - Error processing document_id AHPHB42X5G2Q4BIDEAB5PGRQBNHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,313 - INFO - Processing document AFLP2B2PQRYNSZEAGHXV4ADUXNEQ...
2025-12-30 22:27:30,315 - ERROR - Error processing document_id AFLP2B2PQRYNSZEAGHXV4ADUXNEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,317 - INFO - Processing document AHUSSNQCGN3V7UR4JSCPZE2PHQPQ...
2025-12-30 22:27:30,320 - ERROR - Error processing document_id AHUSSNQCGN3V7UR4JSCPZE2PHQPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,322 - INFO - Processing document AFO7MZD3E5FB33WUAE426J2YUDHA...
2025-12-30 22:27:30,324 - ERROR - Error processing document_id AFO7MZD3E5FB33WUAE426J2YUDHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,326 - INFO - Processing document AE7H55QA77GJTWG7GZCM4DP33UNQ...
2025-12-30 22:27:30,328 - ERROR - Error processing document_id AE7H55QA77GJTWG7GZCM4DP33UNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,330 - INFO - Processing document AH6GSDFLJX5SMNEVVWZ7ZLZXC7VQ...
2025-12-30 22:27:30,332 - ERROR - Error processing document_id AH6GSDFLJX5SMNEVVWZ7ZLZXC7VQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,334 - INFO - Processing document AGP7OWOLP2BDMB5XTTXYCVIFDJKQ...
2025-12-30 22:27:30,338 - ERROR - Error processing document_id AGP7OWOLP2BDMB5XTTXYCVIFDJKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,340 - INFO - Processing document AFAAY32MQYRA56IVSVNSCDLNE62A...
2025-12-30 22:27:30,343 - ERROR - Error processing document_id AFAAY32MQYRA56IVSVNSCDLNE62A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,344 - INFO - Processing document AFKAHZR4KP65KLR2UO27FULIW2VA...
2025-12-30 22:27:30,347 - ERROR - Error processing document_id AFKAHZR4KP65KLR2UO27FULIW2VA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,349 - INFO - Processing document AEDUJMZOG66KA6K4NWAUGSQONZIA...
2025-12-30 22:27:30,352 - ERROR - Error processing document_id AEDUJMZOG66KA6K4NWAUGSQONZIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,354 - INFO - Processing document AF4CUUDNCGKR75IKBATOZOMRQ4CA...
2025-12-30 22:27:30,356 - ERROR - Error processing document_id AF4CUUDNCGKR75IKBATOZOMRQ4CA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,358 - INFO - Processing document AEVU5PDYTKFZPIGBNHLJ7TQ55K4Q...
2025-12-30 22:27:30,361 - ERROR - Error processing document_id AEVU5PDYTKFZPIGBNHLJ7TQ55K4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,362 - INFO - Processing document AHPW4NBU334IURZQPWP4PXODCE4Q...
2025-12-30 22:27:30,367 - ERROR - Error processing document_id AHPW4NBU334IURZQPWP4PXODCE4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,368 - INFO - Processing document AFTLVWXFMN4BHDCGLTH7WC3ZKU5A...
2025-12-30 22:27:30,371 - ERROR - Error processing document_id AFTLVWXFMN4BHDCGLTH7WC3ZKU5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,372 - INFO - Processing document AG6YJNBXXMFFPRYC77XCZQG6OKJA...
2025-12-30 22:27:30,375 - ERROR - Error processing document_id AG6YJNBXXMFFPRYC77XCZQG6OKJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,377 - INFO - Processing document AH6PQ676SWYMJK7NJBVC223AQIRQ...
2025-12-30 22:27:30,379 - ERROR - Error processing document_id AH6PQ676SWYMJK7NJBVC223AQIRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,381 - INFO - Processing document AECEVSGZN74NWZABZHGVPYMW2L7Q...
2025-12-30 22:27:30,382 - ERROR - Error processing document_id AECEVSGZN74NWZABZHGVPYMW2L7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,384 - INFO - Processing document AFIKJCL6K7AHRF6JKJQTBXF5YFKA...
2025-12-30 22:27:30,388 - ERROR - Error processing document_id AFIKJCL6K7AHRF6JKJQTBXF5YFKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,389 - INFO - Processing document AEVTAGCQAPWKZS6JGFPZG5KEB5VQ...
2025-12-30 22:27:30,392 - ERROR - Error processing document_id AEVTAGCQAPWKZS6JGFPZG5KEB5VQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,394 - INFO - Processing document AFNW4ZEAVQGZUHLZ6CMLO2JJUZQQ...
2025-12-30 22:27:30,396 - ERROR - Error processing document_id AFNW4ZEAVQGZUHLZ6CMLO2JJUZQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,397 - INFO - Processing document AHMGQFUR7MJXA43WO23CEIQLPXHQ...
2025-12-30 22:27:30,400 - ERROR - Error processing document_id AHMGQFUR7MJXA43WO23CEIQLPXHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,402 - INFO - Processing document AHASTU2L67MDPLA2USDU4YASK52Q...
2025-12-30 22:27:30,404 - ERROR - Error processing document_id AHASTU2L67MDPLA2USDU4YASK52Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,406 - INFO - Processing document AFMRD5MCSJRU662EHRHA23774HAA...
2025-12-30 22:27:30,408 - ERROR - Error processing document_id AFMRD5MCSJRU662EHRHA23774HAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,410 - INFO - Processing document AFJWKP65YRJXCEFACBOEKZLCWDCQ...
2025-12-30 22:27:30,412 - ERROR - Error processing document_id AFJWKP65YRJXCEFACBOEKZLCWDCQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,414 - INFO - Processing document AGRKSC7RPVBTQSSUGGEP6PPW3KEQ...
2025-12-30 22:27:30,416 - ERROR - Error processing document_id AGRKSC7RPVBTQSSUGGEP6PPW3KEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,418 - INFO - Processing document AHPPAS55ET56U5K7J2J5LTCCL6MA...
2025-12-30 22:27:30,421 - ERROR - Error processing document_id AHPPAS55ET56U5K7J2J5LTCCL6MA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,422 - INFO - Processing document AEHTHEERCGK7EI56YX2MU2SOHNQA...
2025-12-30 22:27:30,425 - ERROR - Error processing document_id AEHTHEERCGK7EI56YX2MU2SOHNQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,427 - INFO - Processing document AFW2PDT3AMT4X3PYQG7FJZH5FXFA...
2025-12-30 22:27:30,431 - ERROR - Error processing document_id AFW2PDT3AMT4X3PYQG7FJZH5FXFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,433 - INFO - Processing document AGIXGEJZOUPBQ3AVGHI7XZUAALQA...
2025-12-30 22:27:30,435 - ERROR - Error processing document_id AGIXGEJZOUPBQ3AVGHI7XZUAALQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,436 - INFO - Processing document AGGFOPNWDB77OFWD4Z77OX67D6MQ...
2025-12-30 22:27:30,440 - ERROR - Error processing document_id AGGFOPNWDB77OFWD4Z77OX67D6MQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,441 - INFO - Processing document AH3TTJCNX4O65Y4BWV44YSAT2V2Q...
2025-12-30 22:27:30,444 - ERROR - Error processing document_id AH3TTJCNX4O65Y4BWV44YSAT2V2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,445 - INFO - Processing document AHVIMKECEZDXXK5QIGX7KWJPN6DQ...
2025-12-30 22:27:30,450 - ERROR - Error processing document_id AHVIMKECEZDXXK5QIGX7KWJPN6DQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,451 - INFO - Processing document AH52CGP4PKDE7F2IEPRVNXUPMLZQ...
2025-12-30 22:27:30,454 - ERROR - Error processing document_id AH52CGP4PKDE7F2IEPRVNXUPMLZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,456 - INFO - Processing document AGUNDDGZTW4YVOTZBH77AKHHW5YA...
2025-12-30 22:27:30,458 - ERROR - Error processing document_id AGUNDDGZTW4YVOTZBH77AKHHW5YA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,459 - INFO - Processing document AF3V5Z3VHOWFMFMYV2USYX3B7DJQ...
2025-12-30 22:27:30,461 - ERROR - Error processing document_id AF3V5Z3VHOWFMFMYV2USYX3B7DJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,463 - INFO - Processing document AHKQLDYRJYEUH67DHHKSWMJ7QEBQ...
2025-12-30 22:27:30,466 - ERROR - Error processing document_id AHKQLDYRJYEUH67DHHKSWMJ7QEBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,467 - INFO - Processing document AHZWVIBN5WVIZP5IULWPKEFJN2AA...
2025-12-30 22:27:30,470 - ERROR - Error processing document_id AHZWVIBN5WVIZP5IULWPKEFJN2AA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,471 - INFO - Processing document AFAWNBABBOB64XUJWVLX2Q2TQHSA...
2025-12-30 22:27:30,474 - ERROR - Error processing document_id AFAWNBABBOB64XUJWVLX2Q2TQHSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,476 - INFO - Processing document AGCR55N5L3KBAY73EZKWNT5GBBCA...
2025-12-30 22:27:30,479 - ERROR - Error processing document_id AGCR55N5L3KBAY73EZKWNT5GBBCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,480 - INFO - Processing document AFUQBAD6D5TVORJLPA6VJVS5A5LA...
2025-12-30 22:27:30,483 - ERROR - Error processing document_id AFUQBAD6D5TVORJLPA6VJVS5A5LA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,484 - INFO - Processing document AHMP2RRHRJ4TZRNLC3DYPTM4YIWQ...
2025-12-30 22:27:30,487 - ERROR - Error processing document_id AHMP2RRHRJ4TZRNLC3DYPTM4YIWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,489 - INFO - Processing document AECKMTIMUQ6ELPSSEXQ53MLHGVBA...
2025-12-30 22:27:30,492 - ERROR - Error processing document_id AECKMTIMUQ6ELPSSEXQ53MLHGVBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,494 - INFO - Processing document AETV6PI62ZZDE3NTYVGTPSXI6EZQ...
2025-12-30 22:27:30,497 - ERROR - Error processing document_id AETV6PI62ZZDE3NTYVGTPSXI6EZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,498 - INFO - Processing document AEBTRL64MQHJNHIJDBWYVSXXFBAQ...
2025-12-30 22:27:30,500 - ERROR - Error processing document_id AEBTRL64MQHJNHIJDBWYVSXXFBAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,502 - INFO - Processing document AGPOYZY5YU4FD5PJPXT3UFCRBK7Q...
2025-12-30 22:27:30,504 - ERROR - Error processing document_id AGPOYZY5YU4FD5PJPXT3UFCRBK7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,506 - INFO - Processing document AFBK6VNU3ZVA35VM77BNZYVSGKEA...
2025-12-30 22:27:30,508 - ERROR - Error processing document_id AFBK6VNU3ZVA35VM77BNZYVSGKEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,509 - INFO - Processing document AH6Z44UZXUEKY5MMOJ3Y3GYGAS4A...
2025-12-30 22:27:30,511 - ERROR - Error processing document_id AH6Z44UZXUEKY5MMOJ3Y3GYGAS4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,513 - INFO - Processing document AEKBAMY5YRDF3H7IHTHPW5TKZODA...
2025-12-30 22:27:30,514 - ERROR - Error processing document_id AEKBAMY5YRDF3H7IHTHPW5TKZODA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,517 - INFO - Processing document AELLN5XAWOAIDR73RWCDW6SWKX2A...
2025-12-30 22:27:30,525 - ERROR - Error processing document_id AELLN5XAWOAIDR73RWCDW6SWKX2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,526 - INFO - Processing document AFFBUHYE6RQ66DBGO3KNWVY36PXA...
2025-12-30 22:27:30,530 - ERROR - Error processing document_id AFFBUHYE6RQ66DBGO3KNWVY36PXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,532 - INFO - Processing document AFUFSGF74FK66TVCFROEEGYGXGCA...
2025-12-30 22:27:30,535 - ERROR - Error processing document_id AFUFSGF74FK66TVCFROEEGYGXGCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,537 - INFO - Processing document AE3HEKYJDDADODF3CTOCMDUHSV4A...
2025-12-30 22:27:30,539 - ERROR - Error processing document_id AE3HEKYJDDADODF3CTOCMDUHSV4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,540 - INFO - Processing document AFS7L4Z6O6KDVLVE3HZLDLMLMHQQ...
2025-12-30 22:27:30,542 - ERROR - Error processing document_id AFS7L4Z6O6KDVLVE3HZLDLMLMHQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,544 - INFO - Processing document AHGZWQO2NJZMTYLPML2RQWTISOJA...
2025-12-30 22:27:30,546 - ERROR - Error processing document_id AHGZWQO2NJZMTYLPML2RQWTISOJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,547 - INFO - Processing document AEEGYFZSO5TVST557JGFT2A2TDLA...
2025-12-30 22:27:30,549 - ERROR - Error processing document_id AEEGYFZSO5TVST557JGFT2A2TDLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,551 - INFO - Processing document AF77WKG632KIF4V4L3BXAEX76UFA...
2025-12-30 22:27:30,553 - ERROR - Error processing document_id AF77WKG632KIF4V4L3BXAEX76UFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,554 - INFO - Processing document AGXFIFO4VWNHPAX3VJPRQ2QJSCSQ...
2025-12-30 22:27:30,557 - ERROR - Error processing document_id AGXFIFO4VWNHPAX3VJPRQ2QJSCSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,559 - INFO - Processing document AFOPCJOHIU3TZDX57MANP44NX7BA...
2025-12-30 22:27:30,561 - ERROR - Error processing document_id AFOPCJOHIU3TZDX57MANP44NX7BA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,563 - INFO - Processing document AHDNOLYDM72Q5EYR7455KTJFG4AA...
2025-12-30 22:27:30,565 - ERROR - Error processing document_id AHDNOLYDM72Q5EYR7455KTJFG4AA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,566 - INFO - Processing document AF5I24JRBXNFZOMWLSXN46DZ3QFQ...
2025-12-30 22:27:30,568 - ERROR - Error processing document_id AF5I24JRBXNFZOMWLSXN46DZ3QFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,570 - INFO - Processing document AHIKRWUS4AMZQXKK2RXD6WOT2SAA...
2025-12-30 22:27:30,573 - ERROR - Error processing document_id AHIKRWUS4AMZQXKK2RXD6WOT2SAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,575 - INFO - Processing document AEZCNXD3N4OFDDRSCUXLC7ZANLXA...
2025-12-30 22:27:30,577 - ERROR - Error processing document_id AEZCNXD3N4OFDDRSCUXLC7ZANLXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,578 - INFO - Processing document AGKSQ3ZJRBYIOQIFKCFHRDSA4ZKQ...
2025-12-30 22:27:30,580 - ERROR - Error processing document_id AGKSQ3ZJRBYIOQIFKCFHRDSA4ZKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,582 - INFO - Processing document AHS7O4ONEBZLCU7YLOSLSV72N2IA...
2025-12-30 22:27:30,585 - ERROR - Error processing document_id AHS7O4ONEBZLCU7YLOSLSV72N2IA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,586 - INFO - Processing document AEWRGEQPEOZTEW36VJLDOK6B4KVA...
2025-12-30 22:27:30,588 - ERROR - Error processing document_id AEWRGEQPEOZTEW36VJLDOK6B4KVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,590 - INFO - Processing document AEJHNIDRGFNJE4WE53A7DDANFLXQ...
2025-12-30 22:27:30,592 - ERROR - Error processing document_id AEJHNIDRGFNJE4WE53A7DDANFLXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,594 - INFO - Processing document AEIRORFGVV3DA2CNKCJ4VOFL5FZQ...
2025-12-30 22:27:30,596 - ERROR - Error processing document_id AEIRORFGVV3DA2CNKCJ4VOFL5FZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,597 - INFO - Processing document AFR3RUIWQ2UB6PIPJDTCCLWW2WPA...
2025-12-30 22:27:30,601 - ERROR - Error processing document_id AFR3RUIWQ2UB6PIPJDTCCLWW2WPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,602 - INFO - Processing document AFNH5WSBLLCV3MXP3V2EDGVK4MAQ...
2025-12-30 22:27:30,605 - ERROR - Error processing document_id AFNH5WSBLLCV3MXP3V2EDGVK4MAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,607 - INFO - Processing document AHIDINICLGW45UUGVEBG43WH4VJA...
2025-12-30 22:27:30,609 - ERROR - Error processing document_id AHIDINICLGW45UUGVEBG43WH4VJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,611 - INFO - Processing document AHJ3J6Y6FHGE4FYBBNBILVL3XDXA...
2025-12-30 22:27:30,614 - ERROR - Error processing document_id AHJ3J6Y6FHGE4FYBBNBILVL3XDXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,616 - INFO - Processing document AERPCS4LJW2XNV6JWAAFMWLPAR5A...
2025-12-30 22:27:30,619 - ERROR - Error processing document_id AERPCS4LJW2XNV6JWAAFMWLPAR5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,621 - INFO - Processing document AHF7I7LZDDTOCHRMHUMOBXHFTKWQ...
2025-12-30 22:27:30,625 - ERROR - Error processing document_id AHF7I7LZDDTOCHRMHUMOBXHFTKWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,626 - INFO - Processing document AGIELHWW62QLOWY4ODQIOOZNHKPA...
2025-12-30 22:27:30,631 - ERROR - Error processing document_id AGIELHWW62QLOWY4ODQIOOZNHKPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,633 - INFO - Processing document AH7SHXCPNHCZ355V7E5WZNPUGWRQ...
2025-12-30 22:27:30,636 - ERROR - Error processing document_id AH7SHXCPNHCZ355V7E5WZNPUGWRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,637 - INFO - Processing document AHR6UNBC4PLK7GZBOQ5HE26BIULA...
2025-12-30 22:27:30,640 - ERROR - Error processing document_id AHR6UNBC4PLK7GZBOQ5HE26BIULA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,642 - INFO - Processing document AGT4S5Q7PPN6FST33GVNUVY5ZGCQ...
2025-12-30 22:27:30,645 - ERROR - Error processing document_id AGT4S5Q7PPN6FST33GVNUVY5ZGCQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,647 - INFO - Processing document AFLISTFBINTN325UXXAEL7GGSKIQ...
2025-12-30 22:27:30,649 - ERROR - Error processing document_id AFLISTFBINTN325UXXAEL7GGSKIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,651 - INFO - Processing document AGMDNBFOXOV4LIOVUUT7MWJTZ3SQ...
2025-12-30 22:27:30,653 - ERROR - Error processing document_id AGMDNBFOXOV4LIOVUUT7MWJTZ3SQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,655 - INFO - Processing document AESEBX4NRYAVWV5WVPIVSQT6GZYQ...
2025-12-30 22:27:30,658 - ERROR - Error processing document_id AESEBX4NRYAVWV5WVPIVSQT6GZYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,660 - INFO - Processing document AHTVVZDX4M2WTFA3LG5AMZDCIRYA...
2025-12-30 22:27:30,662 - ERROR - Error processing document_id AHTVVZDX4M2WTFA3LG5AMZDCIRYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,664 - INFO - Processing document AFPCBZQC3C3MDTQ4H5RG3FCBA4JQ...
2025-12-30 22:27:30,666 - ERROR - Error processing document_id AFPCBZQC3C3MDTQ4H5RG3FCBA4JQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,667 - INFO - Processing document AECOMNWEHJX46AM4K5NC7MSO2HLA...
2025-12-30 22:27:30,670 - ERROR - Error processing document_id AECOMNWEHJX46AM4K5NC7MSO2HLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,672 - INFO - Processing document AE2YWJMGLX7EP2OA27FNSFFVL4SA...
2025-12-30 22:27:30,674 - ERROR - Error processing document_id AE2YWJMGLX7EP2OA27FNSFFVL4SA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,676 - INFO - Processing document AEMG2B5GCCRHZWZJKUXDN7TOWBXQ...
2025-12-30 22:27:30,679 - ERROR - Error processing document_id AEMG2B5GCCRHZWZJKUXDN7TOWBXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,680 - INFO - Processing document AEWY55QQGBVE3AV5VYDPNWLC6X2Q...
2025-12-30 22:27:30,683 - ERROR - Error processing document_id AEWY55QQGBVE3AV5VYDPNWLC6X2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,684 - INFO - Processing document AGLER7XTAC2JP37ADMKTU22ZYZIA...
2025-12-30 22:27:30,687 - ERROR - Error processing document_id AGLER7XTAC2JP37ADMKTU22ZYZIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,688 - INFO - Processing document AENC5KEJIEYK7GROM6QDNE5SDS4A...
2025-12-30 22:27:30,690 - ERROR - Error processing document_id AENC5KEJIEYK7GROM6QDNE5SDS4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,692 - INFO - Processing document AF3OICPENFP4QCWGLJTRS6HLBGYQ...
2025-12-30 22:27:30,694 - ERROR - Error processing document_id AF3OICPENFP4QCWGLJTRS6HLBGYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,696 - INFO - Processing document AENBUXPSQTEVMHI7K4YJ3EYWNRPA...
2025-12-30 22:27:30,699 - ERROR - Error processing document_id AENBUXPSQTEVMHI7K4YJ3EYWNRPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,701 - INFO - Processing document AHU57DCJDKO74XDYOWGKNQ7TWAEQ...
2025-12-30 22:27:30,704 - ERROR - Error processing document_id AHU57DCJDKO74XDYOWGKNQ7TWAEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,706 - INFO - Processing document AGBHMH5GOCFXKTD4FFKJUATMN2BA...
2025-12-30 22:27:30,708 - ERROR - Error processing document_id AGBHMH5GOCFXKTD4FFKJUATMN2BA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,710 - INFO - Processing document AHY5662OAFQ7XPLZQ6AXRRVZPUMQ...
2025-12-30 22:27:30,713 - ERROR - Error processing document_id AHY5662OAFQ7XPLZQ6AXRRVZPUMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,714 - INFO - Processing document AF6KABYSHNUDMY4JY6YTHUSCPRHA...
2025-12-30 22:27:30,716 - ERROR - Error processing document_id AF6KABYSHNUDMY4JY6YTHUSCPRHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,718 - INFO - Processing document AGTRTAXJOWDPILKAATSVLXQNN4HQ...
2025-12-30 22:27:30,722 - ERROR - Error processing document_id AGTRTAXJOWDPILKAATSVLXQNN4HQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,724 - INFO - Processing document AFDE7XOCIL4YGX7F5X3TSDVFZAVA...
2025-12-30 22:27:30,726 - ERROR - Error processing document_id AFDE7XOCIL4YGX7F5X3TSDVFZAVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,728 - INFO - Processing document AE4B34A4YU5TKBYGPZD2V4QWLODA...
2025-12-30 22:27:30,731 - ERROR - Error processing document_id AE4B34A4YU5TKBYGPZD2V4QWLODA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,732 - INFO - Processing document AFCTSM7OKEVZ2TM5G3ZMRCFJHGVQ...
2025-12-30 22:27:30,735 - ERROR - Error processing document_id AFCTSM7OKEVZ2TM5G3ZMRCFJHGVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,736 - INFO - Processing document AHCBJQSD2IY3S7SX5KIQO25DAO3Q...
2025-12-30 22:27:30,739 - ERROR - Error processing document_id AHCBJQSD2IY3S7SX5KIQO25DAO3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,740 - INFO - Processing document AHVX47RCFREZMFZMRKWMZOM77QRA...
2025-12-30 22:27:30,742 - ERROR - Error processing document_id AHVX47RCFREZMFZMRKWMZOM77QRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,744 - INFO - Processing document AHX25MN3P7B6FZCQIZDBLBNIGKZQ...
2025-12-30 22:27:30,747 - ERROR - Error processing document_id AHX25MN3P7B6FZCQIZDBLBNIGKZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,749 - INFO - Processing document AFXAAFVZ556CQTE6KH52QQPFD5YQ...
2025-12-30 22:27:30,751 - ERROR - Error processing document_id AFXAAFVZ556CQTE6KH52QQPFD5YQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,752 - INFO - Processing document AGMWSQKAVBLZBYXPPP34QZRAD2LA...
2025-12-30 22:27:30,754 - ERROR - Error processing document_id AGMWSQKAVBLZBYXPPP34QZRAD2LA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,756 - INFO - Processing document AERSA5MJQSXMXCHOEZTX6RTWMK3Q...
2025-12-30 22:27:30,761 - ERROR - Error processing document_id AERSA5MJQSXMXCHOEZTX6RTWMK3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,763 - INFO - Processing document AEWLPJRTFGJEISNB7XS3J43TAANA...
2025-12-30 22:27:30,764 - ERROR - Error processing document_id AEWLPJRTFGJEISNB7XS3J43TAANA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,766 - INFO - Processing document AHMEDY26WWEIJH6DUJYXLEIIE4UQ...
2025-12-30 22:27:30,768 - ERROR - Error processing document_id AHMEDY26WWEIJH6DUJYXLEIIE4UQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,770 - INFO - Processing document AHCMRFY4JAMNKUPDGMF4IP3BV5YA...
2025-12-30 22:27:30,772 - ERROR - Error processing document_id AHCMRFY4JAMNKUPDGMF4IP3BV5YA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,774 - INFO - Processing document AF2A6UFVFGMNZDM6F7J6A32IFO6Q...
2025-12-30 22:27:30,777 - ERROR - Error processing document_id AF2A6UFVFGMNZDM6F7J6A32IFO6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,778 - INFO - Processing document AE3DU4S4T2M44R7DGCQ3JU7EX6KA...
2025-12-30 22:27:30,781 - ERROR - Error processing document_id AE3DU4S4T2M44R7DGCQ3JU7EX6KA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,783 - INFO - Processing document AFVMOMMBPLQCF7HEPLHZRFJ7JQJQ...
2025-12-30 22:27:30,786 - ERROR - Error processing document_id AFVMOMMBPLQCF7HEPLHZRFJ7JQJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,787 - INFO - Processing document AFFKEHOW5CKVIULHNCF4FX2RBUVA...
2025-12-30 22:27:30,789 - ERROR - Error processing document_id AFFKEHOW5CKVIULHNCF4FX2RBUVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,791 - INFO - Processing document AFMXAJQ2HGQ22JZW4332OYDGQUJQ...
2025-12-30 22:27:30,794 - ERROR - Error processing document_id AFMXAJQ2HGQ22JZW4332OYDGQUJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,796 - INFO - Processing document AHPPNXBRVHYJ4U5ULMVS62PKMWWA...
2025-12-30 22:27:30,798 - ERROR - Error processing document_id AHPPNXBRVHYJ4U5ULMVS62PKMWWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,799 - INFO - Processing document AGSR2LEITQYMREGRT665FWMDSSMA...
2025-12-30 22:27:30,801 - ERROR - Error processing document_id AGSR2LEITQYMREGRT665FWMDSSMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,803 - INFO - Processing document AHVMF7W4JIS2P2JAIKZHCUTTRS6Q...
2025-12-30 22:27:30,806 - ERROR - Error processing document_id AHVMF7W4JIS2P2JAIKZHCUTTRS6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,807 - INFO - Processing document AFXRZKMFZ2HN3FZ2EETJILLVRPBA...
2025-12-30 22:27:30,810 - ERROR - Error processing document_id AFXRZKMFZ2HN3FZ2EETJILLVRPBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,812 - INFO - Processing document AEJRYKWGDINWO75X6E5IT75EEPBQ...
2025-12-30 22:27:30,814 - ERROR - Error processing document_id AEJRYKWGDINWO75X6E5IT75EEPBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,816 - INFO - Processing document AECRBDORQU4EJI4G7R76265KTSMA...
2025-12-30 22:27:30,819 - ERROR - Error processing document_id AECRBDORQU4EJI4G7R76265KTSMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,821 - INFO - Processing document AG2DTAEGURQMHOA64XKUSP7OGFVA...
2025-12-30 22:27:30,823 - ERROR - Error processing document_id AG2DTAEGURQMHOA64XKUSP7OGFVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,824 - INFO - Processing document AEFC6TUGRCV2XBAEP6QRRZA3366A...
2025-12-30 22:27:30,827 - ERROR - Error processing document_id AEFC6TUGRCV2XBAEP6QRRZA3366A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,828 - INFO - Processing document AEPJHTGWFHUCWOENMEI32ZRWI7BA...
2025-12-30 22:27:30,831 - ERROR - Error processing document_id AEPJHTGWFHUCWOENMEI32ZRWI7BA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,832 - INFO - Processing document AGF3QER3IA7TNGDOQ2CPZGFTAUQQ...
2025-12-30 22:27:30,835 - ERROR - Error processing document_id AGF3QER3IA7TNGDOQ2CPZGFTAUQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,837 - INFO - Processing document AGIL6ZZTW7LAY3JTVY3BG2WXU4QA...
2025-12-30 22:27:30,840 - ERROR - Error processing document_id AGIL6ZZTW7LAY3JTVY3BG2WXU4QA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,842 - INFO - Processing document AGMP7YW4R3JIJX5O3WL4FVAISJUQ...
2025-12-30 22:27:30,844 - ERROR - Error processing document_id AGMP7YW4R3JIJX5O3WL4FVAISJUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,846 - INFO - Processing document AGHYGRXLPXJJR5JRETN6HYKNYJ5A...
2025-12-30 22:27:30,848 - ERROR - Error processing document_id AGHYGRXLPXJJR5JRETN6HYKNYJ5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,850 - INFO - Processing document AFG5MGYUODJUATUJTIOJHWGOIQQQ...
2025-12-30 22:27:30,852 - ERROR - Error processing document_id AFG5MGYUODJUATUJTIOJHWGOIQQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,853 - INFO - Processing document AE44IQA5EBEHFJA7T7ILYARSUH4Q...
2025-12-30 22:27:30,856 - ERROR - Error processing document_id AE44IQA5EBEHFJA7T7ILYARSUH4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,858 - INFO - Processing document AH3KO5NJOZ6MVQNOJAPYRYCC7XGQ...
2025-12-30 22:27:30,861 - ERROR - Error processing document_id AH3KO5NJOZ6MVQNOJAPYRYCC7XGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,863 - INFO - Processing document AGM27IYNGL7CRVVOXRXV55RG3ULQ...
2025-12-30 22:27:30,864 - ERROR - Error processing document_id AGM27IYNGL7CRVVOXRXV55RG3ULQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,866 - INFO - Processing document AF5FQY3DDUYZG7O5S66R2PI7E33A...
2025-12-30 22:27:30,872 - ERROR - Error processing document_id AF5FQY3DDUYZG7O5S66R2PI7E33A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,873 - INFO - Processing document AFAJCAF7DP63WJYZ2A5KMRBTMA6A...
2025-12-30 22:27:30,875 - ERROR - Error processing document_id AFAJCAF7DP63WJYZ2A5KMRBTMA6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,877 - INFO - Processing document AHPKVLPDNVH5FTWFWEDLP3T4C6MQ...
2025-12-30 22:27:30,879 - ERROR - Error processing document_id AHPKVLPDNVH5FTWFWEDLP3T4C6MQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,881 - INFO - Processing document AH2WLNR7V5F3AL24T4H5BZ6JRCTQ...
2025-12-30 22:27:30,884 - ERROR - Error processing document_id AH2WLNR7V5F3AL24T4H5BZ6JRCTQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,886 - INFO - Processing document AHBQZNO6D5ALYE7UDVTAJK5UTUGQ...
2025-12-30 22:27:30,888 - ERROR - Error processing document_id AHBQZNO6D5ALYE7UDVTAJK5UTUGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,889 - INFO - Processing document AEY3JAK2RQYMS7V66IW2AFE7DI7Q...
2025-12-30 22:27:30,893 - ERROR - Error processing document_id AEY3JAK2RQYMS7V66IW2AFE7DI7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,895 - INFO - Processing document AF47V2KBMY3GIYNYPPJSKZFCSG7A...
2025-12-30 22:27:30,897 - ERROR - Error processing document_id AF47V2KBMY3GIYNYPPJSKZFCSG7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,899 - INFO - Processing document AGHU4JCPKB5Z3E3F7UPII6IHZ7SA...
2025-12-30 22:27:30,903 - ERROR - Error processing document_id AGHU4JCPKB5Z3E3F7UPII6IHZ7SA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,904 - INFO - Processing document AHKVAYJF4Q6KU5WOCXNBBUYG34MA...
2025-12-30 22:27:30,907 - ERROR - Error processing document_id AHKVAYJF4Q6KU5WOCXNBBUYG34MA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,908 - INFO - Processing document AHDGEHZBULFXMKQBBYO67GS5J74A...
2025-12-30 22:27:30,910 - ERROR - Error processing document_id AHDGEHZBULFXMKQBBYO67GS5J74A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,912 - INFO - Processing document AHUFXZUTSKKFA3RNYMU7QCVBL4SQ...
2025-12-30 22:27:30,914 - ERROR - Error processing document_id AHUFXZUTSKKFA3RNYMU7QCVBL4SQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,916 - INFO - Processing document AGHVL3PFCKADOA354W6NS4W5YHIA...
2025-12-30 22:27:30,919 - ERROR - Error processing document_id AGHVL3PFCKADOA354W6NS4W5YHIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,920 - INFO - Processing document AGXSTG2MUQM7HGOEZ4DR7NFB3JLA...
2025-12-30 22:27:30,923 - ERROR - Error processing document_id AGXSTG2MUQM7HGOEZ4DR7NFB3JLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,925 - INFO - Processing document AEI3YSZORUO6YHTPDTVSSOWBTRXQ...
2025-12-30 22:27:30,927 - ERROR - Error processing document_id AEI3YSZORUO6YHTPDTVSSOWBTRXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,928 - INFO - Processing document AGUC77RC74K7T2EZL3ICLZ25HFQA...
2025-12-30 22:27:30,931 - ERROR - Error processing document_id AGUC77RC74K7T2EZL3ICLZ25HFQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,933 - INFO - Processing document AHJLWPZ7WNXS3LBADCS7MW4X527A...
2025-12-30 22:27:30,935 - ERROR - Error processing document_id AHJLWPZ7WNXS3LBADCS7MW4X527A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,937 - INFO - Processing document AGN2IUTLMDSFZUA2LIQNZFBO6GHA...
2025-12-30 22:27:30,940 - ERROR - Error processing document_id AGN2IUTLMDSFZUA2LIQNZFBO6GHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,942 - INFO - Processing document AEZMW4G3HKJCQNOABU7R4TWWTFTA...
2025-12-30 22:27:30,947 - ERROR - Error processing document_id AEZMW4G3HKJCQNOABU7R4TWWTFTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,948 - INFO - Processing document AGPHRBXDF5VFMGJ36YT2AXRGB6PQ...
2025-12-30 22:27:30,951 - ERROR - Error processing document_id AGPHRBXDF5VFMGJ36YT2AXRGB6PQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,953 - INFO - Processing document AHVMHHFZQQWGXYSXLLIKR4TNWHSQ...
2025-12-30 22:27:30,956 - ERROR - Error processing document_id AHVMHHFZQQWGXYSXLLIKR4TNWHSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,958 - INFO - Processing document AFKJFYRCZ4KHYBCKQXUUZ5WAOHKA...
2025-12-30 22:27:30,961 - ERROR - Error processing document_id AFKJFYRCZ4KHYBCKQXUUZ5WAOHKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,963 - INFO - Processing document AHHHYELK6DAV54EYYP4PMBLSVHBA...
2025-12-30 22:27:30,965 - ERROR - Error processing document_id AHHHYELK6DAV54EYYP4PMBLSVHBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,967 - INFO - Processing document AF7KPKYXYTJYKHB36DEVIZOFVTGA...
2025-12-30 22:27:30,970 - ERROR - Error processing document_id AF7KPKYXYTJYKHB36DEVIZOFVTGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,971 - INFO - Processing document AGYBNFC2UQZ26VHUFURZ6NPXUXCA...
2025-12-30 22:27:30,974 - ERROR - Error processing document_id AGYBNFC2UQZ26VHUFURZ6NPXUXCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,975 - INFO - Processing document AE7XXMSQGLWI2WA65ZYPSJPPJDEQ...
2025-12-30 22:27:30,978 - ERROR - Error processing document_id AE7XXMSQGLWI2WA65ZYPSJPPJDEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,979 - INFO - Processing document AH2ZINAZH27J3PN3N53VWMYSWFAQ...
2025-12-30 22:27:30,985 - ERROR - Error processing document_id AH2ZINAZH27J3PN3N53VWMYSWFAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,986 - INFO - Processing document AHBDCC6PPJ6TDXHQOYIHKDNS3VHQ...
2025-12-30 22:27:30,989 - ERROR - Error processing document_id AHBDCC6PPJ6TDXHQOYIHKDNS3VHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,990 - INFO - Processing document AGV4UFVIVZ3AFNYTJZW4EQKEMVMQ...
2025-12-30 22:27:30,993 - ERROR - Error processing document_id AGV4UFVIVZ3AFNYTJZW4EQKEMVMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,995 - INFO - Processing document AE7BHDEGWRCBHASTI25OLGUUFK3A...
2025-12-30 22:27:30,997 - ERROR - Error processing document_id AE7BHDEGWRCBHASTI25OLGUUFK3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:30,999 - INFO - Processing document AEFH4D7WTMZ6JWO7WLRHWW264ZZA...
2025-12-30 22:27:31,001 - ERROR - Error processing document_id AEFH4D7WTMZ6JWO7WLRHWW264ZZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,003 - INFO - Processing document AEEZL2NEWCB5CY6F5B4CCV5MM2WQ...
2025-12-30 22:27:31,005 - ERROR - Error processing document_id AEEZL2NEWCB5CY6F5B4CCV5MM2WQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,007 - INFO - Processing document AHRFX4VZ7VD5JQPF3TPFZJTP2DVA...
2025-12-30 22:27:31,010 - ERROR - Error processing document_id AHRFX4VZ7VD5JQPF3TPFZJTP2DVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,011 - INFO - Processing document AFFZN6QCBXQCD2EFE7RMFPIP25UQ...
2025-12-30 22:27:31,014 - ERROR - Error processing document_id AFFZN6QCBXQCD2EFE7RMFPIP25UQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,015 - INFO - Processing document AFHQLYIQ7YT74ZMCV76CPRQ23CKA...
2025-12-30 22:27:31,017 - ERROR - Error processing document_id AFHQLYIQ7YT74ZMCV76CPRQ23CKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,019 - INFO - Processing document AGHYYKXVAU6T3HZMH3AQ7GR7USIQ...
2025-12-30 22:27:31,022 - ERROR - Error processing document_id AGHYYKXVAU6T3HZMH3AQ7GR7USIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,024 - INFO - Processing document AHL46ZJDQDTXZMYPUQSCCDK7C2CQ...
2025-12-30 22:27:31,026 - ERROR - Error processing document_id AHL46ZJDQDTXZMYPUQSCCDK7C2CQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,027 - INFO - Processing document AGTO26IADB2GBMDIOFKD54JW6FSA...
2025-12-30 22:27:31,032 - ERROR - Error processing document_id AGTO26IADB2GBMDIOFKD54JW6FSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,034 - INFO - Processing document AETYQ22IDAU3AZAMJPDFEYUVUS7Q...
2025-12-30 22:27:31,037 - ERROR - Error processing document_id AETYQ22IDAU3AZAMJPDFEYUVUS7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,038 - INFO - Processing document AHBXDQCORNOY4PVLA63ZNOY4JFIA...
2025-12-30 22:27:31,042 - ERROR - Error processing document_id AHBXDQCORNOY4PVLA63ZNOY4JFIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,043 - INFO - Processing document AF5NVM6IJCCOOVETAN37BEJ2A5MA...
2025-12-30 22:27:31,046 - ERROR - Error processing document_id AF5NVM6IJCCOOVETAN37BEJ2A5MA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,048 - INFO - Processing document AFQJK3LJ2L5EZSTGZUZIQ75LEUWA...
2025-12-30 22:27:31,050 - ERROR - Error processing document_id AFQJK3LJ2L5EZSTGZUZIQ75LEUWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,052 - INFO - Processing document AHXWUCTMVBQXDVMDFPA3NO43QF2Q...
2025-12-30 22:27:31,054 - ERROR - Error processing document_id AHXWUCTMVBQXDVMDFPA3NO43QF2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,056 - INFO - Processing document AHLKQ7CAY5MNKJZVNNX6LMU2BHTQ...
2025-12-30 22:27:31,058 - ERROR - Error processing document_id AHLKQ7CAY5MNKJZVNNX6LMU2BHTQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,059 - INFO - Processing document AEABITMY564SXYKBSZMY6J6WIWNA...
2025-12-30 22:27:31,061 - ERROR - Error processing document_id AEABITMY564SXYKBSZMY6J6WIWNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,062 - INFO - Processing document AEWY63WVFWEP5UGIXFZHQ5CVGEVQ...
2025-12-30 22:27:31,064 - ERROR - Error processing document_id AEWY63WVFWEP5UGIXFZHQ5CVGEVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,065 - INFO - Processing document AG7WE2ARUS2RKIUW6Z4CQWOJVUNA...
2025-12-30 22:27:31,067 - ERROR - Error processing document_id AG7WE2ARUS2RKIUW6Z4CQWOJVUNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,069 - INFO - Processing document AFQQU6UWJ77PM5ILEXMUY7KJPDHA...
2025-12-30 22:27:31,070 - ERROR - Error processing document_id AFQQU6UWJ77PM5ILEXMUY7KJPDHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,072 - INFO - Processing document AEQJGHLIUSCJKDTWR2YKOYKRC36A...
2025-12-30 22:27:31,073 - ERROR - Error processing document_id AEQJGHLIUSCJKDTWR2YKOYKRC36A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,075 - INFO - Processing document AGEU7G52QXBK7AAZBRJMM5QSUYGQ...
2025-12-30 22:27:31,078 - ERROR - Error processing document_id AGEU7G52QXBK7AAZBRJMM5QSUYGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,079 - INFO - Processing document AFY5AYN6BDZBALVVV66HJVO3BHSQ...
2025-12-30 22:27:31,082 - ERROR - Error processing document_id AFY5AYN6BDZBALVVV66HJVO3BHSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,083 - INFO - Processing document AFM3U2RNHC7WXLLVS4DGNABU46CQ...
2025-12-30 22:27:31,085 - ERROR - Error processing document_id AFM3U2RNHC7WXLLVS4DGNABU46CQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,086 - INFO - Processing document AHNZRQNJHNSBVKERNXSWNLKOZ6YQ...
2025-12-30 22:27:31,089 - ERROR - Error processing document_id AHNZRQNJHNSBVKERNXSWNLKOZ6YQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,091 - INFO - Processing document AE4P2JU3TZNH5PZZUHKF6J6CIM4A...
2025-12-30 22:27:31,093 - ERROR - Error processing document_id AE4P2JU3TZNH5PZZUHKF6J6CIM4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,094 - INFO - Processing document AGLVEAUZVEJRP5RAYPUE5EXKVL6A...
2025-12-30 22:27:31,096 - ERROR - Error processing document_id AGLVEAUZVEJRP5RAYPUE5EXKVL6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,097 - INFO - Processing document AHZIHNW6OOJ4YJJN4WVC46M36BIQ...
2025-12-30 22:27:31,100 - ERROR - Error processing document_id AHZIHNW6OOJ4YJJN4WVC46M36BIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,101 - INFO - Processing document AHLJV2M4NGKYJP27BUKFBHLRVOUA...
2025-12-30 22:27:31,103 - ERROR - Error processing document_id AHLJV2M4NGKYJP27BUKFBHLRVOUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,104 - INFO - Processing document AHHVKDWRY7X643XN7J3PMZK3DXPQ...
2025-12-30 22:27:31,108 - ERROR - Error processing document_id AHHVKDWRY7X643XN7J3PMZK3DXPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,109 - INFO - Processing document AGBZAHE4K7LLIO6KT4XL4I2UGYRQ...
2025-12-30 22:27:31,111 - ERROR - Error processing document_id AGBZAHE4K7LLIO6KT4XL4I2UGYRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,112 - INFO - Processing document AH45D7Q7LGLWZA3YOHOYP3NOZBKA...
2025-12-30 22:27:31,114 - ERROR - Error processing document_id AH45D7Q7LGLWZA3YOHOYP3NOZBKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,115 - INFO - Processing document AEJVCG5LPO4AITZXXLCT6A2ILDRA...
2025-12-30 22:27:31,117 - ERROR - Error processing document_id AEJVCG5LPO4AITZXXLCT6A2ILDRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,119 - INFO - Processing document AHLD5QYHKF2KUHPCFYMYA6LYTQFQ...
2025-12-30 22:27:31,120 - ERROR - Error processing document_id AHLD5QYHKF2KUHPCFYMYA6LYTQFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,121 - INFO - Processing document AFELGBJSJ6X4AHMSIHYZEGHMRXZQ...
2025-12-30 22:27:31,124 - ERROR - Error processing document_id AFELGBJSJ6X4AHMSIHYZEGHMRXZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,126 - INFO - Processing document AHTR42Z3SZAMALR26HEBBW3WFAEQ...
2025-12-30 22:27:31,128 - ERROR - Error processing document_id AHTR42Z3SZAMALR26HEBBW3WFAEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,129 - INFO - Processing document AGDE4FL3QKETFHJRMEFD2RAJGKEA...
2025-12-30 22:27:31,132 - ERROR - Error processing document_id AGDE4FL3QKETFHJRMEFD2RAJGKEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,134 - INFO - Processing document AGJTAZBUDGPK2A3MDP2PTTO5ULLA...
2025-12-30 22:27:31,136 - ERROR - Error processing document_id AGJTAZBUDGPK2A3MDP2PTTO5ULLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,137 - INFO - Processing document AGLGI2QPUE4A5LP345MXGD5TWDKA...
2025-12-30 22:27:31,139 - ERROR - Error processing document_id AGLGI2QPUE4A5LP345MXGD5TWDKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,140 - INFO - Processing document AELUYPBOOXBCGJQUWJRVNVARMJPA...
2025-12-30 22:27:31,143 - ERROR - Error processing document_id AELUYPBOOXBCGJQUWJRVNVARMJPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,144 - INFO - Processing document AFHMXQ7GXEWBNGPU362B4VE75EJQ...
2025-12-30 22:27:31,146 - ERROR - Error processing document_id AFHMXQ7GXEWBNGPU362B4VE75EJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,148 - INFO - Processing document AF4P6A3SEKS2HCISWAKFZVIFM7NQ...
2025-12-30 22:27:31,156 - ERROR - Error processing document_id AF4P6A3SEKS2HCISWAKFZVIFM7NQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,158 - INFO - Processing document AE64KJRO3QOHPHK47RHSD4LDELHQ...
2025-12-30 22:27:31,161 - ERROR - Error processing document_id AE64KJRO3QOHPHK47RHSD4LDELHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,162 - INFO - Processing document AHCKHUPSXLHK3OU4WDTHVLLK4Y6A...
2025-12-30 22:27:31,164 - ERROR - Error processing document_id AHCKHUPSXLHK3OU4WDTHVLLK4Y6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,166 - INFO - Processing document AEYUTWRDI6DZWLXIH45QPEXKKOAA...
2025-12-30 22:27:31,167 - ERROR - Error processing document_id AEYUTWRDI6DZWLXIH45QPEXKKOAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,169 - INFO - Processing document AFPGIRTKMUDCRXCLXV3W3P24SOAA...
2025-12-30 22:27:31,171 - ERROR - Error processing document_id AFPGIRTKMUDCRXCLXV3W3P24SOAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,172 - INFO - Processing document AG4KRVIZ5YYSP7R45ZRB2OWXCNDA...
2025-12-30 22:27:31,174 - ERROR - Error processing document_id AG4KRVIZ5YYSP7R45ZRB2OWXCNDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,175 - INFO - Processing document AEADLBBGA7B7VJWOQMG44JJ6GWLA...
2025-12-30 22:27:31,177 - ERROR - Error processing document_id AEADLBBGA7B7VJWOQMG44JJ6GWLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,178 - INFO - Processing document AEAR4AW75AVXQZ6FIWS6GQ5LZOLA...
2025-12-30 22:27:31,180 - ERROR - Error processing document_id AEAR4AW75AVXQZ6FIWS6GQ5LZOLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,181 - INFO - Processing document AH6ZKID2CYZKYTCN2GO6TEVWHPLQ...
2025-12-30 22:27:31,183 - ERROR - Error processing document_id AH6ZKID2CYZKYTCN2GO6TEVWHPLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,184 - INFO - Processing document AHCEGHGYWR4RIDWR2LGQDCCLJJAA...
2025-12-30 22:27:31,187 - ERROR - Error processing document_id AHCEGHGYWR4RIDWR2LGQDCCLJJAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,189 - INFO - Processing document AFZVFDO35CN4JE23LDBQL5BTOCNA...
2025-12-30 22:27:31,190 - ERROR - Error processing document_id AFZVFDO35CN4JE23LDBQL5BTOCNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,192 - INFO - Processing document AGUFUSKCO37UHIYOSXPITMQLTBCQ...
2025-12-30 22:27:31,195 - ERROR - Error processing document_id AGUFUSKCO37UHIYOSXPITMQLTBCQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,196 - INFO - Processing document AFOUSC3M23O4QZ343KBHXB4DY5NA...
2025-12-30 22:27:31,198 - ERROR - Error processing document_id AFOUSC3M23O4QZ343KBHXB4DY5NA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,199 - INFO - Processing document AF6U623KX6QGM65V2L4EPAUZVYFA...
2025-12-30 22:27:31,202 - ERROR - Error processing document_id AF6U623KX6QGM65V2L4EPAUZVYFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,203 - INFO - Processing document AFEP23NFXJI3T44Q4XC55NT6COAQ...
2025-12-30 22:27:31,205 - ERROR - Error processing document_id AFEP23NFXJI3T44Q4XC55NT6COAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,207 - INFO - Processing document AHV2C7F5C3SWNVJAYRZOU7ORTWYA...
2025-12-30 22:27:31,209 - ERROR - Error processing document_id AHV2C7F5C3SWNVJAYRZOU7ORTWYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,211 - INFO - Processing document AHQBP2HUKEPLWV47IDHBDIMPJX5A...
2025-12-30 22:27:31,213 - ERROR - Error processing document_id AHQBP2HUKEPLWV47IDHBDIMPJX5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,215 - INFO - Processing document AEYY4B2QKL3TBBXEYIO5PG7LWA4Q...
2025-12-30 22:27:31,217 - ERROR - Error processing document_id AEYY4B2QKL3TBBXEYIO5PG7LWA4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,219 - INFO - Processing document AFNOBOT6MPQZRVCC4GX5FCBAKBKA...
2025-12-30 22:27:31,221 - ERROR - Error processing document_id AFNOBOT6MPQZRVCC4GX5FCBAKBKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,222 - INFO - Processing document AFTMTH6CJABHUGAIJIH63XQNHW6Q...
2025-12-30 22:27:31,224 - ERROR - Error processing document_id AFTMTH6CJABHUGAIJIH63XQNHW6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,225 - INFO - Processing document AH2JU5HJAIWZMR33EJSPW555CRMQ...
2025-12-30 22:27:31,227 - ERROR - Error processing document_id AH2JU5HJAIWZMR33EJSPW555CRMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,229 - INFO - Processing document AFQ6GBLHZLQKWNOTOWRSP7QD6RKA...
2025-12-30 22:27:31,230 - ERROR - Error processing document_id AFQ6GBLHZLQKWNOTOWRSP7QD6RKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,232 - INFO - Processing document AGJYUF4UDLSKBTME4FPTS2XEH2QA...
2025-12-30 22:27:31,235 - ERROR - Error processing document_id AGJYUF4UDLSKBTME4FPTS2XEH2QA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,236 - INFO - Processing document AESXHXY257AGBUDJRGOPSQPZQC7A...
2025-12-30 22:27:31,239 - ERROR - Error processing document_id AESXHXY257AGBUDJRGOPSQPZQC7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,241 - INFO - Processing document AGY74VJQOH6UCKFGUTUKNXH3SP7A...
2025-12-30 22:27:31,242 - ERROR - Error processing document_id AGY74VJQOH6UCKFGUTUKNXH3SP7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,244 - INFO - Processing document AEMAPFTBHFICHGYGSXKCWHQPTHUQ...
2025-12-30 22:27:31,246 - ERROR - Error processing document_id AEMAPFTBHFICHGYGSXKCWHQPTHUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,247 - INFO - Processing document AFRIFKMM4R5IOOPR5CZX62IX3BPQ...
2025-12-30 22:27:31,250 - ERROR - Error processing document_id AFRIFKMM4R5IOOPR5CZX62IX3BPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,252 - INFO - Processing document AH4FUZFQBYQ6UJUEEZ2KAZDTUV5A...
2025-12-30 22:27:31,254 - ERROR - Error processing document_id AH4FUZFQBYQ6UJUEEZ2KAZDTUV5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,255 - INFO - Processing document AHO6T4EPIQO72HTYGXUQUYXTBAMA...
2025-12-30 22:27:31,256 - ERROR - Error processing document_id AHO6T4EPIQO72HTYGXUQUYXTBAMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,258 - INFO - Processing document AH3EKTPRWIFUKUGEHMH3VTQJ32UQ...
2025-12-30 22:27:31,260 - ERROR - Error processing document_id AH3EKTPRWIFUKUGEHMH3VTQJ32UQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,261 - INFO - Processing document AH4ABUG2ZZZMTZR5CIZLO75BDJBQ...
2025-12-30 22:27:31,263 - ERROR - Error processing document_id AH4ABUG2ZZZMTZR5CIZLO75BDJBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,264 - INFO - Processing document AHLENCN3XWMJRIJOBGY7HTRFO3QA...
2025-12-30 22:27:31,266 - ERROR - Error processing document_id AHLENCN3XWMJRIJOBGY7HTRFO3QA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,267 - INFO - Processing document AFIIENWM5XY4MMQKPLBBOCP6LKPA...
2025-12-30 22:27:31,270 - ERROR - Error processing document_id AFIIENWM5XY4MMQKPLBBOCP6LKPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,271 - INFO - Processing document AFL3565QCDCKIMUXNBX54SWGHUOA...
2025-12-30 22:27:31,273 - ERROR - Error processing document_id AFL3565QCDCKIMUXNBX54SWGHUOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,274 - INFO - Processing document AGASXUO65F2OZAKO2ZI2MWCYXXDA...
2025-12-30 22:27:31,276 - ERROR - Error processing document_id AGASXUO65F2OZAKO2ZI2MWCYXXDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,277 - INFO - Processing document AHNJ6BZMDIG3M43YTML2CXB22WIQ...
2025-12-30 22:27:31,279 - ERROR - Error processing document_id AHNJ6BZMDIG3M43YTML2CXB22WIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,280 - INFO - Processing document AEPKTKAUAD3ZVLH4NEGWTLQJGV6Q...
2025-12-30 22:27:31,283 - ERROR - Error processing document_id AEPKTKAUAD3ZVLH4NEGWTLQJGV6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,284 - INFO - Processing document AEETGEO3WFJUBZJZC7MJJICYYW4Q...
2025-12-30 22:27:31,286 - ERROR - Error processing document_id AEETGEO3WFJUBZJZC7MJJICYYW4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,287 - INFO - Processing document AF6NBYF5QOLLQAXQFZXNAB7QAPSQ...
2025-12-30 22:27:31,289 - ERROR - Error processing document_id AF6NBYF5QOLLQAXQFZXNAB7QAPSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,291 - INFO - Processing document AGVJKRKBAQ7FYSGF2Y5XA3WPCZNA...
2025-12-30 22:27:31,293 - ERROR - Error processing document_id AGVJKRKBAQ7FYSGF2Y5XA3WPCZNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,294 - INFO - Processing document AHRR57UZPCO55EH4ETYOSRC5VK2Q...
2025-12-30 22:27:31,296 - ERROR - Error processing document_id AHRR57UZPCO55EH4ETYOSRC5VK2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,298 - INFO - Processing document AHKOM2I5YQRME7FQAMZKSUW6HO4Q...
2025-12-30 22:27:31,300 - ERROR - Error processing document_id AHKOM2I5YQRME7FQAMZKSUW6HO4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,301 - INFO - Processing document AFYVMSSEWRVG6CEOM4PF7J6SSDUA...
2025-12-30 22:27:31,303 - ERROR - Error processing document_id AFYVMSSEWRVG6CEOM4PF7J6SSDUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,305 - INFO - Processing document AG5W3NDXG56LK5C4EGXE2WUGXSRA...
2025-12-30 22:27:31,308 - ERROR - Error processing document_id AG5W3NDXG56LK5C4EGXE2WUGXSRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,309 - INFO - Processing document AG6GPIKENPZRBMYKTGNF46XUN7FA...
2025-12-30 22:27:31,311 - ERROR - Error processing document_id AG6GPIKENPZRBMYKTGNF46XUN7FA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,313 - INFO - Processing document AGL3YAW5MYLT5ORWAI2UUKZ2Q4NA...
2025-12-30 22:27:31,315 - ERROR - Error processing document_id AGL3YAW5MYLT5ORWAI2UUKZ2Q4NA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,316 - INFO - Processing document AGS3XQWR3LYGTH5OUJAGMA6HEKXQ...
2025-12-30 22:27:31,318 - ERROR - Error processing document_id AGS3XQWR3LYGTH5OUJAGMA6HEKXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,320 - INFO - Processing document AHM6PNKJS3K3GYC6MXUYVD62KUMQ...
2025-12-30 22:27:31,321 - ERROR - Error processing document_id AHM6PNKJS3K3GYC6MXUYVD62KUMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,323 - INFO - Processing document AESESBLLG3ZZISQONG5SOAEUGC3Q...
2025-12-30 22:27:31,326 - ERROR - Error processing document_id AESESBLLG3ZZISQONG5SOAEUGC3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,328 - INFO - Processing document AFUYB7S5KW5KKBNJA2H6D2G6OBEA...
2025-12-30 22:27:31,330 - ERROR - Error processing document_id AFUYB7S5KW5KKBNJA2H6D2G6OBEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,332 - INFO - Processing document AHLWTOGUGJBSTUU6ZX6E57NKJOXQ...
2025-12-30 22:27:31,335 - ERROR - Error processing document_id AHLWTOGUGJBSTUU6ZX6E57NKJOXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,336 - INFO - Processing document AGAKYI2CU64DG5THSI3TUQ4PRX6Q...
2025-12-30 22:27:31,339 - ERROR - Error processing document_id AGAKYI2CU64DG5THSI3TUQ4PRX6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,340 - INFO - Processing document AGUBKKQN2P6VEMBOGYIE7XJNS5EQ...
2025-12-30 22:27:31,343 - ERROR - Error processing document_id AGUBKKQN2P6VEMBOGYIE7XJNS5EQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,345 - INFO - Processing document AF247ULBUX74I3LJNZKJLBSXTBKQ...
2025-12-30 22:27:31,347 - ERROR - Error processing document_id AF247ULBUX74I3LJNZKJLBSXTBKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,349 - INFO - Processing document AH5KNRSD6RKBCN4F7WT6TPIYEUGA...
2025-12-30 22:27:31,351 - ERROR - Error processing document_id AH5KNRSD6RKBCN4F7WT6TPIYEUGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,353 - INFO - Processing document AGRCXV4NM763VGZGD6BQ2VGNCJNA...
2025-12-30 22:27:31,355 - ERROR - Error processing document_id AGRCXV4NM763VGZGD6BQ2VGNCJNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,357 - INFO - Processing document AGUDKYJFYAATIPZ2ETHEUXMHAOPQ...
2025-12-30 22:27:31,358 - ERROR - Error processing document_id AGUDKYJFYAATIPZ2ETHEUXMHAOPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,360 - INFO - Processing document AGNSE57QKPNWN72DB5MZVRD4XRXQ...
2025-12-30 22:27:31,363 - ERROR - Error processing document_id AGNSE57QKPNWN72DB5MZVRD4XRXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,364 - INFO - Processing document AHUMYCBWTDRYTUVNJ3HFT3TQPV4Q...
2025-12-30 22:27:31,367 - ERROR - Error processing document_id AHUMYCBWTDRYTUVNJ3HFT3TQPV4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,368 - INFO - Processing document AEPVSTEXEJXFHRDY27EOQOFKZ67A...
2025-12-30 22:27:31,371 - ERROR - Error processing document_id AEPVSTEXEJXFHRDY27EOQOFKZ67A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,372 - INFO - Processing document AFI7MUJ73DJSMG3CEAWGJGFCMZ7A...
2025-12-30 22:27:31,374 - ERROR - Error processing document_id AFI7MUJ73DJSMG3CEAWGJGFCMZ7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,375 - INFO - Processing document AG3EGZHIYJ4TPCBQUM7WUWGJWSNQ...
2025-12-30 22:27:31,379 - ERROR - Error processing document_id AG3EGZHIYJ4TPCBQUM7WUWGJWSNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,380 - INFO - Processing document AGQLIVRC4THQRBQB2UG2V4WYCJQA...
2025-12-30 22:27:31,383 - ERROR - Error processing document_id AGQLIVRC4THQRBQB2UG2V4WYCJQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,385 - INFO - Processing document AHBZ7IXCHWMLORZV7ITLXUO2NZPQ...
2025-12-30 22:27:31,388 - ERROR - Error processing document_id AHBZ7IXCHWMLORZV7ITLXUO2NZPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,389 - INFO - Processing document AEHWB6GMGB4JYW2Z3OIKBLEPDMHA...
2025-12-30 22:27:31,392 - ERROR - Error processing document_id AEHWB6GMGB4JYW2Z3OIKBLEPDMHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,394 - INFO - Processing document AE2AFDQHIB2WLJNJNDO6YTEQBKLQ...
2025-12-30 22:27:31,395 - ERROR - Error processing document_id AE2AFDQHIB2WLJNJNDO6YTEQBKLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,397 - INFO - Processing document AHTBPY6EI6JIOCLQZFQNQKWTX2FA...
2025-12-30 22:27:31,399 - ERROR - Error processing document_id AHTBPY6EI6JIOCLQZFQNQKWTX2FA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,401 - INFO - Processing document AGM5F5LB2CG7E7DL3QWM36GQEWEA...
2025-12-30 22:27:31,402 - ERROR - Error processing document_id AGM5F5LB2CG7E7DL3QWM36GQEWEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,403 - INFO - Processing document AGWZTGGIFSISTE2O5XUAHITRCVVA...
2025-12-30 22:27:31,405 - ERROR - Error processing document_id AGWZTGGIFSISTE2O5XUAHITRCVVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,407 - INFO - Processing document AG7L5FKQVXUMI7L4P4SYOL4MBUZA...
2025-12-30 22:27:31,408 - ERROR - Error processing document_id AG7L5FKQVXUMI7L4P4SYOL4MBUZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,410 - INFO - Processing document AH2H6TF2N5L7VM6CR66D6JHMODIQ...
2025-12-30 22:27:31,412 - ERROR - Error processing document_id AH2H6TF2N5L7VM6CR66D6JHMODIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,414 - INFO - Processing document AHHZ46YLJSK474FSQDFOGHGUTM2Q...
2025-12-30 22:27:31,417 - ERROR - Error processing document_id AHHZ46YLJSK474FSQDFOGHGUTM2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,418 - INFO - Processing document AHIYFCUO4NZNY5SCRJVSWZMMADAA...
2025-12-30 22:27:31,420 - ERROR - Error processing document_id AHIYFCUO4NZNY5SCRJVSWZMMADAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,421 - INFO - Processing document AFIAK36PNREZ45GPCP7HXSETRBOA...
2025-12-30 22:27:31,423 - ERROR - Error processing document_id AFIAK36PNREZ45GPCP7HXSETRBOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,425 - INFO - Processing document AGIXHQE6G5QW4R4JWWIRRNR4HBVA...
2025-12-30 22:27:31,427 - ERROR - Error processing document_id AGIXHQE6G5QW4R4JWWIRRNR4HBVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,428 - INFO - Processing document AGKWSVFT6XJCGNC4WXFVUA7S46TA...
2025-12-30 22:27:31,430 - ERROR - Error processing document_id AGKWSVFT6XJCGNC4WXFVUA7S46TA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,431 - INFO - Processing document AEQLFQRWE6XARUHHLI5GD2WCAF7Q...
2025-12-30 22:27:31,433 - ERROR - Error processing document_id AEQLFQRWE6XARUHHLI5GD2WCAF7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,435 - INFO - Processing document AFEUD6CALAUCYZ6R7ZENC2MOAVGA...
2025-12-30 22:27:31,436 - ERROR - Error processing document_id AFEUD6CALAUCYZ6R7ZENC2MOAVGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,438 - INFO - Processing document AHNTOSQFPX55GDMKJ4JROB46WLAA...
2025-12-30 22:27:31,439 - ERROR - Error processing document_id AHNTOSQFPX55GDMKJ4JROB46WLAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,441 - INFO - Processing document AETFI25FYZ6OU5ZZ77QLGT6NN2CQ...
2025-12-30 22:27:31,443 - ERROR - Error processing document_id AETFI25FYZ6OU5ZZ77QLGT6NN2CQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,444 - INFO - Processing document AEX6GC25ANZIT36EQGUAJOGXCPLQ...
2025-12-30 22:27:31,450 - ERROR - Error processing document_id AEX6GC25ANZIT36EQGUAJOGXCPLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,451 - INFO - Processing document AF2FRXKVLLSGILUAME6F6MMBTXOA...
2025-12-30 22:27:31,453 - ERROR - Error processing document_id AF2FRXKVLLSGILUAME6F6MMBTXOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,454 - INFO - Processing document AGCITTMZ7LA5L4DCBTQ4CRSF6VOA...
2025-12-30 22:27:31,457 - ERROR - Error processing document_id AGCITTMZ7LA5L4DCBTQ4CRSF6VOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,458 - INFO - Processing document AF2XPCESIX5S7FSD7H5UYCXW6M4A...
2025-12-30 22:27:31,460 - ERROR - Error processing document_id AF2XPCESIX5S7FSD7H5UYCXW6M4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,461 - INFO - Processing document AGFHHZIEFIIXJZKWRCLKISPPFGWQ...
2025-12-30 22:27:31,463 - ERROR - Error processing document_id AGFHHZIEFIIXJZKWRCLKISPPFGWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,465 - INFO - Processing document AGQMOOZOTWECPLD6N7B6VZTB4P2A...
2025-12-30 22:27:31,467 - ERROR - Error processing document_id AGQMOOZOTWECPLD6N7B6VZTB4P2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,469 - INFO - Processing document AGSANKCCCV6A4HQW3C3Q2ZEJQCZQ...
2025-12-30 22:27:31,470 - ERROR - Error processing document_id AGSANKCCCV6A4HQW3C3Q2ZEJQCZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,472 - INFO - Processing document AFE54LHEJYHZCVETT6NJXN5VYG6Q...
2025-12-30 22:27:31,474 - ERROR - Error processing document_id AFE54LHEJYHZCVETT6NJXN5VYG6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,475 - INFO - Processing document AG22K7WJCWQPGNTE4WZ3ATHWE7LQ...
2025-12-30 22:27:31,477 - ERROR - Error processing document_id AG22K7WJCWQPGNTE4WZ3ATHWE7LQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,478 - INFO - Processing document AHMKCP6BP2KY6J3UJSDEGI2WQE7Q...
2025-12-30 22:27:31,481 - ERROR - Error processing document_id AHMKCP6BP2KY6J3UJSDEGI2WQE7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,482 - INFO - Processing document AEXFT3XUGN2NEAL2DMFRYXKV3CPA...
2025-12-30 22:27:31,485 - ERROR - Error processing document_id AEXFT3XUGN2NEAL2DMFRYXKV3CPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,486 - INFO - Processing document AH7XR446FJSVNQNASDQ5MDPD6ZWA...
2025-12-30 22:27:31,487 - ERROR - Error processing document_id AH7XR446FJSVNQNASDQ5MDPD6ZWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,488 - INFO - Processing document AHPKQFHKU74ZPTH3H4ZLZEWEEJQA...
2025-12-30 22:27:31,490 - ERROR - Error processing document_id AHPKQFHKU74ZPTH3H4ZLZEWEEJQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,492 - INFO - Processing document AGYAG6M4W5PZT67GB7FHRP5AB3WA...
2025-12-30 22:27:31,493 - ERROR - Error processing document_id AGYAG6M4W5PZT67GB7FHRP5AB3WA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,495 - INFO - Processing document AGSP6LSQK32SQEJO3YVVNACPWMSQ...
2025-12-30 22:27:31,497 - ERROR - Error processing document_id AGSP6LSQK32SQEJO3YVVNACPWMSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,498 - INFO - Processing document AHIBVSE3IEWDY3MPKUQHOVE7KAKQ...
2025-12-30 22:27:31,500 - ERROR - Error processing document_id AHIBVSE3IEWDY3MPKUQHOVE7KAKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,501 - INFO - Processing document AGRYGIZK4NUVZTXMXT3G5XOBBBUA...
2025-12-30 22:27:31,503 - ERROR - Error processing document_id AGRYGIZK4NUVZTXMXT3G5XOBBBUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,504 - INFO - Processing document AG4F7RCEDKQY22QFCI3RMEIL7NQA...
2025-12-30 22:27:31,506 - ERROR - Error processing document_id AG4F7RCEDKQY22QFCI3RMEIL7NQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,507 - INFO - Processing document AG7PGOEVD2ZKMF34WJSFURB2P7RA...
2025-12-30 22:27:31,509 - ERROR - Error processing document_id AG7PGOEVD2ZKMF34WJSFURB2P7RA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,511 - INFO - Processing document AFZCXSGIA7QUUUCDXTQVJOOB7YPA...
2025-12-30 22:27:31,513 - ERROR - Error processing document_id AFZCXSGIA7QUUUCDXTQVJOOB7YPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,514 - INFO - Processing document AGETC2KF5CHVA33FN5P33GH67EEQ...
2025-12-30 22:27:31,516 - ERROR - Error processing document_id AGETC2KF5CHVA33FN5P33GH67EEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,517 - INFO - Processing document AFIV4IC7VU6FRI364KGJKBPXXJFA...
2025-12-30 22:27:31,520 - ERROR - Error processing document_id AFIV4IC7VU6FRI364KGJKBPXXJFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,521 - INFO - Processing document AEROGZJEHF7WUHQS7D5YVBRJHAZQ...
2025-12-30 22:27:31,523 - ERROR - Error processing document_id AEROGZJEHF7WUHQS7D5YVBRJHAZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,524 - INFO - Processing document AHARPVCW3BBJHOPKB5CWZV5J5ZOQ...
2025-12-30 22:27:31,526 - ERROR - Error processing document_id AHARPVCW3BBJHOPKB5CWZV5J5ZOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,527 - INFO - Processing document AETVKU5REAWEVC7AW2CC4VIWCR3A...
2025-12-30 22:27:31,529 - ERROR - Error processing document_id AETVKU5REAWEVC7AW2CC4VIWCR3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,531 - INFO - Processing document AH6ZFKJVZMQ6RFUVWBAR5USDYWBA...
2025-12-30 22:27:31,532 - ERROR - Error processing document_id AH6ZFKJVZMQ6RFUVWBAR5USDYWBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,534 - INFO - Processing document AFTJJTMT7S2GWKVSRAKO3IY4TUUQ...
2025-12-30 22:27:31,536 - ERROR - Error processing document_id AFTJJTMT7S2GWKVSRAKO3IY4TUUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,537 - INFO - Processing document AE7UVZFBP4QKUHGZZZNMBNQELLEQ...
2025-12-30 22:27:31,539 - ERROR - Error processing document_id AE7UVZFBP4QKUHGZZZNMBNQELLEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,541 - INFO - Processing document AEBHUJIACHOBQKBQQ4CYTS5GZTYQ...
2025-12-30 22:27:31,543 - ERROR - Error processing document_id AEBHUJIACHOBQKBQQ4CYTS5GZTYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,544 - INFO - Processing document AH44RUNOBWHIMDEJ6LMNKJU7PPMQ...
2025-12-30 22:27:31,546 - ERROR - Error processing document_id AH44RUNOBWHIMDEJ6LMNKJU7PPMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,547 - INFO - Processing document AH6Z2YQWTP6LAMLX7VN2IXJKNVAQ...
2025-12-30 22:27:31,550 - ERROR - Error processing document_id AH6Z2YQWTP6LAMLX7VN2IXJKNVAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,551 - INFO - Processing document AHMNA5UK3V66O2V3DZSBJA4FYMOA...
2025-12-30 22:27:31,553 - ERROR - Error processing document_id AHMNA5UK3V66O2V3DZSBJA4FYMOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,554 - INFO - Processing document AHAR2ITQ3O2FJJDLIORAQ226KZHQ...
2025-12-30 22:27:31,556 - ERROR - Error processing document_id AHAR2ITQ3O2FJJDLIORAQ226KZHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,557 - INFO - Processing document AFQNGOGRVPI4THKBJ4EVHY4BM7HQ...
2025-12-30 22:27:31,560 - ERROR - Error processing document_id AFQNGOGRVPI4THKBJ4EVHY4BM7HQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,561 - INFO - Processing document AHN5RT3UIU3ONH7SF4X7NREDVBAA...
2025-12-30 22:27:31,564 - ERROR - Error processing document_id AHN5RT3UIU3ONH7SF4X7NREDVBAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,565 - INFO - Processing document AGZERWXMZLLPYBUT3K4J3H5BF6HQ...
2025-12-30 22:27:31,567 - ERROR - Error processing document_id AGZERWXMZLLPYBUT3K4J3H5BF6HQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,568 - INFO - Processing document AG3FORDYJAQA2LHP7D3MAOVMJABQ...
2025-12-30 22:27:31,570 - ERROR - Error processing document_id AG3FORDYJAQA2LHP7D3MAOVMJABQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,571 - INFO - Processing document AGGZ4CTVBEXTPSY5653EQ2ET5NZA...
2025-12-30 22:27:31,573 - ERROR - Error processing document_id AGGZ4CTVBEXTPSY5653EQ2ET5NZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,574 - INFO - Processing document AGFMKCAJTHOAI4FEFSXDW5ZXVMSQ...
2025-12-30 22:27:31,576 - ERROR - Error processing document_id AGFMKCAJTHOAI4FEFSXDW5ZXVMSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,577 - INFO - Processing document AF3L2LUMLZHL3FCVWGV3IOM3IOPQ...
2025-12-30 22:27:31,578 - ERROR - Error processing document_id AF3L2LUMLZHL3FCVWGV3IOM3IOPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,579 - INFO - Processing document AFIQDNWL5N3DF5VOKG2JAQIDABBQ...
2025-12-30 22:27:31,582 - ERROR - Error processing document_id AFIQDNWL5N3DF5VOKG2JAQIDABBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,583 - INFO - Processing document AEQMNJF7DPAPKF43TPUAKQ367G7Q...
2025-12-30 22:27:31,586 - ERROR - Error processing document_id AEQMNJF7DPAPKF43TPUAKQ367G7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,587 - INFO - Processing document AH4XKRSSSWAO6CZ5WH66V6PIQJTA...
2025-12-30 22:27:31,589 - ERROR - Error processing document_id AH4XKRSSSWAO6CZ5WH66V6PIQJTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,591 - INFO - Processing document AEYYXCMFGIDDY3OKI4CEIMGCTCOA...
2025-12-30 22:27:31,592 - ERROR - Error processing document_id AEYYXCMFGIDDY3OKI4CEIMGCTCOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,594 - INFO - Processing document AHDO42WMTRLITHD7PXKPMHH4ZB6A...
2025-12-30 22:27:31,595 - ERROR - Error processing document_id AHDO42WMTRLITHD7PXKPMHH4ZB6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,596 - INFO - Processing document AGVLWIZVAXZLKXXFFIYEP2I5LQAA...
2025-12-30 22:27:31,598 - ERROR - Error processing document_id AGVLWIZVAXZLKXXFFIYEP2I5LQAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,599 - INFO - Processing document AHUO5YNDUNR6XRCCNLAB6EXFFTGA...
2025-12-30 22:27:31,601 - ERROR - Error processing document_id AHUO5YNDUNR6XRCCNLAB6EXFFTGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,603 - INFO - Processing document AHGZC6ZN7CPMMNTSJYJMFSSANDVA...
2025-12-30 22:27:31,605 - ERROR - Error processing document_id AHGZC6ZN7CPMMNTSJYJMFSSANDVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,606 - INFO - Processing document AHFPRPGZHYZE6XT257EQ65ATKJ3Q...
2025-12-30 22:27:31,609 - ERROR - Error processing document_id AHFPRPGZHYZE6XT257EQ65ATKJ3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,610 - INFO - Processing document AFOFHPEADOYSR5GY3MKFDFPUFA3Q...
2025-12-30 22:27:31,612 - ERROR - Error processing document_id AFOFHPEADOYSR5GY3MKFDFPUFA3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,613 - INFO - Processing document AELNBR7GNU242WGB3325WUV3UVEQ...
2025-12-30 22:27:31,617 - ERROR - Error processing document_id AELNBR7GNU242WGB3325WUV3UVEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,618 - INFO - Processing document AGJOYLQKHZ7X7JUYU5HTC5LTWJHA...
2025-12-30 22:27:31,621 - ERROR - Error processing document_id AGJOYLQKHZ7X7JUYU5HTC5LTWJHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,622 - INFO - Processing document AHGMM77R7NADVGMWMHLLL7C3FNPA...
2025-12-30 22:27:31,624 - ERROR - Error processing document_id AHGMM77R7NADVGMWMHLLL7C3FNPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,626 - INFO - Processing document AE6RHG5E4YHCJIP46UAOJD644CKQ...
2025-12-30 22:27:31,627 - ERROR - Error processing document_id AE6RHG5E4YHCJIP46UAOJD644CKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,628 - INFO - Processing document AEJRQS2JO42K7DCYJTTIU4C7HXJA...
2025-12-30 22:27:31,631 - ERROR - Error processing document_id AEJRQS2JO42K7DCYJTTIU4C7HXJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,633 - INFO - Processing document AE6U2ZCGCBRDU3TT3PVH44ITRLOA...
2025-12-30 22:27:31,635 - ERROR - Error processing document_id AE6U2ZCGCBRDU3TT3PVH44ITRLOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,636 - INFO - Processing document AEVAZGP6F6GUGOKER7U6KUAWNRBA...
2025-12-30 22:27:31,638 - ERROR - Error processing document_id AEVAZGP6F6GUGOKER7U6KUAWNRBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,640 - INFO - Processing document AGEER2DYRRZSP24SG5FXVCD2ECEA...
2025-12-30 22:27:31,641 - ERROR - Error processing document_id AGEER2DYRRZSP24SG5FXVCD2ECEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,642 - INFO - Processing document AFXXZJVGETF24ZGDKHYHUBYIH47Q...
2025-12-30 22:27:31,644 - ERROR - Error processing document_id AFXXZJVGETF24ZGDKHYHUBYIH47Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,646 - INFO - Processing document AHGRKJQQM3GQDZRIVWKTYEEMTBOQ...
2025-12-30 22:27:31,648 - ERROR - Error processing document_id AHGRKJQQM3GQDZRIVWKTYEEMTBOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,649 - INFO - Processing document AGIZTYYZBBQ5KMWWJDBORRIFVSOA...
2025-12-30 22:27:31,651 - ERROR - Error processing document_id AGIZTYYZBBQ5KMWWJDBORRIFVSOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,652 - INFO - Processing document AFR34VATA3I43FGP3RQVEBA6TO4A...
2025-12-30 22:27:31,655 - ERROR - Error processing document_id AFR34VATA3I43FGP3RQVEBA6TO4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,656 - INFO - Processing document AEKIT3VLPVB2Q5SOPLVBNF647B2A...
2025-12-30 22:27:31,658 - ERROR - Error processing document_id AEKIT3VLPVB2Q5SOPLVBNF647B2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,659 - INFO - Processing document AHGFZQ4P5SXNDAZTOF6E2CIEMGTA...
2025-12-30 22:27:31,661 - ERROR - Error processing document_id AHGFZQ4P5SXNDAZTOF6E2CIEMGTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,663 - INFO - Processing document AEJZD67XLMEY5KNEBQ5UQ46EZCQA...
2025-12-30 22:27:31,666 - ERROR - Error processing document_id AEJZD67XLMEY5KNEBQ5UQ46EZCQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,667 - INFO - Processing document AG3DVZITBYMPL3ZPAWBMQSHL3QYQ...
2025-12-30 22:27:31,669 - ERROR - Error processing document_id AG3DVZITBYMPL3ZPAWBMQSHL3QYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,671 - INFO - Processing document AEZP44HCL3YHG46TQPTLKUWLVW5Q...
2025-12-30 22:27:31,673 - ERROR - Error processing document_id AEZP44HCL3YHG46TQPTLKUWLVW5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,675 - INFO - Processing document AGUQKOGHR35GDIS5IESSIDXR7D6Q...
2025-12-30 22:27:31,677 - ERROR - Error processing document_id AGUQKOGHR35GDIS5IESSIDXR7D6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,678 - INFO - Processing document AHK7NSIL76UKGXKKWGEZXQGUFHEA...
2025-12-30 22:27:31,681 - ERROR - Error processing document_id AHK7NSIL76UKGXKKWGEZXQGUFHEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,683 - INFO - Processing document AF432DS2KIULEQFWJCB6AKMXHE2A...
2025-12-30 22:27:31,685 - ERROR - Error processing document_id AF432DS2KIULEQFWJCB6AKMXHE2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,686 - INFO - Processing document AFLGINDB6PJ2HFPHAESCIYMHOHQA...
2025-12-30 22:27:31,688 - ERROR - Error processing document_id AFLGINDB6PJ2HFPHAESCIYMHOHQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,690 - INFO - Processing document AFMWIOYEHBKITEP3OCYTSON7UWAQ...
2025-12-30 22:27:31,692 - ERROR - Error processing document_id AFMWIOYEHBKITEP3OCYTSON7UWAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,693 - INFO - Processing document AGTIO2WD5NFWNXTFO57MJYCA5KWA...
2025-12-30 22:27:31,696 - ERROR - Error processing document_id AGTIO2WD5NFWNXTFO57MJYCA5KWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,697 - INFO - Processing document AEDT5EC7SLKP3WBVF6YQQWFM4OPQ...
2025-12-30 22:27:31,701 - ERROR - Error processing document_id AEDT5EC7SLKP3WBVF6YQQWFM4OPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,702 - INFO - Processing document AGF3GTGWVWB7QONIOHQMYOVQFAJA...
2025-12-30 22:27:31,704 - ERROR - Error processing document_id AGF3GTGWVWB7QONIOHQMYOVQFAJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,705 - INFO - Processing document AHFMRBU6X2VX4YJNTSP4R4SP5ZRQ...
2025-12-30 22:27:31,708 - ERROR - Error processing document_id AHFMRBU6X2VX4YJNTSP4R4SP5ZRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,709 - INFO - Processing document AF5QRLB244L5DAVBAGB4F4ASQT5Q...
2025-12-30 22:27:31,711 - ERROR - Error processing document_id AF5QRLB244L5DAVBAGB4F4ASQT5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,712 - INFO - Processing document AGFPQAYCF25GAI7K2IUQ73FDVHBQ...
2025-12-30 22:27:31,714 - ERROR - Error processing document_id AGFPQAYCF25GAI7K2IUQ73FDVHBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,716 - INFO - Processing document AFWBS3TNTLMEV4J2DO4E2J7YLLRA...
2025-12-30 22:27:31,718 - ERROR - Error processing document_id AFWBS3TNTLMEV4J2DO4E2J7YLLRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,719 - INFO - Processing document AF4V25WZ4KWQ4N6UCEFHBG6URTJA...
2025-12-30 22:27:31,722 - ERROR - Error processing document_id AF4V25WZ4KWQ4N6UCEFHBG6URTJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,723 - INFO - Processing document AGZQCDVIXBBPPBGCZ24R6JZWF2YA...
2025-12-30 22:27:31,725 - ERROR - Error processing document_id AGZQCDVIXBBPPBGCZ24R6JZWF2YA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,727 - INFO - Processing document AGQRDSM3OOWP2HJUZMSXDL63B2PA...
2025-12-30 22:27:31,729 - ERROR - Error processing document_id AGQRDSM3OOWP2HJUZMSXDL63B2PA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,731 - INFO - Processing document AFMIIYKWWCQGBNEDPNYX5OCYE3KQ...
2025-12-30 22:27:31,733 - ERROR - Error processing document_id AFMIIYKWWCQGBNEDPNYX5OCYE3KQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,735 - INFO - Processing document AFLQET7I7SDY57D24DYPY644Q6KQ...
2025-12-30 22:27:31,737 - ERROR - Error processing document_id AFLQET7I7SDY57D24DYPY644Q6KQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,739 - INFO - Processing document AEOIX7564BPQETUKABZCIQ6TAPJA...
2025-12-30 22:27:31,741 - ERROR - Error processing document_id AEOIX7564BPQETUKABZCIQ6TAPJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,742 - INFO - Processing document AGQ4JE3J3SN57GE46FVLTNL3YPVQ...
2025-12-30 22:27:31,745 - ERROR - Error processing document_id AGQ4JE3J3SN57GE46FVLTNL3YPVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,746 - INFO - Processing document AGQEJRSIMGUMDSFHM2IUMGEAOKGQ...
2025-12-30 22:27:31,748 - ERROR - Error processing document_id AGQEJRSIMGUMDSFHM2IUMGEAOKGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,750 - INFO - Processing document AH5JNZWP66X7DNYBIKQXH247DTDQ...
2025-12-30 22:27:31,751 - ERROR - Error processing document_id AH5JNZWP66X7DNYBIKQXH247DTDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,753 - INFO - Processing document AHIGMCBFM3676424O3YQ4JWXOJBA...
2025-12-30 22:27:31,755 - ERROR - Error processing document_id AHIGMCBFM3676424O3YQ4JWXOJBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,756 - INFO - Processing document AHPSCUG26LH4Y4CSGTZUSIPCXKRA...
2025-12-30 22:27:31,758 - ERROR - Error processing document_id AHPSCUG26LH4Y4CSGTZUSIPCXKRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,760 - INFO - Processing document AE553QIDXG63LHHBD3ET5J6G4BRA...
2025-12-30 22:27:31,762 - ERROR - Error processing document_id AE553QIDXG63LHHBD3ET5J6G4BRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,763 - INFO - Processing document AFHJ75ULPJHIGXTU4IUTV3ELYZEQ...
2025-12-30 22:27:31,765 - ERROR - Error processing document_id AFHJ75ULPJHIGXTU4IUTV3ELYZEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,767 - INFO - Processing document AGVI5SYVZWHBITYLRERPF2NDZ7MA...
2025-12-30 22:27:31,769 - ERROR - Error processing document_id AGVI5SYVZWHBITYLRERPF2NDZ7MA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,770 - INFO - Processing document AEKLQBIHJXN3TC6SZSNTCWME7EJA...
2025-12-30 22:27:31,773 - ERROR - Error processing document_id AEKLQBIHJXN3TC6SZSNTCWME7EJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,774 - INFO - Processing document AGZXX6P46XTAHEZPXGICDU3D6AJA...
2025-12-30 22:27:31,776 - ERROR - Error processing document_id AGZXX6P46XTAHEZPXGICDU3D6AJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,777 - INFO - Processing document AGM7REUE2SLPGCHD5OQ6JQSBF5WA...
2025-12-30 22:27:31,779 - ERROR - Error processing document_id AGM7REUE2SLPGCHD5OQ6JQSBF5WA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,781 - INFO - Processing document AFTNI2PTKDNFFFQ3EMQTRHB532FQ...
2025-12-30 22:27:31,783 - ERROR - Error processing document_id AFTNI2PTKDNFFFQ3EMQTRHB532FQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,784 - INFO - Processing document AH4MA4GUGQMFEXA6MRNUO7GXKK3A...
2025-12-30 22:27:31,786 - ERROR - Error processing document_id AH4MA4GUGQMFEXA6MRNUO7GXKK3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,787 - INFO - Processing document AEQBZTPSQKVALA3QYBTOQF3ZX3XA...
2025-12-30 22:27:31,791 - ERROR - Error processing document_id AEQBZTPSQKVALA3QYBTOQF3ZX3XA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,792 - INFO - Processing document AHHUBO2INNEVQWVK7D3E5DZR2TIA...
2025-12-30 22:27:31,794 - ERROR - Error processing document_id AHHUBO2INNEVQWVK7D3E5DZR2TIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,796 - INFO - Processing document AH75DPMVVY73SKJNRGKUZK2RKXBA...
2025-12-30 22:27:31,799 - ERROR - Error processing document_id AH75DPMVVY73SKJNRGKUZK2RKXBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,801 - INFO - Processing document AGDQWQFMRK6E2ANODFEETENEIJHA...
2025-12-30 22:27:31,803 - ERROR - Error processing document_id AGDQWQFMRK6E2ANODFEETENEIJHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,805 - INFO - Processing document AHJISMX7DPLPCHUCVPUPFWC264RA...
2025-12-30 22:27:31,807 - ERROR - Error processing document_id AHJISMX7DPLPCHUCVPUPFWC264RA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,808 - INFO - Processing document AGUZTAHWOMETRT2JTITUPJET2LQQ...
2025-12-30 22:27:31,811 - ERROR - Error processing document_id AGUZTAHWOMETRT2JTITUPJET2LQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,813 - INFO - Processing document AFKLOSSD6HX4TCFEPWEER7YREJPA...
2025-12-30 22:27:31,815 - ERROR - Error processing document_id AFKLOSSD6HX4TCFEPWEER7YREJPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,816 - INFO - Processing document AEZZLBC2W7SGONNG4O4GA7EX3QUA...
2025-12-30 22:27:31,818 - ERROR - Error processing document_id AEZZLBC2W7SGONNG4O4GA7EX3QUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,820 - INFO - Processing document AHPICNODVVNCJRSUMTKJWXV7GLWA...
2025-12-30 22:27:31,822 - ERROR - Error processing document_id AHPICNODVVNCJRSUMTKJWXV7GLWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,823 - INFO - Processing document AHKYSHFK55HR2DUBR4E2JZFIVHLQ...
2025-12-30 22:27:31,825 - ERROR - Error processing document_id AHKYSHFK55HR2DUBR4E2JZFIVHLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,826 - INFO - Processing document AGTMZCWIWBH43TCW7UKG2YV2EKKA...
2025-12-30 22:27:31,828 - ERROR - Error processing document_id AGTMZCWIWBH43TCW7UKG2YV2EKKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,830 - INFO - Processing document AE2UE4MFTXIDF6K3CJGS6DY5L2FA...
2025-12-30 22:27:31,832 - ERROR - Error processing document_id AE2UE4MFTXIDF6K3CJGS6DY5L2FA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,833 - INFO - Processing document AFBGS5AZJW2AQZMHEEGR6XE7KL7Q...
2025-12-30 22:27:31,835 - ERROR - Error processing document_id AFBGS5AZJW2AQZMHEEGR6XE7KL7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,836 - INFO - Processing document AFMTB6XAI3YBCJVTOGKOFJU3LUOQ...
2025-12-30 22:27:31,838 - ERROR - Error processing document_id AFMTB6XAI3YBCJVTOGKOFJU3LUOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,839 - INFO - Processing document AF5VPA7HEEI3XWE5V5R4SEYQQ55A...
2025-12-30 22:27:31,842 - ERROR - Error processing document_id AF5VPA7HEEI3XWE5V5R4SEYQQ55A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,843 - INFO - Processing document AHWBAFIC3I7XDRJRSHN6JIESAFHA...
2025-12-30 22:27:31,845 - ERROR - Error processing document_id AHWBAFIC3I7XDRJRSHN6JIESAFHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,847 - INFO - Processing document AGY2HAREOXSYWLCEU7OS6RSFMWSA...
2025-12-30 22:27:31,849 - ERROR - Error processing document_id AGY2HAREOXSYWLCEU7OS6RSFMWSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,850 - INFO - Processing document AELWJW4LXVDRX4FXV3ALUSUPA4OQ...
2025-12-30 22:27:31,852 - ERROR - Error processing document_id AELWJW4LXVDRX4FXV3ALUSUPA4OQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,854 - INFO - Processing document AHXCZ6P3BEX5VCKW66KJNOCN45NQ...
2025-12-30 22:27:31,856 - ERROR - Error processing document_id AHXCZ6P3BEX5VCKW66KJNOCN45NQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,857 - INFO - Processing document AGAV7IY4HWGYRUIAGUZRXYUJ22DA...
2025-12-30 22:27:31,859 - ERROR - Error processing document_id AGAV7IY4HWGYRUIAGUZRXYUJ22DA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,860 - INFO - Processing document AGOHJCKSB45NCVC33FCGXBS7GFPA...
2025-12-30 22:27:31,862 - ERROR - Error processing document_id AGOHJCKSB45NCVC33FCGXBS7GFPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,864 - INFO - Processing document AGHMC4RG2ILDEOAG6WVNNDL2JTLQ...
2025-12-30 22:27:31,865 - ERROR - Error processing document_id AGHMC4RG2ILDEOAG6WVNNDL2JTLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,867 - INFO - Processing document AFTBSGIRJ2GAGBPBNQUPLNGSPF2A...
2025-12-30 22:27:31,869 - ERROR - Error processing document_id AFTBSGIRJ2GAGBPBNQUPLNGSPF2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,871 - INFO - Processing document AEI3YHB4HHAOCC3WDCL4YNOBC4CA...
2025-12-30 22:27:31,873 - ERROR - Error processing document_id AEI3YHB4HHAOCC3WDCL4YNOBC4CA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,874 - INFO - Processing document AGIOSDBFWZCPIQVGUKQLS6WFRQ3A...
2025-12-30 22:27:31,876 - ERROR - Error processing document_id AGIOSDBFWZCPIQVGUKQLS6WFRQ3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,878 - INFO - Processing document AFTI5BO7ZDM7JYLL7EG22TMUJJEQ...
2025-12-30 22:27:31,880 - ERROR - Error processing document_id AFTI5BO7ZDM7JYLL7EG22TMUJJEQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,881 - INFO - Processing document AHB3UXKMIFBPCHFL3U25H4HT2PBQ...
2025-12-30 22:27:31,883 - ERROR - Error processing document_id AHB3UXKMIFBPCHFL3U25H4HT2PBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,885 - INFO - Processing document AHV7PEC77IZ5RX2D7GASJQN7BHXA...
2025-12-30 22:27:31,886 - ERROR - Error processing document_id AHV7PEC77IZ5RX2D7GASJQN7BHXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,888 - INFO - Processing document AEZX2674HRPYN6ZSUTCZ3CRWWCOA...
2025-12-30 22:27:31,889 - ERROR - Error processing document_id AEZX2674HRPYN6ZSUTCZ3CRWWCOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,891 - INFO - Processing document AHWE2L3ZSXVWQ7BZKOIHABHSVU6A...
2025-12-30 22:27:31,893 - ERROR - Error processing document_id AHWE2L3ZSXVWQ7BZKOIHABHSVU6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,894 - INFO - Processing document AFUS6JV5VTKFF7C54JUHAO5SPFPA...
2025-12-30 22:27:31,896 - ERROR - Error processing document_id AFUS6JV5VTKFF7C54JUHAO5SPFPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,897 - INFO - Processing document AFYBCW45GFXEBR3JB6FUANYEJUQQ...
2025-12-30 22:27:31,899 - ERROR - Error processing document_id AFYBCW45GFXEBR3JB6FUANYEJUQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,900 - INFO - Processing document AHLFIFS7AXKE4ZUNSJ6G6NP2YE3Q...
2025-12-30 22:27:31,902 - ERROR - Error processing document_id AHLFIFS7AXKE4ZUNSJ6G6NP2YE3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,904 - INFO - Processing document AGQJNLHNTHRALVRUPTKQED3DFEVQ...
2025-12-30 22:27:31,906 - ERROR - Error processing document_id AGQJNLHNTHRALVRUPTKQED3DFEVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,907 - INFO - Processing document AGFGW3TE6Q2LAF726AOJX6GBCMWA...
2025-12-30 22:27:31,910 - ERROR - Error processing document_id AGFGW3TE6Q2LAF726AOJX6GBCMWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,911 - INFO - Processing document AEYJHJSQ2GKZDEZMTAXEKOBOMXVA...
2025-12-30 22:27:31,913 - ERROR - Error processing document_id AEYJHJSQ2GKZDEZMTAXEKOBOMXVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,914 - INFO - Processing document AE43JDTOAHNTZIMRP5ROITQP2RVQ...
2025-12-30 22:27:31,917 - ERROR - Error processing document_id AE43JDTOAHNTZIMRP5ROITQP2RVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,918 - INFO - Processing document AGGXGGDPQJWMGTM5W2UB2AISHBRA...
2025-12-30 22:27:31,920 - ERROR - Error processing document_id AGGXGGDPQJWMGTM5W2UB2AISHBRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,921 - INFO - Processing document AF5PNNIFXVKKU7PNEG4NVQYE7FYA...
2025-12-30 22:27:31,923 - ERROR - Error processing document_id AF5PNNIFXVKKU7PNEG4NVQYE7FYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,925 - INFO - Processing document AFHYWBO5AXFYTKTNL5X6AVBUAHWA...
2025-12-30 22:27:31,927 - ERROR - Error processing document_id AFHYWBO5AXFYTKTNL5X6AVBUAHWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,928 - INFO - Processing document AFLAKKRJ36SWUCM65O7HJMV3NUKQ...
2025-12-30 22:27:31,930 - ERROR - Error processing document_id AFLAKKRJ36SWUCM65O7HJMV3NUKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,932 - INFO - Processing document AERZZTZO3JB5IXSMULBX5GHFCLSA...
2025-12-30 22:27:31,935 - ERROR - Error processing document_id AERZZTZO3JB5IXSMULBX5GHFCLSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,937 - INFO - Processing document AGFXAW2AWST7TFNKBMJLQCISGSXA...
2025-12-30 22:27:31,938 - ERROR - Error processing document_id AGFXAW2AWST7TFNKBMJLQCISGSXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,940 - INFO - Processing document AHSWOEKNYEWSO7JG47ECXR4YGTTA...
2025-12-30 22:27:31,942 - ERROR - Error processing document_id AHSWOEKNYEWSO7JG47ECXR4YGTTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,944 - INFO - Processing document AFVRQNBYC72LL4EJCAZEJJOLJWIQ...
2025-12-30 22:27:31,946 - ERROR - Error processing document_id AFVRQNBYC72LL4EJCAZEJJOLJWIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,948 - INFO - Processing document AGLKDL45I4DSJ24SXR2LMZ7XOM7Q...
2025-12-30 22:27:31,950 - ERROR - Error processing document_id AGLKDL45I4DSJ24SXR2LMZ7XOM7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,951 - INFO - Processing document AE4XWDMQ4GPQ2CUMNBZRU47S7EOQ...
2025-12-30 22:27:31,954 - ERROR - Error processing document_id AE4XWDMQ4GPQ2CUMNBZRU47S7EOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,955 - INFO - Processing document AHRGTIMQO47C2VLJILIDU53BQKSA...
2025-12-30 22:27:31,958 - ERROR - Error processing document_id AHRGTIMQO47C2VLJILIDU53BQKSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,959 - INFO - Processing document AGVJGHT6I2ZCTC3IAVSKKS6OYVIA...
2025-12-30 22:27:31,961 - ERROR - Error processing document_id AGVJGHT6I2ZCTC3IAVSKKS6OYVIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,963 - INFO - Processing document AHQTVUFJQ4CVXXJ2IWP2JTL3RNRQ...
2025-12-30 22:27:31,964 - ERROR - Error processing document_id AHQTVUFJQ4CVXXJ2IWP2JTL3RNRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,966 - INFO - Processing document AEV34LMCHXB3XPA5LZUH3SZTR32A...
2025-12-30 22:27:31,967 - ERROR - Error processing document_id AEV34LMCHXB3XPA5LZUH3SZTR32A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,969 - INFO - Processing document AGNNZPMHDYQ2UGELEIHLWRORRR2A...
2025-12-30 22:27:31,971 - ERROR - Error processing document_id AGNNZPMHDYQ2UGELEIHLWRORRR2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,972 - INFO - Processing document AHFRAASH4BVPTCCUGNHH5APBSR4A...
2025-12-30 22:27:31,974 - ERROR - Error processing document_id AHFRAASH4BVPTCCUGNHH5APBSR4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,975 - INFO - Processing document AERWW5VNVWL2EAV5OYB72W3ZCLSA...
2025-12-30 22:27:31,977 - ERROR - Error processing document_id AERWW5VNVWL2EAV5OYB72W3ZCLSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,979 - INFO - Processing document AGGBUGZGXX6ST226PIUFRMWAPIDA...
2025-12-30 22:27:31,982 - ERROR - Error processing document_id AGGBUGZGXX6ST226PIUFRMWAPIDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,983 - INFO - Processing document AGDAJPTINRY4CUXIDXTX5FYBOQ3Q...
2025-12-30 22:27:31,985 - ERROR - Error processing document_id AGDAJPTINRY4CUXIDXTX5FYBOQ3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,986 - INFO - Processing document AFSIVZMMUEWXWTBFVJSFRIPRYMKA...
2025-12-30 22:27:31,989 - ERROR - Error processing document_id AFSIVZMMUEWXWTBFVJSFRIPRYMKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,990 - INFO - Processing document AEVQJ6GXVPTMBICAAGMDDGQ5QKOQ...
2025-12-30 22:27:31,992 - ERROR - Error processing document_id AEVQJ6GXVPTMBICAAGMDDGQ5QKOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,994 - INFO - Processing document AFM3RFEXWOC6E46QMKLN5LABTPIA...
2025-12-30 22:27:31,996 - ERROR - Error processing document_id AFM3RFEXWOC6E46QMKLN5LABTPIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:31,997 - INFO - Processing document AFRGBSNQZOV2BVPKCDTHYYN572NA...
2025-12-30 22:27:32,001 - ERROR - Error processing document_id AFRGBSNQZOV2BVPKCDTHYYN572NA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,002 - INFO - Processing document AFPP3OQTVYOSEF4T6S5Q77KOBO3A...
2025-12-30 22:27:32,005 - ERROR - Error processing document_id AFPP3OQTVYOSEF4T6S5Q77KOBO3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,006 - INFO - Processing document AEOWPZKBQ6UY2KQH4I4MEYR2MKNA...
2025-12-30 22:27:32,008 - ERROR - Error processing document_id AEOWPZKBQ6UY2KQH4I4MEYR2MKNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,010 - INFO - Processing document AFETKZOTPBQALJ7EBGMTEZTB25FA...
2025-12-30 22:27:32,011 - ERROR - Error processing document_id AFETKZOTPBQALJ7EBGMTEZTB25FA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,013 - INFO - Processing document AEARHVAN35NQUBFLHTLQTCYYKYXA...
2025-12-30 22:27:32,015 - ERROR - Error processing document_id AEARHVAN35NQUBFLHTLQTCYYKYXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,016 - INFO - Processing document AE64FRJFR6C3YXZBQQ6WZHEGLWLQ...
2025-12-30 22:27:32,020 - ERROR - Error processing document_id AE64FRJFR6C3YXZBQQ6WZHEGLWLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,021 - INFO - Processing document AEHAES7KNKK64RUKPZSOPOOSQUAQ...
2025-12-30 22:27:32,023 - ERROR - Error processing document_id AEHAES7KNKK64RUKPZSOPOOSQUAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,024 - INFO - Processing document AH345L4AXYLP2UOFN2W6TYLRIFQQ...
2025-12-30 22:27:32,026 - ERROR - Error processing document_id AH345L4AXYLP2UOFN2W6TYLRIFQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,027 - INFO - Processing document AEBMSOQLX55ISSFOJT37IYIMIQGQ...
2025-12-30 22:27:32,030 - ERROR - Error processing document_id AEBMSOQLX55ISSFOJT37IYIMIQGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,031 - INFO - Processing document AHWYSJIMUEIUBSBWKASEA5WOSCGA...
2025-12-30 22:27:32,033 - ERROR - Error processing document_id AHWYSJIMUEIUBSBWKASEA5WOSCGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,035 - INFO - Processing document AHES2DCSZ2VK5PRYIYLLIQEI6XZA...
2025-12-30 22:27:32,039 - ERROR - Error processing document_id AHES2DCSZ2VK5PRYIYLLIQEI6XZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,040 - INFO - Processing document AGV6YU6AYAKHMQ5YCSPRDJBAZ3GA...
2025-12-30 22:27:32,044 - ERROR - Error processing document_id AGV6YU6AYAKHMQ5YCSPRDJBAZ3GA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,045 - INFO - Processing document AG4LTKFLRI5BWKCN3IFBKOMZIXHA...
2025-12-30 22:27:32,047 - ERROR - Error processing document_id AG4LTKFLRI5BWKCN3IFBKOMZIXHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,048 - INFO - Processing document AFTCHNNBEIKB5KFKUWMRYNMUUXXA...
2025-12-30 22:27:32,050 - ERROR - Error processing document_id AFTCHNNBEIKB5KFKUWMRYNMUUXXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,052 - INFO - Processing document AHWZXPYCUYGBI65ZBE7U3RTC7KTQ...
2025-12-30 22:27:32,053 - ERROR - Error processing document_id AHWZXPYCUYGBI65ZBE7U3RTC7KTQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,054 - INFO - Processing document AE4W5W3WXOSAX2M3VHZMRSNNZ3AA...
2025-12-30 22:27:32,056 - ERROR - Error processing document_id AE4W5W3WXOSAX2M3VHZMRSNNZ3AA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,058 - INFO - Processing document AF7M3XH7DD5XPUUGMO3X7QOU3Z6A...
2025-12-30 22:27:32,061 - ERROR - Error processing document_id AF7M3XH7DD5XPUUGMO3X7QOU3Z6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,062 - INFO - Processing document AFETR2JP3ACDZU5YQWAARTONN77A...
2025-12-30 22:27:32,064 - ERROR - Error processing document_id AFETR2JP3ACDZU5YQWAARTONN77A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,066 - INFO - Processing document AGUHGEZAPR5NNJ4AFJEHUHSN6K4Q...
2025-12-30 22:27:32,068 - ERROR - Error processing document_id AGUHGEZAPR5NNJ4AFJEHUHSN6K4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,069 - INFO - Processing document AG57B2M5KKUEAWKPU72MGXS4O3VA...
2025-12-30 22:27:32,070 - ERROR - Error processing document_id AG57B2M5KKUEAWKPU72MGXS4O3VA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,071 - INFO - Processing document AG7CRZGJIJ2467R6TT7HEKZANIYQ...
2025-12-30 22:27:32,073 - ERROR - Error processing document_id AG7CRZGJIJ2467R6TT7HEKZANIYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,075 - INFO - Processing document AEYSU3OBQEFH2WSJY3KXTIILWW5A...
2025-12-30 22:27:32,079 - ERROR - Error processing document_id AEYSU3OBQEFH2WSJY3KXTIILWW5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,080 - INFO - Processing document AGSHRJKDKJMYVEXQGX7Z3JJD2MCQ...
2025-12-30 22:27:32,082 - ERROR - Error processing document_id AGSHRJKDKJMYVEXQGX7Z3JJD2MCQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,084 - INFO - Processing document AEWZYLWXI5Z3VZB2PLLFMFFCDF4Q...
2025-12-30 22:27:32,085 - ERROR - Error processing document_id AEWZYLWXI5Z3VZB2PLLFMFFCDF4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,087 - INFO - Processing document AHWOK7OWYUTKJWBK5KPC4LZUHVOQ...
2025-12-30 22:27:32,089 - ERROR - Error processing document_id AHWOK7OWYUTKJWBK5KPC4LZUHVOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,090 - INFO - Processing document AFVZA2O6RR5JNR2GEEF2UJLV7HGA...
2025-12-30 22:27:32,092 - ERROR - Error processing document_id AFVZA2O6RR5JNR2GEEF2UJLV7HGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,094 - INFO - Processing document AFAMI22UIIELLKHKENXA3IMG2QLQ...
2025-12-30 22:27:32,095 - ERROR - Error processing document_id AFAMI22UIIELLKHKENXA3IMG2QLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,097 - INFO - Processing document AFBD2EAYRHPQAUQAPPB3C6GAVX3Q...
2025-12-30 22:27:32,100 - ERROR - Error processing document_id AFBD2EAYRHPQAUQAPPB3C6GAVX3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,102 - INFO - Processing document AHHUGKXFMIR6BIX3DBCUHR4BEN4A...
2025-12-30 22:27:32,104 - ERROR - Error processing document_id AHHUGKXFMIR6BIX3DBCUHR4BEN4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,106 - INFO - Processing document AHHV4DVQ7GPHWOSROGQKKIP6C3KQ...
2025-12-30 22:27:32,108 - ERROR - Error processing document_id AHHV4DVQ7GPHWOSROGQKKIP6C3KQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,109 - INFO - Processing document AHVLW7VRMOYOHVGNDFPT4A3722PA...
2025-12-30 22:27:32,112 - ERROR - Error processing document_id AHVLW7VRMOYOHVGNDFPT4A3722PA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,113 - INFO - Processing document AHKCJXG2UQWWIRS6VYEJF2OOFCFQ...
2025-12-30 22:27:32,116 - ERROR - Error processing document_id AHKCJXG2UQWWIRS6VYEJF2OOFCFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,118 - INFO - Processing document AFZEFP54WMV5NTSSJZVHDOZV2LGQ...
2025-12-30 22:27:32,119 - ERROR - Error processing document_id AFZEFP54WMV5NTSSJZVHDOZV2LGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,121 - INFO - Processing document AEIMW2BKNTFJK7NFEWHVASC4LEYA...
2025-12-30 22:27:32,124 - ERROR - Error processing document_id AEIMW2BKNTFJK7NFEWHVASC4LEYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,125 - INFO - Processing document AGDF7ZOWM6GPX2XLDGRSDUHE7VHQ...
2025-12-30 22:27:32,127 - ERROR - Error processing document_id AGDF7ZOWM6GPX2XLDGRSDUHE7VHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,128 - INFO - Processing document AETAT3RJ5XU6NHOSGOSSJUNFYYKQ...
2025-12-30 22:27:32,130 - ERROR - Error processing document_id AETAT3RJ5XU6NHOSGOSSJUNFYYKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,131 - INFO - Processing document AHV25QXIC4BZ5MNJXPTPGVQR73VA...
2025-12-30 22:27:32,133 - ERROR - Error processing document_id AHV25QXIC4BZ5MNJXPTPGVQR73VA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,134 - INFO - Processing document AHVGYNSUM3O3ARYKIRTC64PYYZYQ...
2025-12-30 22:27:32,136 - ERROR - Error processing document_id AHVGYNSUM3O3ARYKIRTC64PYYZYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,137 - INFO - Processing document AGQRRV3PURFW32M33CFUGR4RZDNQ...
2025-12-30 22:27:32,148 - ERROR - Error processing document_id AGQRRV3PURFW32M33CFUGR4RZDNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,150 - INFO - Processing document AFJRPZYSXTNGVALSYVDRUX6PPUYQ...
2025-12-30 22:27:32,152 - ERROR - Error processing document_id AFJRPZYSXTNGVALSYVDRUX6PPUYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,154 - INFO - Processing document AHBSIUVLAGBLQI4ICTT3IZK2WY4A...
2025-12-30 22:27:32,156 - ERROR - Error processing document_id AHBSIUVLAGBLQI4ICTT3IZK2WY4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,157 - INFO - Processing document AFK7Z2Q3GK2YVGN7VEBKPGIZPJOA...
2025-12-30 22:27:32,159 - ERROR - Error processing document_id AFK7Z2Q3GK2YVGN7VEBKPGIZPJOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,160 - INFO - Processing document AHEOI7HGJKHMJWFDXHDNTKRJ6NEA...
2025-12-30 22:27:32,162 - ERROR - Error processing document_id AHEOI7HGJKHMJWFDXHDNTKRJ6NEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,164 - INFO - Processing document AH7SNDITF3FMLZUIDRJLMUQ6ESJA...
2025-12-30 22:27:32,166 - ERROR - Error processing document_id AH7SNDITF3FMLZUIDRJLMUQ6ESJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,167 - INFO - Processing document AFGZGS437NTBXSVTPBAQHS2YJ2GA...
2025-12-30 22:27:32,170 - ERROR - Error processing document_id AFGZGS437NTBXSVTPBAQHS2YJ2GA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,171 - INFO - Processing document AGSCOMCBRNZEKQACFIBCSWB3SIOA...
2025-12-30 22:27:32,173 - ERROR - Error processing document_id AGSCOMCBRNZEKQACFIBCSWB3SIOA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,175 - INFO - Processing document AEO3Q7XPILWJPQSWIRP2PWZIAINA...
2025-12-30 22:27:32,177 - ERROR - Error processing document_id AEO3Q7XPILWJPQSWIRP2PWZIAINA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,178 - INFO - Processing document AHCN36H3P6SHR6775UKGUPYNGS6Q...
2025-12-30 22:27:32,181 - ERROR - Error processing document_id AHCN36H3P6SHR6775UKGUPYNGS6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,182 - INFO - Processing document AH555SL4VBTDSU5Z42OUGNAHA3UA...
2025-12-30 22:27:32,184 - ERROR - Error processing document_id AH555SL4VBTDSU5Z42OUGNAHA3UA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,186 - INFO - Processing document AG4VGD6X2IPQDM72HUQ2HIKMVDZQ...
2025-12-30 22:27:32,188 - ERROR - Error processing document_id AG4VGD6X2IPQDM72HUQ2HIKMVDZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,189 - INFO - Processing document AGZS3N525JHU2V3LTECIJUZFU3BA...
2025-12-30 22:27:32,192 - ERROR - Error processing document_id AGZS3N525JHU2V3LTECIJUZFU3BA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,193 - INFO - Processing document AGVZFZTN4MW2CXG7R34S6N3NTB6A...
2025-12-30 22:27:32,197 - ERROR - Error processing document_id AGVZFZTN4MW2CXG7R34S6N3NTB6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,198 - INFO - Processing document AEAVZFWQ2AFI24DKWYJGQNB443WQ...
2025-12-30 22:27:32,201 - ERROR - Error processing document_id AEAVZFWQ2AFI24DKWYJGQNB443WQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,203 - INFO - Processing document AFCAUNR6FINKGMRSRFNRBIBHOGTA...
2025-12-30 22:27:32,206 - ERROR - Error processing document_id AFCAUNR6FINKGMRSRFNRBIBHOGTA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,207 - INFO - Processing document AGSMTCCIUPEPE6UXP5YUJ3KVQN3Q...
2025-12-30 22:27:32,209 - ERROR - Error processing document_id AGSMTCCIUPEPE6UXP5YUJ3KVQN3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,211 - INFO - Processing document AEGUQS7N244RJHKQDYOE74K67NXQ...
2025-12-30 22:27:32,214 - ERROR - Error processing document_id AEGUQS7N244RJHKQDYOE74K67NXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,215 - INFO - Processing document AFQHN6KJ5BGEKXAH7Z5KXKF5CG5Q...
2025-12-30 22:27:32,218 - ERROR - Error processing document_id AFQHN6KJ5BGEKXAH7Z5KXKF5CG5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,220 - INFO - Processing document AHVA3FI44KMTDRWUGAVXWKEJ5AAQ...
2025-12-30 22:27:32,224 - ERROR - Error processing document_id AHVA3FI44KMTDRWUGAVXWKEJ5AAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,226 - INFO - Processing document AHM7BCRFFOEMTWG2T6MFEP3CHJQQ...
2025-12-30 22:27:32,228 - ERROR - Error processing document_id AHM7BCRFFOEMTWG2T6MFEP3CHJQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,229 - INFO - Processing document AGSILWSPM5F3LAZMEA3RM3X73WQQ...
2025-12-30 22:27:32,231 - ERROR - Error processing document_id AGSILWSPM5F3LAZMEA3RM3X73WQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,233 - INFO - Processing document AH5T6ZKFCPQZGX6OUKHNYVGHALVA...
2025-12-30 22:27:32,235 - ERROR - Error processing document_id AH5T6ZKFCPQZGX6OUKHNYVGHALVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,236 - INFO - Processing document AGJYFX7AMPAQASAVWMZRK4UAQOGA...
2025-12-30 22:27:32,238 - ERROR - Error processing document_id AGJYFX7AMPAQASAVWMZRK4UAQOGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,239 - INFO - Processing document AFO52FR7YNYKS65XCQ2RWDSPVQ4A...
2025-12-30 22:27:32,242 - ERROR - Error processing document_id AFO52FR7YNYKS65XCQ2RWDSPVQ4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,244 - INFO - Processing document AGKK6BC2EOI2E72W53FOUXVPUD6Q...
2025-12-30 22:27:32,246 - ERROR - Error processing document_id AGKK6BC2EOI2E72W53FOUXVPUD6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,247 - INFO - Processing document AGIHFHBLA7OGZT5NOXFPIILNQCKQ...
2025-12-30 22:27:32,249 - ERROR - Error processing document_id AGIHFHBLA7OGZT5NOXFPIILNQCKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,250 - INFO - Processing document AHMVZ5BXA5BXMPHH7EUEOALV5C7Q...
2025-12-30 22:27:32,252 - ERROR - Error processing document_id AHMVZ5BXA5BXMPHH7EUEOALV5C7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,254 - INFO - Processing document AHMQDRH27XV4XTN3WVCY4KOILNNA...
2025-12-30 22:27:32,256 - ERROR - Error processing document_id AHMQDRH27XV4XTN3WVCY4KOILNNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,257 - INFO - Processing document AEPZSIWDD7LVIO4J7J66G345V6EQ...
2025-12-30 22:27:32,259 - ERROR - Error processing document_id AEPZSIWDD7LVIO4J7J66G345V6EQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,260 - INFO - Processing document AH7WC2F3TWEZLE74SOB2UQRRD45A...
2025-12-30 22:27:32,262 - ERROR - Error processing document_id AH7WC2F3TWEZLE74SOB2UQRRD45A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,263 - INFO - Processing document AEXV7CUDU6CGNSVIS5IPGNSXZYUQ...
2025-12-30 22:27:32,265 - ERROR - Error processing document_id AEXV7CUDU6CGNSVIS5IPGNSXZYUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,266 - INFO - Processing document AFB2ASCACS5ZF62HEZ47DKC2QVNQ...
2025-12-30 22:27:32,267 - ERROR - Error processing document_id AFB2ASCACS5ZF62HEZ47DKC2QVNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,269 - INFO - Processing document AF2TQWQBEARXAXRGIVSSQLV7WPUQ...
2025-12-30 22:27:32,270 - ERROR - Error processing document_id AF2TQWQBEARXAXRGIVSSQLV7WPUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,271 - INFO - Processing document AEKXTCRO7YBU4MT6WTY4BKAAH5JQ...
2025-12-30 22:27:32,273 - ERROR - Error processing document_id AEKXTCRO7YBU4MT6WTY4BKAAH5JQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,274 - INFO - Processing document AE3GRCJPL7IXD22AJODJBC6OGZDA...
2025-12-30 22:27:32,276 - ERROR - Error processing document_id AE3GRCJPL7IXD22AJODJBC6OGZDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,277 - INFO - Processing document AEFCIXHEUPXN35ENU3ZROCJTUGIQ...
2025-12-30 22:27:32,279 - ERROR - Error processing document_id AEFCIXHEUPXN35ENU3ZROCJTUGIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,281 - INFO - Processing document AEWLCWKOD3JANRCNATUKIQMJ6FNQ...
2025-12-30 22:27:32,282 - ERROR - Error processing document_id AEWLCWKOD3JANRCNATUKIQMJ6FNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,284 - INFO - Processing document AE4AR33536GHJGWVUPL3SPAY6Z3A...
2025-12-30 22:27:32,285 - ERROR - Error processing document_id AE4AR33536GHJGWVUPL3SPAY6Z3A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,287 - INFO - Processing document AGFFP46FC5NNVLVNUBVTU3IEDTXA...
2025-12-30 22:27:32,289 - ERROR - Error processing document_id AGFFP46FC5NNVLVNUBVTU3IEDTXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,290 - INFO - Processing document AH63KJZUAEPZIGSYC6OXAXZJD3HQ...
2025-12-30 22:27:32,292 - ERROR - Error processing document_id AH63KJZUAEPZIGSYC6OXAXZJD3HQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,294 - INFO - Processing document AG6IH7UC7NL74VS2CX2IOUHL63CA...
2025-12-30 22:27:32,295 - ERROR - Error processing document_id AG6IH7UC7NL74VS2CX2IOUHL63CA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,296 - INFO - Processing document AHP2Q5FW7LO5HDGBLOEBADK2INRQ...
2025-12-30 22:27:32,299 - ERROR - Error processing document_id AHP2Q5FW7LO5HDGBLOEBADK2INRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,300 - INFO - Processing document AETFYI5ANZGO4W4NEG3NJ4FMON2A...
2025-12-30 22:27:32,302 - ERROR - Error processing document_id AETFYI5ANZGO4W4NEG3NJ4FMON2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,303 - INFO - Processing document AG5FGCIM6PBJX3JNDRKQMITZTIMA...
2025-12-30 22:27:32,306 - ERROR - Error processing document_id AG5FGCIM6PBJX3JNDRKQMITZTIMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,307 - INFO - Processing document AH6VFZ4XI47J5G3BSGDVFFALXIJA...
2025-12-30 22:27:32,310 - ERROR - Error processing document_id AH6VFZ4XI47J5G3BSGDVFFALXIJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,311 - INFO - Processing document AFTWA5URATBOFF5JT75W32W4QO2Q...
2025-12-30 22:27:32,314 - ERROR - Error processing document_id AFTWA5URATBOFF5JT75W32W4QO2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,315 - INFO - Processing document AGTR4DK6DYJN65MAHG72H552AADQ...
2025-12-30 22:27:32,317 - ERROR - Error processing document_id AGTR4DK6DYJN65MAHG72H552AADQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,318 - INFO - Processing document AHK4ZVMPTFGUCBNJFHRDSNMV362A...
2025-12-30 22:27:32,320 - ERROR - Error processing document_id AHK4ZVMPTFGUCBNJFHRDSNMV362A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,321 - INFO - Processing document AF2YMYY5XYNIFOIPBNVVGRHXFUAQ...
2025-12-30 22:27:32,323 - ERROR - Error processing document_id AF2YMYY5XYNIFOIPBNVVGRHXFUAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,324 - INFO - Processing document AHWK3H4EOWNZSGX6TQAFXA5YHFRQ...
2025-12-30 22:27:32,327 - ERROR - Error processing document_id AHWK3H4EOWNZSGX6TQAFXA5YHFRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,328 - INFO - Processing document AFWTT6GHVMIOXN7DNUOJVRDIX3QQ...
2025-12-30 22:27:32,330 - ERROR - Error processing document_id AFWTT6GHVMIOXN7DNUOJVRDIX3QQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,332 - INFO - Processing document AG7S5J677OCQ4KBDGQPTKXMXKJ2Q...
2025-12-30 22:27:32,335 - ERROR - Error processing document_id AG7S5J677OCQ4KBDGQPTKXMXKJ2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,336 - INFO - Processing document AHE4VSNIGVJPNHY5OHLX3HPENEZQ...
2025-12-30 22:27:32,338 - ERROR - Error processing document_id AHE4VSNIGVJPNHY5OHLX3HPENEZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,339 - INFO - Processing document AECILX3VOTY5BN57IT4TBJN3DS7Q...
2025-12-30 22:27:32,344 - ERROR - Error processing document_id AECILX3VOTY5BN57IT4TBJN3DS7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,346 - INFO - Processing document AFJUWJRBJ76R7KAEE7HA7YC472HA...
2025-12-30 22:27:32,347 - ERROR - Error processing document_id AFJUWJRBJ76R7KAEE7HA7YC472HA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,349 - INFO - Processing document AECNDW73XODITFJUYPKRF5UN4ARQ...
2025-12-30 22:27:32,351 - ERROR - Error processing document_id AECNDW73XODITFJUYPKRF5UN4ARQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,352 - INFO - Processing document AE6PG6NUBEQ7AS3MTWZJEB3VUVDQ...
2025-12-30 22:27:32,353 - ERROR - Error processing document_id AE6PG6NUBEQ7AS3MTWZJEB3VUVDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,355 - INFO - Processing document AH2ZDODWQZ72BO7K6LCDZ63YAQNQ...
2025-12-30 22:27:32,357 - ERROR - Error processing document_id AH2ZDODWQZ72BO7K6LCDZ63YAQNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,358 - INFO - Processing document AHAXTKZXG64KPAV6TPVUASJ33E4A...
2025-12-30 22:27:32,359 - ERROR - Error processing document_id AHAXTKZXG64KPAV6TPVUASJ33E4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,361 - INFO - Processing document AEXWB34NUYYTRNGYK4SJUETOZ33A...
2025-12-30 22:27:32,362 - ERROR - Error processing document_id AEXWB34NUYYTRNGYK4SJUETOZ33A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,364 - INFO - Processing document AGKM4XB7JVBERUUB4IQ7SOY3WCSA...
2025-12-30 22:27:32,365 - ERROR - Error processing document_id AGKM4XB7JVBERUUB4IQ7SOY3WCSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,367 - INFO - Processing document AGNLRIXUGGGR4E47KQDGWOG6ZGMQ...
2025-12-30 22:27:32,370 - ERROR - Error processing document_id AGNLRIXUGGGR4E47KQDGWOG6ZGMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,371 - INFO - Processing document AF4ZA45XPGWOTXU5K4EZ24ASTPWQ...
2025-12-30 22:27:32,373 - ERROR - Error processing document_id AF4ZA45XPGWOTXU5K4EZ24ASTPWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,374 - INFO - Processing document AEBDNPW4KHGPASHJLUMEAUVIYOFA...
2025-12-30 22:27:32,376 - ERROR - Error processing document_id AEBDNPW4KHGPASHJLUMEAUVIYOFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,377 - INFO - Processing document AFFKGBUPPSQWTSNYGAVRBZA2DXWA...
2025-12-30 22:27:32,379 - ERROR - Error processing document_id AFFKGBUPPSQWTSNYGAVRBZA2DXWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,380 - INFO - Processing document AG3HCENG262UDAWJWGHQ3HWA7IGA...
2025-12-30 22:27:32,382 - ERROR - Error processing document_id AG3HCENG262UDAWJWGHQ3HWA7IGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,383 - INFO - Processing document AEKKCF4VA6WXK363LQXYWF5EBPOQ...
2025-12-30 22:27:32,385 - ERROR - Error processing document_id AEKKCF4VA6WXK363LQXYWF5EBPOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,386 - INFO - Processing document AF47RH72GMFNYQJSQQWG7Y4SHSGQ...
2025-12-30 22:27:32,388 - ERROR - Error processing document_id AF47RH72GMFNYQJSQQWG7Y4SHSGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,390 - INFO - Processing document AENZAUFWDIPHZDZGQCFTKGRIJ5VA...
2025-12-30 22:27:32,391 - ERROR - Error processing document_id AENZAUFWDIPHZDZGQCFTKGRIJ5VA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,393 - INFO - Processing document AH74XLC3QBXVC6UCOAQOJH3WJYGA...
2025-12-30 22:27:32,395 - ERROR - Error processing document_id AH74XLC3QBXVC6UCOAQOJH3WJYGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,396 - INFO - Processing document AHWLIOEEWVDGKMMSO5IR5PAYT7VQ...
2025-12-30 22:27:32,398 - ERROR - Error processing document_id AHWLIOEEWVDGKMMSO5IR5PAYT7VQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,399 - INFO - Processing document AEESMMTHKZCT2BBJNQ552MGSQ7ZA...
2025-12-30 22:27:32,402 - ERROR - Error processing document_id AEESMMTHKZCT2BBJNQ552MGSQ7ZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,403 - INFO - Processing document AFERCDY2EFJKT7QUQ75GISNHTFOQ...
2025-12-30 22:27:32,406 - ERROR - Error processing document_id AFERCDY2EFJKT7QUQ75GISNHTFOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,408 - INFO - Processing document AFDAREU2GAOSVUON5FW6NAYTHC4A...
2025-12-30 22:27:32,411 - ERROR - Error processing document_id AFDAREU2GAOSVUON5FW6NAYTHC4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,413 - INFO - Processing document AFZ74FSC74LTHBNEFUXXQUQPGZ7A...
2025-12-30 22:27:32,415 - ERROR - Error processing document_id AFZ74FSC74LTHBNEFUXXQUQPGZ7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,416 - INFO - Processing document AEWKPTWJVBBIEL7EUKFUQWRQK22Q...
2025-12-30 22:27:32,418 - ERROR - Error processing document_id AEWKPTWJVBBIEL7EUKFUQWRQK22Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,420 - INFO - Processing document AHT7DZC5QF7OHDLOTANBDAYS23DA...
2025-12-30 22:27:32,421 - ERROR - Error processing document_id AHT7DZC5QF7OHDLOTANBDAYS23DA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,423 - INFO - Processing document AH5HSMUMRJRDWBPFDQ6G3EAAOSFQ...
2025-12-30 22:27:32,425 - ERROR - Error processing document_id AH5HSMUMRJRDWBPFDQ6G3EAAOSFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,426 - INFO - Processing document AG4J2QYEUQP3RQGNBYDJQ32SILYQ...
2025-12-30 22:27:32,428 - ERROR - Error processing document_id AG4J2QYEUQP3RQGNBYDJQ32SILYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,429 - INFO - Processing document AHR2B25J345P4OU6TK7RSTZWHFLA...
2025-12-30 22:27:32,431 - ERROR - Error processing document_id AHR2B25J345P4OU6TK7RSTZWHFLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,432 - INFO - Processing document AGJIKQXXM2ELQ7FVHILD76SPK22Q...
2025-12-30 22:27:32,434 - ERROR - Error processing document_id AGJIKQXXM2ELQ7FVHILD76SPK22Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,435 - INFO - Processing document AF2HXGT7ZF2O7EMSHNDTQ4OQN3RA...
2025-12-30 22:27:32,437 - ERROR - Error processing document_id AF2HXGT7ZF2O7EMSHNDTQ4OQN3RA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,439 - INFO - Processing document AFNKPUJHXI62LMB2U7OAWC7ICUXA...
2025-12-30 22:27:32,442 - ERROR - Error processing document_id AFNKPUJHXI62LMB2U7OAWC7ICUXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,444 - INFO - Processing document AF7SEGOC2AWF7UDIGAIG4STWH3DQ...
2025-12-30 22:27:32,445 - ERROR - Error processing document_id AF7SEGOC2AWF7UDIGAIG4STWH3DQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,446 - INFO - Processing document AGRA57SHKANYII4SYISVBBKNMU2Q...
2025-12-30 22:27:32,448 - ERROR - Error processing document_id AGRA57SHKANYII4SYISVBBKNMU2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,449 - INFO - Processing document AGADBPIRHJ4JSESM5CXWH4ZUBYRA...
2025-12-30 22:27:32,451 - ERROR - Error processing document_id AGADBPIRHJ4JSESM5CXWH4ZUBYRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,453 - INFO - Processing document AEYFCXNFRVL5ISYTH7AO2CZH4WZQ...
2025-12-30 22:27:32,457 - ERROR - Error processing document_id AEYFCXNFRVL5ISYTH7AO2CZH4WZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,458 - INFO - Processing document AG5PK25GTSF3Z5KDP2464Y7Z4J3Q...
2025-12-30 22:27:32,461 - ERROR - Error processing document_id AG5PK25GTSF3Z5KDP2464Y7Z4J3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,462 - INFO - Processing document AHP3LSTXZ4EKWZIGMKWKSQN4ZHPA...
2025-12-30 22:27:32,464 - ERROR - Error processing document_id AHP3LSTXZ4EKWZIGMKWKSQN4ZHPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,466 - INFO - Processing document AHE5ZXDOJL6AK2O2FVDEQPY7ESTQ...
2025-12-30 22:27:32,467 - ERROR - Error processing document_id AHE5ZXDOJL6AK2O2FVDEQPY7ESTQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,468 - INFO - Processing document AGSOG6C6SM5W7UXXVRN42OB2XY2A...
2025-12-30 22:27:32,470 - ERROR - Error processing document_id AGSOG6C6SM5W7UXXVRN42OB2XY2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,472 - INFO - Processing document AGLMH7GYYOVSH3IFQDXJZQLWZ3HQ...
2025-12-30 22:27:32,474 - ERROR - Error processing document_id AGLMH7GYYOVSH3IFQDXJZQLWZ3HQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,475 - INFO - Processing document AE2MNTWYK5NKLNH6JP2XMV5VB7GQ...
2025-12-30 22:27:32,477 - ERROR - Error processing document_id AE2MNTWYK5NKLNH6JP2XMV5VB7GQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,479 - INFO - Processing document AH7NNOOXP3T6CP6VK2V4ELFPW3HQ...
2025-12-30 22:27:32,480 - ERROR - Error processing document_id AH7NNOOXP3T6CP6VK2V4ELFPW3HQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,482 - INFO - Processing document AHD2SPGI3S373OPNPYXCYCHY3E5A...
2025-12-30 22:27:32,485 - ERROR - Error processing document_id AHD2SPGI3S373OPNPYXCYCHY3E5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,486 - INFO - Processing document AG2HZEBIHMFDMP7Q3H2JBUW5FWXA...
2025-12-30 22:27:32,489 - ERROR - Error processing document_id AG2HZEBIHMFDMP7Q3H2JBUW5FWXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,490 - INFO - Processing document AF3RENOGZJOCO24HPO75EEIF4EHQ...
2025-12-30 22:27:32,492 - ERROR - Error processing document_id AF3RENOGZJOCO24HPO75EEIF4EHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,493 - INFO - Processing document AGJKEO7CAVS472K2352YHRZEL2JA...
2025-12-30 22:27:32,497 - ERROR - Error processing document_id AGJKEO7CAVS472K2352YHRZEL2JA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,498 - INFO - Processing document AGWQGECWDKYJOJYXDUDOJMJ66ITA...
2025-12-30 22:27:32,501 - ERROR - Error processing document_id AGWQGECWDKYJOJYXDUDOJMJ66ITA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,502 - INFO - Processing document AESHPUKHUAOSHKYKQEFE4D5L2ROQ...
2025-12-30 22:27:32,505 - ERROR - Error processing document_id AESHPUKHUAOSHKYKQEFE4D5L2ROQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,506 - INFO - Processing document AG7V77KYFEMZDCT3HSHKYU2N2TSA...
2025-12-30 22:27:32,508 - ERROR - Error processing document_id AG7V77KYFEMZDCT3HSHKYU2N2TSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,510 - INFO - Processing document AGZAEU3O5UHRTK2EA44OLKFF2NGA...
2025-12-30 22:27:32,512 - ERROR - Error processing document_id AGZAEU3O5UHRTK2EA44OLKFF2NGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,514 - INFO - Processing document AEEVFGWGYBN5SNTCXGBJHVNNANNQ...
2025-12-30 22:27:32,516 - ERROR - Error processing document_id AEEVFGWGYBN5SNTCXGBJHVNNANNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,517 - INFO - Processing document AHEQDC5UXL4VPPMS4NMTDL2A2LZA...
2025-12-30 22:27:32,520 - ERROR - Error processing document_id AHEQDC5UXL4VPPMS4NMTDL2A2LZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,521 - INFO - Processing document AFZ2ZZXXCXDLWJ4JDR2VGYV64BUA...
2025-12-30 22:27:32,523 - ERROR - Error processing document_id AFZ2ZZXXCXDLWJ4JDR2VGYV64BUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,525 - INFO - Processing document AEP2IBCNZ3PEH4PNI3VYMEEMFGBQ...
2025-12-30 22:27:32,527 - ERROR - Error processing document_id AEP2IBCNZ3PEH4PNI3VYMEEMFGBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,528 - INFO - Processing document AHVNNMTVAOMADLOALKQ2GODDN6TQ...
2025-12-30 22:27:32,530 - ERROR - Error processing document_id AHVNNMTVAOMADLOALKQ2GODDN6TQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,531 - INFO - Processing document AH25FB5EAAE5OPYNROJMTZ2GFEQQ...
2025-12-30 22:27:32,535 - ERROR - Error processing document_id AH25FB5EAAE5OPYNROJMTZ2GFEQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,536 - INFO - Processing document AHPJFSRMWDXQ6UKYAR4SZERD2JQA...
2025-12-30 22:27:32,538 - ERROR - Error processing document_id AHPJFSRMWDXQ6UKYAR4SZERD2JQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,540 - INFO - Processing document AFD7O6H32EUKD3IXZWBIGBUNI65Q...
2025-12-30 22:27:32,543 - ERROR - Error processing document_id AFD7O6H32EUKD3IXZWBIGBUNI65Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,545 - INFO - Processing document AEICZD35OTDYVWA2KPYWN4PDZ2SA...
2025-12-30 22:27:32,546 - ERROR - Error processing document_id AEICZD35OTDYVWA2KPYWN4PDZ2SA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,548 - INFO - Processing document AHM2XKCUWJ4GZQNGDGNEVEN7FJYQ...
2025-12-30 22:27:32,549 - ERROR - Error processing document_id AHM2XKCUWJ4GZQNGDGNEVEN7FJYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,551 - INFO - Processing document AETQAYY2ZSBU526J4PRRFFAC6GQA...
2025-12-30 22:27:32,552 - ERROR - Error processing document_id AETQAYY2ZSBU526J4PRRFFAC6GQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,553 - INFO - Processing document AFGJBFWIW6VA46BRJ4ZGQR5D5LJQ...
2025-12-30 22:27:32,555 - ERROR - Error processing document_id AFGJBFWIW6VA46BRJ4ZGQR5D5LJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,557 - INFO - Processing document AE74C4DFANAVSBSBBXOAIXLGI5ZQ...
2025-12-30 22:27:32,558 - ERROR - Error processing document_id AE74C4DFANAVSBSBBXOAIXLGI5ZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,559 - INFO - Processing document AHI2BCZAEZ2HWM6LLYZN742HMRFQ...
2025-12-30 22:27:32,561 - ERROR - Error processing document_id AHI2BCZAEZ2HWM6LLYZN742HMRFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,563 - INFO - Processing document AH4DUK6NIIIVHPWUZ42MP5P3SAXA...
2025-12-30 22:27:32,565 - ERROR - Error processing document_id AH4DUK6NIIIVHPWUZ42MP5P3SAXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,566 - INFO - Processing document AFC4L6YCFUMKPSTZP4E4DS6E2PPA...
2025-12-30 22:27:32,569 - ERROR - Error processing document_id AFC4L6YCFUMKPSTZP4E4DS6E2PPA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,570 - INFO - Processing document AHII3COZDVFGDNAJM5VDC3THSKFA...
2025-12-30 22:27:32,573 - ERROR - Error processing document_id AHII3COZDVFGDNAJM5VDC3THSKFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,574 - INFO - Processing document AF6HUO7EHILCFURLFP4RRAQEMY2A...
2025-12-30 22:27:32,575 - ERROR - Error processing document_id AF6HUO7EHILCFURLFP4RRAQEMY2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,576 - INFO - Processing document AEH6YL4ASM5IWWVZDZLQZVZJWFUA...
2025-12-30 22:27:32,578 - ERROR - Error processing document_id AEH6YL4ASM5IWWVZDZLQZVZJWFUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,579 - INFO - Processing document AHLMMHVAPE4WQMXEWZ6J2SV6QKFQ...
2025-12-30 22:27:32,581 - ERROR - Error processing document_id AHLMMHVAPE4WQMXEWZ6J2SV6QKFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,583 - INFO - Processing document AH6IPUNIUMLCXHKFXDKJFMM6JPDA...
2025-12-30 22:27:32,584 - ERROR - Error processing document_id AH6IPUNIUMLCXHKFXDKJFMM6JPDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,586 - INFO - Processing document AFF2K4V6E6R7YQOZGODLMYB6SLXQ...
2025-12-30 22:27:32,588 - ERROR - Error processing document_id AFF2K4V6E6R7YQOZGODLMYB6SLXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,590 - INFO - Processing document AFX5PIHEVFUXGNTKJGTOTBE7CY5A...
2025-12-30 22:27:32,591 - ERROR - Error processing document_id AFX5PIHEVFUXGNTKJGTOTBE7CY5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,593 - INFO - Processing document AHNMRZUHW5F6HEGESBTZ42HYHYAA...
2025-12-30 22:27:32,595 - ERROR - Error processing document_id AHNMRZUHW5F6HEGESBTZ42HYHYAA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,596 - INFO - Processing document AGZVCJ6UZCBOVALW4D2IPZ5LBUKQ...
2025-12-30 22:27:32,598 - ERROR - Error processing document_id AGZVCJ6UZCBOVALW4D2IPZ5LBUKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,600 - INFO - Processing document AGFDUH6ELFIEPL32ILGKQJA2CW5A...
2025-12-30 22:27:32,602 - ERROR - Error processing document_id AGFDUH6ELFIEPL32ILGKQJA2CW5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,603 - INFO - Processing document AFNOAEJEWT2VHOTLYK2GDL2Q2YNQ...
2025-12-30 22:27:32,605 - ERROR - Error processing document_id AFNOAEJEWT2VHOTLYK2GDL2Q2YNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,606 - INFO - Processing document AF3IS2UKY7ZKZIORTD3B6MMEUUHA...
2025-12-30 22:27:32,608 - ERROR - Error processing document_id AF3IS2UKY7ZKZIORTD3B6MMEUUHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,610 - INFO - Processing document AHBHBD27MLASKYJLAHAN2BRSYDKA...
2025-12-30 22:27:32,613 - ERROR - Error processing document_id AHBHBD27MLASKYJLAHAN2BRSYDKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,614 - INFO - Processing document AGQW6RM5XGSFRAA4ZUXRZYVLLCCA...
2025-12-30 22:27:32,616 - ERROR - Error processing document_id AGQW6RM5XGSFRAA4ZUXRZYVLLCCA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,617 - INFO - Processing document AFU3TK3ERDCF2NTBO5BCE3U2LXFA...
2025-12-30 22:27:32,619 - ERROR - Error processing document_id AFU3TK3ERDCF2NTBO5BCE3U2LXFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,620 - INFO - Processing document AF5MEOLDJYILPFJNL77KQTZN3S2Q...
2025-12-30 22:27:32,622 - ERROR - Error processing document_id AF5MEOLDJYILPFJNL77KQTZN3S2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,623 - INFO - Processing document AG5LC5HSFYGJSK3AQGHEFHKX6ZWQ...
2025-12-30 22:27:32,625 - ERROR - Error processing document_id AG5LC5HSFYGJSK3AQGHEFHKX6ZWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,626 - INFO - Processing document AFXMH7SR7ZIZ4NFQDFILEQUMUYRQ...
2025-12-30 22:27:32,628 - ERROR - Error processing document_id AFXMH7SR7ZIZ4NFQDFILEQUMUYRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,629 - INFO - Processing document AFKQDPTVJZZVBBGRFL6EZQBCDBTQ...
2025-12-30 22:27:32,632 - ERROR - Error processing document_id AFKQDPTVJZZVBBGRFL6EZQBCDBTQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,633 - INFO - Processing document AHPUCOJLAJLZXDWO7SFEONAGAOIQ...
2025-12-30 22:27:32,635 - ERROR - Error processing document_id AHPUCOJLAJLZXDWO7SFEONAGAOIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,636 - INFO - Processing document AEVVN5LDA42ZJPGM7MCDVLJESYYA...
2025-12-30 22:27:32,637 - ERROR - Error processing document_id AEVVN5LDA42ZJPGM7MCDVLJESYYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,639 - INFO - Processing document AHVSTRKAXNRALZPRLXRPEVK4EB5A...
2025-12-30 22:27:32,641 - ERROR - Error processing document_id AHVSTRKAXNRALZPRLXRPEVK4EB5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,642 - INFO - Processing document AHKBRYFSRIZMXKTFFN26632XZBMA...
2025-12-30 22:27:32,644 - ERROR - Error processing document_id AHKBRYFSRIZMXKTFFN26632XZBMA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,645 - INFO - Processing document AH5WGCQY44BN3WJ4WXNT37P3OSGQ...
2025-12-30 22:27:32,647 - ERROR - Error processing document_id AH5WGCQY44BN3WJ4WXNT37P3OSGQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,648 - INFO - Processing document AHLJYIMWCH2AQUJVDPNAZHB5SGZA...
2025-12-30 22:27:32,650 - ERROR - Error processing document_id AHLJYIMWCH2AQUJVDPNAZHB5SGZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,651 - INFO - Processing document AGLHRQLAWKWQKEF6VRO2Y3MRS4JQ...
2025-12-30 22:27:32,653 - ERROR - Error processing document_id AGLHRQLAWKWQKEF6VRO2Y3MRS4JQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,654 - INFO - Processing document AEP6FGKTGLGXAWVO2DVBXVNYW7EQ...
2025-12-30 22:27:32,657 - ERROR - Error processing document_id AEP6FGKTGLGXAWVO2DVBXVNYW7EQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,658 - INFO - Processing document AEMGUP6RLGIZA6TOUXXAFKZ7GV7A...
2025-12-30 22:27:32,660 - ERROR - Error processing document_id AEMGUP6RLGIZA6TOUXXAFKZ7GV7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,662 - INFO - Processing document AHBWAVRW22FT4GAN2VTWYNAEQFHQ...
2025-12-30 22:27:32,664 - ERROR - Error processing document_id AHBWAVRW22FT4GAN2VTWYNAEQFHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,665 - INFO - Processing document AFUKDOBF2U5NUBWU3NO2267V7WBQ...
2025-12-30 22:27:32,666 - ERROR - Error processing document_id AFUKDOBF2U5NUBWU3NO2267V7WBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,668 - INFO - Processing document AF55NHGCCFCFDLIP7Q3WBC6RDZKA...
2025-12-30 22:27:32,671 - ERROR - Error processing document_id AF55NHGCCFCFDLIP7Q3WBC6RDZKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,672 - INFO - Processing document AGWSTTG2AZTEU7U5E3OCSB3QK5OA...
2025-12-30 22:27:32,674 - ERROR - Error processing document_id AGWSTTG2AZTEU7U5E3OCSB3QK5OA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,675 - INFO - Processing document AG47MYNOV7GGIQ3RVTC2JQAL6XPQ...
2025-12-30 22:27:32,677 - ERROR - Error processing document_id AG47MYNOV7GGIQ3RVTC2JQAL6XPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,679 - INFO - Processing document AHPFLF6UFWW5RH5ZLRTPBNP35VDA...
2025-12-30 22:27:32,684 - ERROR - Error processing document_id AHPFLF6UFWW5RH5ZLRTPBNP35VDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,686 - INFO - Processing document AE5UNWY45HIU4WNB2VFQALRY7IZQ...
2025-12-30 22:27:32,689 - ERROR - Error processing document_id AE5UNWY45HIU4WNB2VFQALRY7IZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,690 - INFO - Processing document AH5O6WWC5UGG2VKMETUAERLDY47Q...
2025-12-30 22:27:32,693 - ERROR - Error processing document_id AH5O6WWC5UGG2VKMETUAERLDY47Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,694 - INFO - Processing document AHNVIEWWCLC7E3NZU5UT36JPOBFA...
2025-12-30 22:27:32,697 - ERROR - Error processing document_id AHNVIEWWCLC7E3NZU5UT36JPOBFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,699 - INFO - Processing document AE6VHXXCXULPIRV4US4A6MTDPZNQ...
2025-12-30 22:27:32,701 - ERROR - Error processing document_id AE6VHXXCXULPIRV4US4A6MTDPZNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,703 - INFO - Processing document AGZOE6M2YGTDCHJAQNQOVKYCHM2A...
2025-12-30 22:27:32,705 - ERROR - Error processing document_id AGZOE6M2YGTDCHJAQNQOVKYCHM2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,707 - INFO - Processing document AGN2K7OZA3UQYGYPMLWM67NRPJUA...
2025-12-30 22:27:32,710 - ERROR - Error processing document_id AGN2K7OZA3UQYGYPMLWM67NRPJUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,712 - INFO - Processing document AEL3TWZNM4T36YFEHTLJJU2IUATQ...
2025-12-30 22:27:32,714 - ERROR - Error processing document_id AEL3TWZNM4T36YFEHTLJJU2IUATQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,716 - INFO - Processing document AG662FMJFCYUSYQUBY4ZBYXHVBZQ...
2025-12-30 22:27:32,719 - ERROR - Error processing document_id AG662FMJFCYUSYQUBY4ZBYXHVBZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,720 - INFO - Processing document AGDRR3IHE7QOFY255YQ2G5T42WVA...
2025-12-30 22:27:32,723 - ERROR - Error processing document_id AGDRR3IHE7QOFY255YQ2G5T42WVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,725 - INFO - Processing document AHHTPAGNL2AIEB4VPMYNSXCTTN7A...
2025-12-30 22:27:32,727 - ERROR - Error processing document_id AHHTPAGNL2AIEB4VPMYNSXCTTN7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,729 - INFO - Processing document AETQT5GNMNCKKJFROBDS55FH3YQA...
2025-12-30 22:27:32,731 - ERROR - Error processing document_id AETQT5GNMNCKKJFROBDS55FH3YQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,733 - INFO - Processing document AGUXG2WC3C44WZFN76HK6MXURGUQ...
2025-12-30 22:27:32,735 - ERROR - Error processing document_id AGUXG2WC3C44WZFN76HK6MXURGUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,736 - INFO - Processing document AHCI37PD33CY6T7L5ERKHWKKFTIQ...
2025-12-30 22:27:32,739 - ERROR - Error processing document_id AHCI37PD33CY6T7L5ERKHWKKFTIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,740 - INFO - Processing document AG5JY4JK75YVC5GM7EBNVBRSZTWQ...
2025-12-30 22:27:32,744 - ERROR - Error processing document_id AG5JY4JK75YVC5GM7EBNVBRSZTWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,746 - INFO - Processing document AFYETZTNENW7TJP4IUJ4VSI6NQEA...
2025-12-30 22:27:32,748 - ERROR - Error processing document_id AFYETZTNENW7TJP4IUJ4VSI6NQEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,749 - INFO - Processing document AG3VWMRAIDNQIEUG7PBGV5WYSL4A...
2025-12-30 22:27:32,751 - ERROR - Error processing document_id AG3VWMRAIDNQIEUG7PBGV5WYSL4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,753 - INFO - Processing document AERIMUQMCVUEN3D5WMGN7JER747A...
2025-12-30 22:27:32,755 - ERROR - Error processing document_id AERIMUQMCVUEN3D5WMGN7JER747A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,756 - INFO - Processing document AGC345DNZWNBM5KQY6LNCS66BMXA...
2025-12-30 22:27:32,758 - ERROR - Error processing document_id AGC345DNZWNBM5KQY6LNCS66BMXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,760 - INFO - Processing document AFQ2VTVVORY33X7BWPRE7A5BZP4A...
2025-12-30 22:27:32,762 - ERROR - Error processing document_id AFQ2VTVVORY33X7BWPRE7A5BZP4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,764 - INFO - Processing document AEJRYPOQY3GU2EMMSQHOHEONHD4Q...
2025-12-30 22:27:32,767 - ERROR - Error processing document_id AEJRYPOQY3GU2EMMSQHOHEONHD4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,768 - INFO - Processing document AHW7ETGBKGX6DCG2LBAG5YAXU46A...
2025-12-30 22:27:32,773 - ERROR - Error processing document_id AHW7ETGBKGX6DCG2LBAG5YAXU46A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,775 - INFO - Processing document AEWUWC3QEJ5ZXJ2R6ERWG6OQWQBQ...
2025-12-30 22:27:32,777 - ERROR - Error processing document_id AEWUWC3QEJ5ZXJ2R6ERWG6OQWQBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,779 - INFO - Processing document AHDLZ3Q2CTZ3U5EDN52RZV3QRKWQ...
2025-12-30 22:27:32,782 - ERROR - Error processing document_id AHDLZ3Q2CTZ3U5EDN52RZV3QRKWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,784 - INFO - Processing document AEDF7OCVKG7BVCVSWL3GA4RP5LUA...
2025-12-30 22:27:32,786 - ERROR - Error processing document_id AEDF7OCVKG7BVCVSWL3GA4RP5LUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,788 - INFO - Processing document AE3DHH4ITYD373F5H5N6WLOQFZBA...
2025-12-30 22:27:32,794 - ERROR - Error processing document_id AE3DHH4ITYD373F5H5N6WLOQFZBA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,796 - INFO - Processing document AGL4CWYKOEAGJRO75VGH7UCYM26A...
2025-12-30 22:27:32,798 - ERROR - Error processing document_id AGL4CWYKOEAGJRO75VGH7UCYM26A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,799 - INFO - Processing document AGLQQCIS5V6EGUCS5SSNHWZHQM6Q...
2025-12-30 22:27:32,801 - ERROR - Error processing document_id AGLQQCIS5V6EGUCS5SSNHWZHQM6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,803 - INFO - Processing document AFIAFYGY52CLYABTK4YS42R6NWDA...
2025-12-30 22:27:32,806 - ERROR - Error processing document_id AFIAFYGY52CLYABTK4YS42R6NWDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,808 - INFO - Processing document AGEK4QDZOY7OCA3RDD4UP27V43IQ...
2025-12-30 22:27:32,810 - ERROR - Error processing document_id AGEK4QDZOY7OCA3RDD4UP27V43IQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,812 - INFO - Processing document AHOWG3275HPD6BE4CRKH5J2VH5YQ...
2025-12-30 22:27:32,818 - ERROR - Error processing document_id AHOWG3275HPD6BE4CRKH5J2VH5YQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,819 - INFO - Processing document AGAYTQENQ4WNDQK6CHIVWTCP4PNQ...
2025-12-30 22:27:32,823 - ERROR - Error processing document_id AGAYTQENQ4WNDQK6CHIVWTCP4PNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,825 - INFO - Processing document AFCH5OVVRAEL5EPHPXAKAFW2ZFDQ...
2025-12-30 22:27:32,827 - ERROR - Error processing document_id AFCH5OVVRAEL5EPHPXAKAFW2ZFDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,829 - INFO - Processing document AH2ODJELVTXKEN7FNUMIOYIL6FVQ...
2025-12-30 22:27:32,831 - ERROR - Error processing document_id AH2ODJELVTXKEN7FNUMIOYIL6FVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,833 - INFO - Processing document AEAWNTD7SZZRA75U7YC6N4CK6UAQ...
2025-12-30 22:27:32,834 - ERROR - Error processing document_id AEAWNTD7SZZRA75U7YC6N4CK6UAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,836 - INFO - Processing document AFSD3W4TU6BN3ZAAL7IU4G426QDA...
2025-12-30 22:27:32,838 - ERROR - Error processing document_id AFSD3W4TU6BN3ZAAL7IU4G426QDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,839 - INFO - Processing document AEK7BFJFZ5ME3JO2PWIG2FQFSIMQ...
2025-12-30 22:27:32,841 - ERROR - Error processing document_id AEK7BFJFZ5ME3JO2PWIG2FQFSIMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,842 - INFO - Processing document AGZLCBX2OBLTCYIUG3UW2635IJIQ...
2025-12-30 22:27:32,844 - ERROR - Error processing document_id AGZLCBX2OBLTCYIUG3UW2635IJIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,846 - INFO - Processing document AGZMKHWSCB3UXDGFUPFRZSL4EAWQ...
2025-12-30 22:27:32,848 - ERROR - Error processing document_id AGZMKHWSCB3UXDGFUPFRZSL4EAWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,850 - INFO - Processing document AG375WAXLZ7PIOQKIQ6KQB4J3JVQ...
2025-12-30 22:27:32,852 - ERROR - Error processing document_id AG375WAXLZ7PIOQKIQ6KQB4J3JVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,853 - INFO - Processing document AHT2EOTOVUJTYDLVGOM6A4C5WOFA...
2025-12-30 22:27:32,855 - ERROR - Error processing document_id AHT2EOTOVUJTYDLVGOM6A4C5WOFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,857 - INFO - Processing document AE32HKESSYBEQJID7PWYW3PUUP7Q...
2025-12-30 22:27:32,860 - ERROR - Error processing document_id AE32HKESSYBEQJID7PWYW3PUUP7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,862 - INFO - Processing document AE6NJ3NX6WMUBFR4DCHLFLDNS7ZQ...
2025-12-30 22:27:32,864 - ERROR - Error processing document_id AE6NJ3NX6WMUBFR4DCHLFLDNS7ZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,865 - INFO - Processing document AFEUXFPQ3FWDKDMSKO775AIRVN5A...
2025-12-30 22:27:32,867 - ERROR - Error processing document_id AFEUXFPQ3FWDKDMSKO775AIRVN5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,868 - INFO - Processing document AFA4ULAX2OCPIAD37ADWIG5EJDRA...
2025-12-30 22:27:32,871 - ERROR - Error processing document_id AFA4ULAX2OCPIAD37ADWIG5EJDRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,872 - INFO - Processing document AHFYFPZOWMWEBUNT5LK3PULX3K5A...
2025-12-30 22:27:32,873 - ERROR - Error processing document_id AHFYFPZOWMWEBUNT5LK3PULX3K5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,874 - INFO - Processing document AFH3JYQC7DXM32SKU7Z6FPLO3GFQ...
2025-12-30 22:27:32,876 - ERROR - Error processing document_id AFH3JYQC7DXM32SKU7Z6FPLO3GFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,877 - INFO - Processing document AFXHSB22Q4GDPMJHJFMIXUJIFEBQ...
2025-12-30 22:27:32,879 - ERROR - Error processing document_id AFXHSB22Q4GDPMJHJFMIXUJIFEBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,881 - INFO - Processing document AFTE7Q5HLLLV3NE6ZK5CTJXFTT2A...
2025-12-30 22:27:32,883 - ERROR - Error processing document_id AFTE7Q5HLLLV3NE6ZK5CTJXFTT2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,884 - INFO - Processing document AFBR7YY365SSYENKSPML677BSK6A...
2025-12-30 22:27:32,886 - ERROR - Error processing document_id AFBR7YY365SSYENKSPML677BSK6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,887 - INFO - Processing document AFUSYPTE3VMHPWFKLEEJQQAIJQBQ...
2025-12-30 22:27:32,890 - ERROR - Error processing document_id AFUSYPTE3VMHPWFKLEEJQQAIJQBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,891 - INFO - Processing document AHJ3S3V3XBCJZFF7EE4RALKBJPPQ...
2025-12-30 22:27:32,893 - ERROR - Error processing document_id AHJ3S3V3XBCJZFF7EE4RALKBJPPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,894 - INFO - Processing document AHTPER2N5GBMX374B32RYBJ7GHWA...
2025-12-30 22:27:32,896 - ERROR - Error processing document_id AHTPER2N5GBMX374B32RYBJ7GHWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,898 - INFO - Processing document AET2QAIIPGMOLRSHAS7FHUPB472A...
2025-12-30 22:27:32,900 - ERROR - Error processing document_id AET2QAIIPGMOLRSHAS7FHUPB472A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,901 - INFO - Processing document AEZE6E7FWD2MHFPMUA42J6S7OS4Q...
2025-12-30 22:27:32,903 - ERROR - Error processing document_id AEZE6E7FWD2MHFPMUA42J6S7OS4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,904 - INFO - Processing document AG7TROD5VUYZ4TWTJ3D4HAD2ZBAQ...
2025-12-30 22:27:32,907 - ERROR - Error processing document_id AG7TROD5VUYZ4TWTJ3D4HAD2ZBAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,908 - INFO - Processing document AG636EMD7OVJ7V4QYL4AI7OT6WWQ...
2025-12-30 22:27:32,910 - ERROR - Error processing document_id AG636EMD7OVJ7V4QYL4AI7OT6WWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,912 - INFO - Processing document AFHUV7QQW4YWSOXURHIP2VGUCPNA...
2025-12-30 22:27:32,914 - ERROR - Error processing document_id AFHUV7QQW4YWSOXURHIP2VGUCPNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,915 - INFO - Processing document AHCHUBP5X5NRLP2MTJP43ERNB5QA...
2025-12-30 22:27:32,917 - ERROR - Error processing document_id AHCHUBP5X5NRLP2MTJP43ERNB5QA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,918 - INFO - Processing document AG4EYBMQGE6GICMNUCND5UXUZCQQ...
2025-12-30 22:27:32,920 - ERROR - Error processing document_id AG4EYBMQGE6GICMNUCND5UXUZCQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,921 - INFO - Processing document AHBFASD2QE5UW6ZFZLCRDXOXOAYA...
2025-12-30 22:27:32,923 - ERROR - Error processing document_id AHBFASD2QE5UW6ZFZLCRDXOXOAYA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,924 - INFO - Processing document AHZHHXVX5VSJH67MAVNKDNLWHJGA...
2025-12-30 22:27:32,926 - ERROR - Error processing document_id AHZHHXVX5VSJH67MAVNKDNLWHJGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,928 - INFO - Processing document AF4RALYMWOMKUJA7XMXX6CR6JCUQ...
2025-12-30 22:27:32,930 - ERROR - Error processing document_id AF4RALYMWOMKUJA7XMXX6CR6JCUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,931 - INFO - Processing document AEUV3EF6IQ5YHL44XWCTODUMPRRQ...
2025-12-30 22:27:32,934 - ERROR - Error processing document_id AEUV3EF6IQ5YHL44XWCTODUMPRRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,935 - INFO - Processing document AFIA5YDIOGEGNRECRIKC2U72H3SQ...
2025-12-30 22:27:32,937 - ERROR - Error processing document_id AFIA5YDIOGEGNRECRIKC2U72H3SQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,939 - INFO - Processing document AHBGHXADI7434HJ2LECPDAIDKDDA...
2025-12-30 22:27:32,941 - ERROR - Error processing document_id AHBGHXADI7434HJ2LECPDAIDKDDA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,942 - INFO - Processing document AHMN3HKT5BUZXMUOQ2FRRDLHEMFQ...
2025-12-30 22:27:32,944 - ERROR - Error processing document_id AHMN3HKT5BUZXMUOQ2FRRDLHEMFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,946 - INFO - Processing document AFJ66H2I53TNAIACVJMVQ56VHANQ...
2025-12-30 22:27:32,948 - ERROR - Error processing document_id AFJ66H2I53TNAIACVJMVQ56VHANQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,949 - INFO - Processing document AESVZYDHDS2VRVG527ESY3TQSTFA...
2025-12-30 22:27:32,951 - ERROR - Error processing document_id AESVZYDHDS2VRVG527ESY3TQSTFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,952 - INFO - Processing document AEQIAKGSC3BQPDP4ZHVPP4R4VWWQ...
2025-12-30 22:27:32,954 - ERROR - Error processing document_id AEQIAKGSC3BQPDP4ZHVPP4R4VWWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,955 - INFO - Processing document AG2IKJXY2CRSPFUGACG2723BRM6A...
2025-12-30 22:27:32,960 - ERROR - Error processing document_id AG2IKJXY2CRSPFUGACG2723BRM6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,962 - INFO - Processing document AFXV5PSXRBJTK3E2APZQACUKO2SA...
2025-12-30 22:27:32,965 - ERROR - Error processing document_id AFXV5PSXRBJTK3E2APZQACUKO2SA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,966 - INFO - Processing document AEAFOIDVOLQHRZ6V62ICWJ3GDFQA...
2025-12-30 22:27:32,968 - ERROR - Error processing document_id AEAFOIDVOLQHRZ6V62ICWJ3GDFQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,970 - INFO - Processing document AFI4NKLX7TFV4465XH4PSBG2CRVQ...
2025-12-30 22:27:32,972 - ERROR - Error processing document_id AFI4NKLX7TFV4465XH4PSBG2CRVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,974 - INFO - Processing document AEY5LBDPLA3PSNCHEWU7CKBWTM6A...
2025-12-30 22:27:32,976 - ERROR - Error processing document_id AEY5LBDPLA3PSNCHEWU7CKBWTM6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,977 - INFO - Processing document AHKNZYVK6SKFTGD45HHY6VZFSIIA...
2025-12-30 22:27:32,979 - ERROR - Error processing document_id AHKNZYVK6SKFTGD45HHY6VZFSIIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,981 - INFO - Processing document AGGKRBR6G6TOQXFGVW5RME24YLYQ...
2025-12-30 22:27:32,984 - ERROR - Error processing document_id AGGKRBR6G6TOQXFGVW5RME24YLYQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,985 - INFO - Processing document AFCBT6FE7AFKYPC2SETZDKGF5QJQ...
2025-12-30 22:27:32,988 - ERROR - Error processing document_id AFCBT6FE7AFKYPC2SETZDKGF5QJQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,989 - INFO - Processing document AEWOFGDZJMGNMC36BUAXVIQPDVRQ...
2025-12-30 22:27:32,992 - ERROR - Error processing document_id AEWOFGDZJMGNMC36BUAXVIQPDVRQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,994 - INFO - Processing document AE43U6LTVHUILYMB67CFGXXK7RPQ...
2025-12-30 22:27:32,997 - ERROR - Error processing document_id AE43U6LTVHUILYMB67CFGXXK7RPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:32,998 - INFO - Processing document AGQBVRWTF5UHNTHSULZ3KZUSILSA...
2025-12-30 22:27:33,001 - ERROR - Error processing document_id AGQBVRWTF5UHNTHSULZ3KZUSILSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,002 - INFO - Processing document AG6D6LX3B7YL3KJKWTWR5BYBXYLQ...
2025-12-30 22:27:33,004 - ERROR - Error processing document_id AG6D6LX3B7YL3KJKWTWR5BYBXYLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,006 - INFO - Processing document AFEYHZ6ZZCDYHVCZKIEDM2HZTVSA...
2025-12-30 22:27:33,008 - ERROR - Error processing document_id AFEYHZ6ZZCDYHVCZKIEDM2HZTVSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,009 - INFO - Processing document AFBXDI55GEOEED5P42NJ252QDYUQ...
2025-12-30 22:27:33,011 - ERROR - Error processing document_id AFBXDI55GEOEED5P42NJ252QDYUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,013 - INFO - Processing document AGFCADEBNSMSX2DQDIH3RXMU23YQ...
2025-12-30 22:27:33,015 - ERROR - Error processing document_id AGFCADEBNSMSX2DQDIH3RXMU23YQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,017 - INFO - Processing document AGJE5ZWDJQR4VZC3GZW5Z4JSCQWQ...
2025-12-30 22:27:33,019 - ERROR - Error processing document_id AGJE5ZWDJQR4VZC3GZW5Z4JSCQWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,021 - INFO - Processing document AEP4KJDGBH6XPJKFEHPYSAPUNDBQ...
2025-12-30 22:27:33,023 - ERROR - Error processing document_id AEP4KJDGBH6XPJKFEHPYSAPUNDBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,025 - INFO - Processing document AEBSTN2OGMVGOZQX4KWYDXQRM6XQ...
2025-12-30 22:27:33,027 - ERROR - Error processing document_id AEBSTN2OGMVGOZQX4KWYDXQRM6XQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,028 - INFO - Processing document AF3S4OODVOC25BTNXCXWZIJJ37JQ...
2025-12-30 22:27:33,032 - ERROR - Error processing document_id AF3S4OODVOC25BTNXCXWZIJJ37JQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,034 - INFO - Processing document AHFCZ7XCEX5WIFH66QNTN2U5BN7Q...
2025-12-30 22:27:33,037 - ERROR - Error processing document_id AHFCZ7XCEX5WIFH66QNTN2U5BN7Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,039 - INFO - Processing document AHXTFYXUTFW7DPWTCBT27TZYGQSQ...
2025-12-30 22:27:33,042 - ERROR - Error processing document_id AHXTFYXUTFW7DPWTCBT27TZYGQSQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,044 - INFO - Processing document AFLLYLKKMJH7PFBFGQ6A4XPWJXAQ...
2025-12-30 22:27:33,048 - ERROR - Error processing document_id AFLLYLKKMJH7PFBFGQ6A4XPWJXAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,050 - INFO - Processing document AGN443VTLY3CKVLH2FYP2ZCIWL7A...
2025-12-30 22:27:33,053 - ERROR - Error processing document_id AGN443VTLY3CKVLH2FYP2ZCIWL7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,054 - INFO - Processing document AFA3EMBUOYCUMQ3L7FKOJULL2TUA...
2025-12-30 22:27:33,057 - ERROR - Error processing document_id AFA3EMBUOYCUMQ3L7FKOJULL2TUA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,058 - INFO - Processing document AEKVHQLP3E26QFMIFJOCPINQVWSA...
2025-12-30 22:27:33,060 - ERROR - Error processing document_id AEKVHQLP3E26QFMIFJOCPINQVWSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,062 - INFO - Processing document AHQESZRWRQOXPCCSW5M4OG6FQ34A...
2025-12-30 22:27:33,066 - ERROR - Error processing document_id AHQESZRWRQOXPCCSW5M4OG6FQ34A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,068 - INFO - Processing document AHQUOWNRCATPS6JBCQ7Y3M7VL2YQ...
2025-12-30 22:27:33,069 - ERROR - Error processing document_id AHQUOWNRCATPS6JBCQ7Y3M7VL2YQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,071 - INFO - Processing document AGKTNGBZO7H6F7B6HBJNP7GUWADA...
2025-12-30 22:27:33,073 - ERROR - Error processing document_id AGKTNGBZO7H6F7B6HBJNP7GUWADA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,074 - INFO - Processing document AEHHUVA44NHQCOGEQU6B7CGPYR3Q...
2025-12-30 22:27:33,076 - ERROR - Error processing document_id AEHHUVA44NHQCOGEQU6B7CGPYR3Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,077 - INFO - Processing document AGDI3LGJBMYIAJL27ZTLHPCJUGHQ...
2025-12-30 22:27:33,079 - ERROR - Error processing document_id AGDI3LGJBMYIAJL27ZTLHPCJUGHQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,081 - INFO - Processing document AGQQPNHYQK7NWL6D5S3ULMPMO3NQ...
2025-12-30 22:27:33,083 - ERROR - Error processing document_id AGQQPNHYQK7NWL6D5S3ULMPMO3NQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,084 - INFO - Processing document AGHTTJNQMZK3YHJOXMOLZWYXKDSA...
2025-12-30 22:27:33,086 - ERROR - Error processing document_id AGHTTJNQMZK3YHJOXMOLZWYXKDSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,088 - INFO - Processing document AFZQ26E6BQLTEZUFUU6RZH22VZDQ...
2025-12-30 22:27:33,090 - ERROR - Error processing document_id AFZQ26E6BQLTEZUFUU6RZH22VZDQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,092 - INFO - Processing document AFPQS5EYMDPVNSTKOJRHGR4UWIIQ...
2025-12-30 22:27:33,094 - ERROR - Error processing document_id AFPQS5EYMDPVNSTKOJRHGR4UWIIQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,095 - INFO - Processing document AHDGF32KTM63IXVFNWPFXQGIKQNA...
2025-12-30 22:27:33,098 - ERROR - Error processing document_id AHDGF32KTM63IXVFNWPFXQGIKQNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,099 - INFO - Processing document AHNTLSKAXRNUWUVO6L34VRNO6IUQ...
2025-12-30 22:27:33,101 - ERROR - Error processing document_id AHNTLSKAXRNUWUVO6L34VRNO6IUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,103 - INFO - Processing document AHAAKEORZEGD5MHEGQV76ZCXCAOQ...
2025-12-30 22:27:33,104 - ERROR - Error processing document_id AHAAKEORZEGD5MHEGQV76ZCXCAOQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,106 - INFO - Processing document AEIIZDRM4H63JBHG74ELLSXVKUVA...
2025-12-30 22:27:33,108 - ERROR - Error processing document_id AEIIZDRM4H63JBHG74ELLSXVKUVA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,110 - INFO - Processing document AHJRJCJMK3XVV4BSPBRAHIYEODWA...
2025-12-30 22:27:33,112 - ERROR - Error processing document_id AHJRJCJMK3XVV4BSPBRAHIYEODWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,113 - INFO - Processing document AFDNLNTANUGENK62RSTAQALJOWPQ...
2025-12-30 22:27:33,115 - ERROR - Error processing document_id AFDNLNTANUGENK62RSTAQALJOWPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,116 - INFO - Processing document AFOS2SS7R2WSHHPZBXALKHD32U7A...
2025-12-30 22:27:33,118 - ERROR - Error processing document_id AFOS2SS7R2WSHHPZBXALKHD32U7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,119 - INFO - Processing document AFH75OQDZOXFY2YB5YNDDWCQVXWQ...
2025-12-30 22:27:33,122 - ERROR - Error processing document_id AFH75OQDZOXFY2YB5YNDDWCQVXWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,123 - INFO - Processing document AE52IM2PQLICR4YQSBN6EN3VY2QA...
2025-12-30 22:27:33,125 - ERROR - Error processing document_id AE52IM2PQLICR4YQSBN6EN3VY2QA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,126 - INFO - Processing document AHPG65LKS3QKRWDUWAKZNLEK5RZQ...
2025-12-30 22:27:33,128 - ERROR - Error processing document_id AHPG65LKS3QKRWDUWAKZNLEK5RZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,130 - INFO - Processing document AEP4US4HIIHTDUIYK2ZKXRXV72WQ...
2025-12-30 22:27:33,132 - ERROR - Error processing document_id AEP4US4HIIHTDUIYK2ZKXRXV72WQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,134 - INFO - Processing document AHTNJHH3I2PCUUPM326VSZGG7THQ...
2025-12-30 22:27:33,136 - ERROR - Error processing document_id AHTNJHH3I2PCUUPM326VSZGG7THQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,137 - INFO - Processing document AELR7MDYHN5S2XNXAQBRBUWNPSPQ...
2025-12-30 22:27:33,139 - ERROR - Error processing document_id AELR7MDYHN5S2XNXAQBRBUWNPSPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,140 - INFO - Processing document AG2X3ZLGBQTPWMB47EKSV2R6AVLA...
2025-12-30 22:27:33,143 - ERROR - Error processing document_id AG2X3ZLGBQTPWMB47EKSV2R6AVLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,144 - INFO - Processing document AGQ2QIH2GQSUYITWCW5IPARVTQXA...
2025-12-30 22:27:33,146 - ERROR - Error processing document_id AGQ2QIH2GQSUYITWCW5IPARVTQXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,147 - INFO - Processing document AFCGT6IWC5C32D5UJXLZWCELQRVQ...
2025-12-30 22:27:33,149 - ERROR - Error processing document_id AFCGT6IWC5C32D5UJXLZWCELQRVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,150 - INFO - Processing document AF4IC4PJV2IUIFXUP54UV3KQWZFQ...
2025-12-30 22:27:33,152 - ERROR - Error processing document_id AF4IC4PJV2IUIFXUP54UV3KQWZFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,153 - INFO - Processing document AHR4IP7KCYDKH2IVHVRVW3Y54I6Q...
2025-12-30 22:27:33,155 - ERROR - Error processing document_id AHR4IP7KCYDKH2IVHVRVW3Y54I6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,156 - INFO - Processing document AHERRN6ALBXBEO2NUVC6YBOW4Y2A...
2025-12-30 22:27:33,159 - ERROR - Error processing document_id AHERRN6ALBXBEO2NUVC6YBOW4Y2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,160 - INFO - Processing document AE32QAGQRQFJ7JM22HU5P2OEQUFA...
2025-12-30 22:27:33,162 - ERROR - Error processing document_id AE32QAGQRQFJ7JM22HU5P2OEQUFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,164 - INFO - Processing document AFNVURTNRTC2OM3YTN3MWTHIIWNQ...
2025-12-30 22:27:33,165 - ERROR - Error processing document_id AFNVURTNRTC2OM3YTN3MWTHIIWNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,167 - INFO - Processing document AHF6R37O4OTZ3YCDPFJQRSUD7FWA...
2025-12-30 22:27:33,169 - ERROR - Error processing document_id AHF6R37O4OTZ3YCDPFJQRSUD7FWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,170 - INFO - Processing document AFZI7NRJ3EHUXZ7M2MZG2SSTH73A...
2025-12-30 22:27:33,173 - ERROR - Error processing document_id AFZI7NRJ3EHUXZ7M2MZG2SSTH73A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,175 - INFO - Processing document AGZYNKTWTUPP4ZLJ77LH7TFL6Y5A...
2025-12-30 22:27:33,176 - ERROR - Error processing document_id AGZYNKTWTUPP4ZLJ77LH7TFL6Y5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,178 - INFO - Processing document AGPEVQWD4R6SOGLZ26CW6DZ7HP4A...
2025-12-30 22:27:33,179 - ERROR - Error processing document_id AGPEVQWD4R6SOGLZ26CW6DZ7HP4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,181 - INFO - Processing document AHWCZ47A7FIYEJ7KEJG3BO3F5YXQ...
2025-12-30 22:27:33,183 - ERROR - Error processing document_id AHWCZ47A7FIYEJ7KEJG3BO3F5YXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,184 - INFO - Processing document AEFCPVWZRFUVKGN3TMXEKJOCQGVQ...
2025-12-30 22:27:33,187 - ERROR - Error processing document_id AEFCPVWZRFUVKGN3TMXEKJOCQGVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,188 - INFO - Processing document AETNAFSIOZVNJ6QRKK5JPBSSHLWQ...
2025-12-30 22:27:33,191 - ERROR - Error processing document_id AETNAFSIOZVNJ6QRKK5JPBSSHLWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,192 - INFO - Processing document AHBUUZSCO7SNXMWITNM6TSDMF5GA...
2025-12-30 22:27:33,194 - ERROR - Error processing document_id AHBUUZSCO7SNXMWITNM6TSDMF5GA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,196 - INFO - Processing document AF5BJFLIVCZUSDYDXK56RHTPD2FA...
2025-12-30 22:27:33,199 - ERROR - Error processing document_id AF5BJFLIVCZUSDYDXK56RHTPD2FA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,201 - INFO - Processing document AFXSVEP3YV4X5H2YRFZJGLAFFZWQ...
2025-12-30 22:27:33,203 - ERROR - Error processing document_id AFXSVEP3YV4X5H2YRFZJGLAFFZWQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,205 - INFO - Processing document AGQ4GCAMMF2QLK5KT3CIIYCKRWAQ...
2025-12-30 22:27:33,206 - ERROR - Error processing document_id AGQ4GCAMMF2QLK5KT3CIIYCKRWAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,208 - INFO - Processing document AFGEL7XKPUAHC4TE4DUYL4D5XLUQ...
2025-12-30 22:27:33,211 - ERROR - Error processing document_id AFGEL7XKPUAHC4TE4DUYL4D5XLUQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,213 - INFO - Processing document AGFOO4AJUJI3ZYD7OUN3HQXVPM6Q...
2025-12-30 22:27:33,215 - ERROR - Error processing document_id AGFOO4AJUJI3ZYD7OUN3HQXVPM6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,217 - INFO - Processing document AGFH3JIX2B4DKNG6UWJYZ7KIWHSA...
2025-12-30 22:27:33,221 - ERROR - Error processing document_id AGFH3JIX2B4DKNG6UWJYZ7KIWHSA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,223 - INFO - Processing document AG2SIMIPQV3W6VUMEQSYKX7H227A...
2025-12-30 22:27:33,225 - ERROR - Error processing document_id AG2SIMIPQV3W6VUMEQSYKX7H227A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,227 - INFO - Processing document AGMMY7BD5GN3YWIIC6WH7P2646IA...
2025-12-30 22:27:33,229 - ERROR - Error processing document_id AGMMY7BD5GN3YWIIC6WH7P2646IA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,230 - INFO - Processing document AG6MIHPYOU26DQDOI4LJNFU4HZNQ...
2025-12-30 22:27:33,233 - ERROR - Error processing document_id AG6MIHPYOU26DQDOI4LJNFU4HZNQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,235 - INFO - Processing document AFUKL76HRWGYKAZ6WYWCTYVRUBIA...
2025-12-30 22:27:33,238 - ERROR - Error processing document_id AFUKL76HRWGYKAZ6WYWCTYVRUBIA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,240 - INFO - Processing document AHTMGUXGMVRSJ6QEEQNZHAZJ6HFQ...
2025-12-30 22:27:33,243 - ERROR - Error processing document_id AHTMGUXGMVRSJ6QEEQNZHAZJ6HFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,245 - INFO - Processing document AFUNUNBPGAML3VCYIANGF3FIH57Q...
2025-12-30 22:27:33,247 - ERROR - Error processing document_id AFUNUNBPGAML3VCYIANGF3FIH57Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,248 - INFO - Processing document AFT62M2ZBCFVU2IABESLM2IXEXNA...
2025-12-30 22:27:33,251 - ERROR - Error processing document_id AFT62M2ZBCFVU2IABESLM2IXEXNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,253 - INFO - Processing document AELTNX7LHW5JTCGNAUVAKNUCVGXQ...
2025-12-30 22:27:33,255 - ERROR - Error processing document_id AELTNX7LHW5JTCGNAUVAKNUCVGXQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,256 - INFO - Processing document AGVJVJS3XH3OEDCCR2IBPGUF3JMQ...
2025-12-30 22:27:33,259 - ERROR - Error processing document_id AGVJVJS3XH3OEDCCR2IBPGUF3JMQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,261 - INFO - Processing document AGKMRGMZ7Y2O2MU24AWQ4RA5WESQ...
2025-12-30 22:27:33,264 - ERROR - Error processing document_id AGKMRGMZ7Y2O2MU24AWQ4RA5WESQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,266 - INFO - Processing document AHIORZMQFBFIN7FXNHEDPPI6TPRA...
2025-12-30 22:27:33,270 - ERROR - Error processing document_id AHIORZMQFBFIN7FXNHEDPPI6TPRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,271 - INFO - Processing document AHG34BLPBFJAZLKSZTRSVPBYMNQA...
2025-12-30 22:27:33,274 - ERROR - Error processing document_id AHG34BLPBFJAZLKSZTRSVPBYMNQA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,275 - INFO - Processing document AERWHGDBA6B3XDADBXDKSBQZOW4A...
2025-12-30 22:27:33,278 - ERROR - Error processing document_id AERWHGDBA6B3XDADBXDKSBQZOW4A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,279 - INFO - Processing document AG2GEFJ2XW2YLFLBXLEHHMNWP2ZA...
2025-12-30 22:27:33,283 - ERROR - Error processing document_id AG2GEFJ2XW2YLFLBXLEHHMNWP2ZA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,284 - INFO - Processing document AH6MR4SZHU7CRKL6VEC3AZQMPHNA...
2025-12-30 22:27:33,289 - ERROR - Error processing document_id AH6MR4SZHU7CRKL6VEC3AZQMPHNA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,290 - INFO - Processing document AFRACD34S4JL6NI732UQCYVDTVHA...
2025-12-30 22:27:33,293 - ERROR - Error processing document_id AFRACD34S4JL6NI732UQCYVDTVHA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,294 - INFO - Processing document AFDZLAWMZLKXLSQZLNQ5URRWZAVQ...
2025-12-30 22:27:33,297 - ERROR - Error processing document_id AFDZLAWMZLKXLSQZLNQ5URRWZAVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,298 - INFO - Processing document AEEAYQY5YJPQEXFUXK2TIDMRAUKQ...
2025-12-30 22:27:33,301 - ERROR - Error processing document_id AEEAYQY5YJPQEXFUXK2TIDMRAUKQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,303 - INFO - Processing document AF3B7N46YKJFHVZJWAKKCUIDO6HQ...
2025-12-30 22:27:33,305 - ERROR - Error processing document_id AF3B7N46YKJFHVZJWAKKCUIDO6HQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,307 - INFO - Processing document AEG6AHYV5WATXWBJ6UXREMFJZVPQ...
2025-12-30 22:27:33,309 - ERROR - Error processing document_id AEG6AHYV5WATXWBJ6UXREMFJZVPQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,311 - INFO - Processing document AECUQNTYGADM6XKCDKWEJPDQGNVQ...
2025-12-30 22:27:33,313 - ERROR - Error processing document_id AECUQNTYGADM6XKCDKWEJPDQGNVQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,315 - INFO - Processing document AF6E2F7EE6KLTN6ZCGAMJHC65V6Q...
2025-12-30 22:27:33,318 - ERROR - Error processing document_id AF6E2F7EE6KLTN6ZCGAMJHC65V6Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,319 - INFO - Processing document AETD4B3NZCZICXNC7LY7UZ5RUWFQ...
2025-12-30 22:27:33,321 - ERROR - Error processing document_id AETD4B3NZCZICXNC7LY7UZ5RUWFQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,323 - INFO - Processing document AHYYNTUYOY643QKQNBOOZGNZSD6A...
2025-12-30 22:27:33,325 - ERROR - Error processing document_id AHYYNTUYOY643QKQNBOOZGNZSD6A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,326 - INFO - Processing document AGBLTK6LA74UADVPEXXFCZP5JK5Q...
2025-12-30 22:27:33,329 - ERROR - Error processing document_id AGBLTK6LA74UADVPEXXFCZP5JK5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,331 - INFO - Processing document AGXJSDVZF367CAPBHLVODOMWWF4Q...
2025-12-30 22:27:33,334 - ERROR - Error processing document_id AGXJSDVZF367CAPBHLVODOMWWF4Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,335 - INFO - Processing document AHZ7XZQVCIWUY6ONMGNMADRXW3WA...
2025-12-30 22:27:33,338 - ERROR - Error processing document_id AHZ7XZQVCIWUY6ONMGNMADRXW3WA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,339 - INFO - Processing document AELOPKQYD2X63V3T4EICWXGXODQQ...
2025-12-30 22:27:33,342 - ERROR - Error processing document_id AELOPKQYD2X63V3T4EICWXGXODQQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,344 - INFO - Processing document AFPNEXI52BHL4CYZ4S3NNXEMHVFA...
2025-12-30 22:27:33,346 - ERROR - Error processing document_id AFPNEXI52BHL4CYZ4S3NNXEMHVFA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,347 - INFO - Processing document AHPPSVUX6OTSHIVWOOYOMAAWCGAQ...
2025-12-30 22:27:33,351 - ERROR - Error processing document_id AHPPSVUX6OTSHIVWOOYOMAAWCGAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,352 - INFO - Processing document AGSM75HHGVYI263VKMFQMK6GJQ7A...
2025-12-30 22:27:33,354 - ERROR - Error processing document_id AGSM75HHGVYI263VKMFQMK6GJQ7A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,356 - INFO - Processing document AGZ7VUXZEQRQ7SZB4YC73IWMRUBQ...
2025-12-30 22:27:33,359 - ERROR - Error processing document_id AGZ7VUXZEQRQ7SZB4YC73IWMRUBQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,361 - INFO - Processing document AEVPE24D65URL5YVA4CZ3OKL3Q5Q...
2025-12-30 22:27:33,364 - ERROR - Error processing document_id AEVPE24D65URL5YVA4CZ3OKL3Q5Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,366 - INFO - Processing document AHWVA32YSIO5YBTB7LXWHEX26QLA...
2025-12-30 22:27:33,368 - ERROR - Error processing document_id AHWVA32YSIO5YBTB7LXWHEX26QLA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,370 - INFO - Processing document AHXIV7OEQP6H5OZ7VIAHD2PFEZXA...
2025-12-30 22:27:33,372 - ERROR - Error processing document_id AHXIV7OEQP6H5OZ7VIAHD2PFEZXA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,374 - INFO - Processing document AGFZ4CLSLXP2WIHC6AWOPRIUAW2Q...
2025-12-30 22:27:33,376 - ERROR - Error processing document_id AGFZ4CLSLXP2WIHC6AWOPRIUAW2Q: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,378 - INFO - Processing document AGHUETPXYFI73ZQUKRHXRILKM4SQ...
2025-12-30 22:27:33,381 - ERROR - Error processing document_id AGHUETPXYFI73ZQUKRHXRILKM4SQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,382 - INFO - Processing document AHHEGLEMWY7GRQI36TFBWCUT24BQ...
2025-12-30 22:27:33,385 - ERROR - Error processing document_id AHHEGLEMWY7GRQI36TFBWCUT24BQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,386 - INFO - Processing document AGU2JN5KGHXDVFDXZ4HPCO573QEA...
2025-12-30 22:27:33,389 - ERROR - Error processing document_id AGU2JN5KGHXDVFDXZ4HPCO573QEA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,391 - INFO - Processing document AHKG4QWB3CKG3RB7J7DGP5EYJBRA...
2025-12-30 22:27:33,393 - ERROR - Error processing document_id AHKG4QWB3CKG3RB7J7DGP5EYJBRA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,395 - INFO - Processing document AHMIG4FAZQK2VEFIFU2TBOGYTPLQ...
2025-12-30 22:27:33,397 - ERROR - Error processing document_id AHMIG4FAZQK2VEFIFU2TBOGYTPLQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,398 - INFO - Processing document AF63RGZRCYCKWRSBIU3IH6OJLJ5A...
2025-12-30 22:27:33,400 - ERROR - Error processing document_id AF63RGZRCYCKWRSBIU3IH6OJLJ5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,402 - INFO - Processing document AG7OQTILOAFFSVAYH3U2H6FSBFWA...
2025-12-30 22:27:33,404 - ERROR - Error processing document_id AG7OQTILOAFFSVAYH3U2H6FSBFWA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,405 - INFO - Processing document AGLIQOXT77X6WDFYQ2E2RAAKIR2A...
2025-12-30 22:27:33,410 - ERROR - Error processing document_id AGLIQOXT77X6WDFYQ2E2RAAKIR2A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,412 - INFO - Processing document AFCZERNJOXNOVCXZ4NZKIAITHEAQ...
2025-12-30 22:27:33,414 - ERROR - Error processing document_id AFCZERNJOXNOVCXZ4NZKIAITHEAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,416 - INFO - Processing document AEBSY4Q5CDXMDELKLR4MEMEI6JGA...
2025-12-30 22:27:33,420 - ERROR - Error processing document_id AEBSY4Q5CDXMDELKLR4MEMEI6JGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,421 - INFO - Processing document AFIBVGIF34DLZNIOMBIWKTTDUL5A...
2025-12-30 22:27:33,424 - ERROR - Error processing document_id AFIBVGIF34DLZNIOMBIWKTTDUL5A: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,426 - INFO - Processing document AGHFFQNGGPTSXM35ONFNWHM55QGA...
2025-12-30 22:27:33,428 - ERROR - Error processing document_id AGHFFQNGGPTSXM35ONFNWHM55QGA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,429 - INFO - Processing document AG252QMDC7EOHUHQD26JNBMXXPKA...
2025-12-30 22:27:33,433 - ERROR - Error processing document_id AG252QMDC7EOHUHQD26JNBMXXPKA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,434 - INFO - Processing document AHAZZDO3X4TZK7M64VSFTXZKIPJA...
2025-12-30 22:27:33,436 - ERROR - Error processing document_id AHAZZDO3X4TZK7M64VSFTXZKIPJA: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,438 - INFO - Processing document AE22QOZDLAODPKJHYZI2HXQ7MLZQ...
2025-12-30 22:27:33,440 - ERROR - Error processing document_id AE22QOZDLAODPKJHYZI2HXQ7MLZQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
2025-12-30 22:27:33,441 - INFO - Processing document AHGAFLDRVLOCEYDXK3XOVP6BUBAQ...
2025-12-30 22:27:33,443 - ERROR - Error processing document_id AHGAFLDRVLOCEYDXK3XOVP6BUBAQ: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
Traceback (most recent call last):
  File "/root/autodl-pvt/amazon_books/dos_rag/precreate_nodes.py", line 46, in precreate_nodes
    rag.chunk_and_embed_document(review_text)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/root/autodl-pvt/amazon_books/dos_rag/method/RAG.py", line 62, in chunk_and_embed_document
    chunks = split_text(text, self.tokenizer, self.chunk_size)
  File "/root/autodl-pvt/amazon_books/dos_rag/method/utils.py", line 26, in split_text
    sentences = nltk.tokenize.sent_tokenize(text)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 119, in sent_tokenize
    tokenizer = _get_punkt_tokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/__init__.py", line 105, in _get_punkt_tokenizer
    return PunktTokenizer(language)
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1744, in __init__
    self.load_lang(lang)
    ~~~~~~~~~~~~~~^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1750, in load_lang
    self._params = load_punkt_params(lang_dir)
                   ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/root/miniconda3/envs/amazon/lib/python3.14/site-packages/nltk/tokenize/punkt.py", line 1763, in load_punkt_params
    with open(f"{lang_dir}/collocations.tab", encoding="utf-8") as f:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NotADirectoryError: [Errno 20] Not a directory: '/root/nltk_data/tokenizers/punkt_tab.zip/punkt_tab/english/collocations.tab'
